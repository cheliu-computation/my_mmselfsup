Mon Feb 13 10:39:56 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                  Off |
| N/A   24C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |                  Off |
| N/A   23C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                  Off |
| N/A   24C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |                  Off |
| N/A   24C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA A100-SXM...  On   | 00000000:87:00.0 Off |                  Off |
| N/A   29C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA A100-SXM...  On   | 00000000:90:00.0 Off |                  Off |
| N/A   29C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                  Off |
| N/A   30C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                  Off |
| N/A   30C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Thu_Nov_18_09:45:30_PST_2021
Cuda compilation tools, release 11.5, V11.5.119
Build cuda_11.5.r11.5/compiler.30672275_0
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
2023-02-13 10:40:08,418 - mmselfsup - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.11 (main, Mar 29 2022, 19:08:29) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-40GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.5, V11.5.119
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.11.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0
OpenCV: 4.7.0
MMCV: 1.7.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSelfSup: 0.10.1+c3f655a
------------------------------------------------------------

2023-02-13 10:40:09,099 - mmselfsup - INFO - Distributed training: True
2023-02-13 10:40:09,669 - mmselfsup - INFO - Config:
model = dict(
    type='BYOL',
    base_momentum=0.99,
    backbone=dict(
        type='ResNet',
        depth=50,
        in_channels=3,
        out_indices=[4],
        norm_cfg=dict(type='SyncBN')),
    neck=dict(
        type='NonLinearNeck',
        in_channels=2048,
        hid_channels=4096,
        out_channels=256,
        num_layers=2,
        with_bias=True,
        with_last_bn=False,
        with_avg_pool=True),
    head=dict(
        type='LatentPredictHead',
        predictor=dict(
            type='NonLinearNeck',
            in_channels=256,
            hid_channels=4096,
            out_channels=256,
            num_layers=2,
            with_bias=True,
            with_last_bn=False,
            with_avg_pool=False)))
data_source = 'CXR'
dataset_type_mimic = 'MultiViewDatasetMIMIC'
dataset_type_cxr14 = 'MultiViewDatasetNIH'
dataset_type_cxp = 'MultiViewDatasetCXP'
dataset_type_pdc = 'MultiViewDatasetPDC'
img_norm_cfg = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
train_pipeline1 = [
    dict(
        type='Normalize',
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]),
    dict(type='RandomResizedCrop', size=224, interpolation=3),
    dict(type='RandomHorizontalFlip')
]
train_pipeline2 = [
    dict(
        type='Normalize',
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]),
    dict(type='RandomResizedCrop', size=224, interpolation=3),
    dict(type='RandomHorizontalFlip')
]
prefetch = False
data = dict(
    samples_per_gpu=256,
    workers_per_gpu=8,
    train=[
        dict(
            type='MultiViewDatasetPDC',
            data_source=dict(type='CXR', data_prefix='cxr', ann_file='cxr'),
            num_views=[1, 1],
            pipelines=[[{
                'type': 'Normalize',
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            }, {
                'type': 'RandomResizedCrop',
                'size': 224,
                'interpolation': 3
            }, {
                'type': 'RandomHorizontalFlip'
            }],
                       [{
                           'type': 'Normalize',
                           'mean': [0.485, 0.456, 0.406],
                           'std': [0.229, 0.224, 0.225]
                       }, {
                           'type': 'RandomResizedCrop',
                           'size': 224,
                           'interpolation': 3
                       }, {
                           'type': 'RandomHorizontalFlip'
                       }]],
            prefetch=False),
        dict(
            type='MultiViewDatasetNIH',
            data_source=dict(type='CXR', data_prefix='cxr', ann_file='cxr'),
            num_views=[1, 1],
            pipelines=[[{
                'type': 'Normalize',
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            }, {
                'type': 'RandomResizedCrop',
                'size': 224,
                'interpolation': 3
            }, {
                'type': 'RandomHorizontalFlip'
            }],
                       [{
                           'type': 'Normalize',
                           'mean': [0.485, 0.456, 0.406],
                           'std': [0.229, 0.224, 0.225]
                       }, {
                           'type': 'RandomResizedCrop',
                           'size': 224,
                           'interpolation': 3
                       }, {
                           'type': 'RandomHorizontalFlip'
                       }]],
            prefetch=False),
        dict(
            type='MultiViewDatasetCXP',
            data_source=dict(type='CXR', data_prefix='cxr', ann_file='cxr'),
            num_views=[1, 1],
            pipelines=[[{
                'type': 'Normalize',
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            }, {
                'type': 'RandomResizedCrop',
                'size': 224,
                'interpolation': 3
            }, {
                'type': 'RandomHorizontalFlip'
            }],
                       [{
                           'type': 'Normalize',
                           'mean': [0.485, 0.456, 0.406],
                           'std': [0.229, 0.224, 0.225]
                       }, {
                           'type': 'RandomResizedCrop',
                           'size': 224,
                           'interpolation': 3
                       }, {
                           'type': 'RandomHorizontalFlip'
                       }]],
            prefetch=False),
        dict(
            type='MultiViewDatasetMIMIC',
            data_source=dict(type='CXR', data_prefix='cxr', ann_file='cxr'),
            num_views=[1, 1],
            pipelines=[[{
                'type': 'Normalize',
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            }, {
                'type': 'RandomResizedCrop',
                'size': 224,
                'interpolation': 3
            }, {
                'type': 'RandomHorizontalFlip'
            }],
                       [{
                           'type': 'Normalize',
                           'mean': [0.485, 0.456, 0.406],
                           'std': [0.229, 0.224, 0.225]
                       }, {
                           'type': 'RandomResizedCrop',
                           'size': 224,
                           'interpolation': 3
                       }, {
                           'type': 'RandomHorizontalFlip'
                       }]],
            prefetch=False)
    ])
optimizer = dict(
    type='LARS',
    lr=4.8,
    weight_decay=1e-06,
    momentum=0.9,
    paramwise_options=dict({
        '(bn|gn)(\d+)?.(weight|bias)':
        dict(weight_decay=0.0, lars_exclude=True),
        'bias':
        dict(weight_decay=0.0, lars_exclude=True)
    }))
optimizer_config = dict(update_interval=2)
lr_config = dict(
    policy='CosineAnnealing',
    min_lr=0.0,
    warmup='linear',
    warmup_iters=10,
    warmup_ratio=0.0001,
    warmup_by_epoch=True)
runner = dict(type='EpochBasedRunner', max_epochs=100)
checkpoint_config = dict(interval=10, max_keep_ckpts=3)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
cudnn_benchmark = True
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
persistent_workers = True
opencv_num_threads = 0
mp_start_method = 'fork'
update_interval = 2
custom_hooks = [dict(type='BYOLHook', end_momentum=1.0, update_interval=2)]
fp16 = dict(loss_scale=512.0)
work_dir = '/home/cl522/github_repo/res50_allCXR_log/BYOL'
auto_resume = False
gpu_ids = range(0, 8)

2023-02-13 10:40:09,669 - mmselfsup - INFO - Set random seed to 0, deterministic: False
2023-02-13 10:40:10,022 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,024 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,039 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,077 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,092 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,092 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,098 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,108 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,164 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,164 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,168 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,169 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,179 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,184 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,219 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,225 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,232 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,234 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,237 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,239 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,241 - mmcv - INFO - 
online_net.0.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
-------
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 

2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,242 - mmcv - INFO - 
online_net.0.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,243 - mmcv - INFO - 
online_net.0.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,244 - mmcv - INFO - 
online_net.0.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,245 - mmcv - INFO - 
online_net.0.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
online_net.0.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
online_net.0.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
online_net.1.fc0.weight - torch.Size([4096, 2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
online_net.1.fc0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
online_net.1.bn0.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
online_net.1.bn0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
online_net.1.fc1.weight - torch.Size([256, 4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,246 - mmcv - INFO - 
target_net.0.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,247 - mmcv - INFO - 
target_net.0.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}2023-02-13 10:40:10,248 - mmcv - INFO - 
target_net.0.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 

2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,249 - mmcv - INFO - 
target_net.0.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
-------
checking cfg!!!
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
-------2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 

checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,250 - mmcv - INFO - 
target_net.0.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
target_net.0.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
target_net.0.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
target_net.1.fc0.weight - torch.Size([4096, 2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
target_net.1.fc0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
target_net.1.bn0.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
target_net.1.bn0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
target_net.1.fc1.weight - torch.Size([256, 4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
head.predictor.fc0.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
head.predictor.fc0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
head.predictor.bn0.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
head.predictor.bn0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 10:40:10,251 - mmcv - INFO - 
head.predictor.fc1.weight - torch.Size([256, 4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,251 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 10:40:10,256 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
paramwise_options --                                     online_net.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.downsample.1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.downsample.1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.downsample.1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.downsample.1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.downsample.1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.downsample.1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.downsample.1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.downsample.1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.1.fc0.bias: weight_decay=0.0
paramwise_options --                                     online_net.1.fc0.bias: lars_exclude=True
paramwise_options --                                     online_net.1.bn0.weight: weight_decay=0.0
paramwise_options --                                     online_net.1.bn0.weight: lars_exclude=True
paramwise_options --                                     online_net.1.bn0.bias: weight_decay=0.0
paramwise_options --                                     online_net.1.bn0.bias: lars_exclude=True
paramwise_options --                                     online_net.1.bn0.bias: weight_decay=0.0
paramwise_options --                                     online_net.1.bn0.bias: lars_exclude=True
paramwise_options --                                     head.predictor.fc0.bias: weight_decay=0.0
paramwise_options --                                     head.predictor.fc0.bias: lars_exclude=True
paramwise_options --                                     head.predictor.bn0.weight: weight_decay=0.0
paramwise_options --                                     head.predictor.bn0.weight: lars_exclude=True
paramwise_options --                                     head.predictor.bn0.bias: weight_decay=0.0
paramwise_options --                                     head.predictor.bn0.bias: lars_exclude=True
paramwise_options --                                     head.predictor.bn0.bias: weight_decay=0.0
paramwise_options --                                     head.predictor.bn0.bias: lars_exclude=True
2023-02-13 10:40:18,638 - mmselfsup - INFO - Start running, host: cl522@ese-hivemind, work_dir: /home/cl522/github_repo/res50_allCXR_log/BYOL
2023-02-13 10:40:18,638 - mmselfsup - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(ABOVE_NORMAL) GradAccumFp16OptimizerHook         
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) MomentumUpdateHook                 
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) GradAccumFp16OptimizerHook         
(NORMAL      ) CheckpointHook                     
(NORMAL      ) MomentumUpdateHook                 
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-13 10:40:18,638 - mmselfsup - INFO - workflow: [('train', 1)], max: 100 epochs
2023-02-13 10:40:18,640 - mmselfsup - INFO - Checkpoints will be saved to /home/cl522/github_repo/res50_allCXR_log/BYOL by HardDiskBackend.
2023-02-13 10:40:40,062 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 10:40:40,062 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 10:40:40,062 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 10:40:40,062 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 10:40:40,062 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 10:40:40,062 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 10:40:40,062 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 10:40:40,063 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 10:41:13,642 - mmselfsup - INFO - Epoch [1][50/427]	lr: 5.556e-02, eta: 13:01:53, time: 1.100, data_time: 0.229, memory: 25346, loss: 6.7934
2023-02-13 10:41:47,803 - mmselfsup - INFO - Epoch [1][100/427]	lr: 1.118e-01, eta: 10:33:01, time: 0.683, data_time: 0.001, memory: 25346, loss: 3.2023
2023-02-13 10:42:21,910 - mmselfsup - INFO - Epoch [1][150/427]	lr: 1.680e-01, eta: 9:42:41, time: 0.682, data_time: 0.001, memory: 25346, loss: 2.3450
2023-02-13 10:42:56,037 - mmselfsup - INFO - Epoch [1][200/427]	lr: 2.242e-01, eta: 9:17:24, time: 0.683, data_time: 0.001, memory: 25346, loss: 2.0564
2023-02-13 10:43:30,154 - mmselfsup - INFO - Epoch [1][250/427]	lr: 2.804e-01, eta: 9:01:58, time: 0.682, data_time: 0.001, memory: 25346, loss: 1.8791
2023-02-13 10:44:04,244 - mmselfsup - INFO - Epoch [1][300/427]	lr: 3.366e-01, eta: 8:51:23, time: 0.682, data_time: 0.001, memory: 25346, loss: 1.7417
2023-02-13 10:44:38,218 - mmselfsup - INFO - Epoch [1][350/427]	lr: 3.928e-01, eta: 8:43:27, time: 0.680, data_time: 0.001, memory: 25346, loss: 1.6385
2023-02-13 10:45:12,086 - mmselfsup - INFO - Epoch [1][400/427]	lr: 4.490e-01, eta: 8:37:11, time: 0.677, data_time: 0.001, memory: 25346, loss: 1.5782
2023-02-13 10:46:14,632 - mmselfsup - INFO - Epoch [2][50/427]	lr: 5.354e-01, eta: 8:14:15, time: 0.832, data_time: 0.125, memory: 25346, loss: 1.4521
2023-02-13 10:46:48,730 - mmselfsup - INFO - Epoch [2][100/427]	lr: 5.916e-01, eta: 8:12:19, time: 0.682, data_time: 0.003, memory: 25346, loss: 1.3648
2023-02-13 10:47:22,967 - mmselfsup - INFO - Epoch [2][150/427]	lr: 6.477e-01, eta: 8:10:46, time: 0.685, data_time: 0.003, memory: 25346, loss: 1.2548
2023-02-13 10:47:57,213 - mmselfsup - INFO - Epoch [2][200/427]	lr: 7.039e-01, eta: 8:09:24, time: 0.685, data_time: 0.003, memory: 25346, loss: 1.1967
2023-02-13 10:48:31,376 - mmselfsup - INFO - Epoch [2][250/427]	lr: 7.601e-01, eta: 8:08:03, time: 0.683, data_time: 0.003, memory: 25346, loss: 1.1240
2023-02-13 10:49:05,596 - mmselfsup - INFO - Epoch [2][300/427]	lr: 8.163e-01, eta: 8:06:53, time: 0.684, data_time: 0.003, memory: 25346, loss: 1.0293
2023-02-13 10:49:39,710 - mmselfsup - INFO - Epoch [2][350/427]	lr: 8.725e-01, eta: 8:05:41, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.9913
2023-02-13 10:50:13,985 - mmselfsup - INFO - Epoch [2][400/427]	lr: 9.287e-01, eta: 8:04:42, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.9385
2023-02-13 10:51:13,513 - mmselfsup - INFO - Epoch [3][50/427]	lr: 1.014e+00, eta: 7:54:45, time: 0.835, data_time: 0.121, memory: 25346, loss: 0.8477
2023-02-13 10:51:47,630 - mmselfsup - INFO - Epoch [3][100/427]	lr: 1.071e+00, eta: 7:54:13, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7879
2023-02-13 10:52:21,538 - mmselfsup - INFO - Epoch [3][150/427]	lr: 1.127e+00, eta: 7:53:32, time: 0.678, data_time: 0.001, memory: 25346, loss: 0.7215
2023-02-13 10:52:55,557 - mmselfsup - INFO - Epoch [3][200/427]	lr: 1.183e+00, eta: 7:52:56, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.6852
2023-02-13 10:53:29,528 - mmselfsup - INFO - Epoch [3][250/427]	lr: 1.239e+00, eta: 7:52:18, time: 0.679, data_time: 0.001, memory: 25346, loss: 0.6579
2023-02-13 10:54:03,463 - mmselfsup - INFO - Epoch [3][300/427]	lr: 1.295e+00, eta: 7:51:39, time: 0.678, data_time: 0.001, memory: 25346, loss: 0.6323
2023-02-13 10:54:37,470 - mmselfsup - INFO - Epoch [3][350/427]	lr: 1.351e+00, eta: 7:51:04, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.6045
2023-02-13 10:55:11,490 - mmselfsup - INFO - Epoch [3][400/427]	lr: 1.407e+00, eta: 7:50:28, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.5531
2023-02-13 10:56:10,903 - mmselfsup - INFO - Epoch [4][50/427]	lr: 1.492e+00, eta: 7:44:00, time: 0.833, data_time: 0.130, memory: 25346, loss: 0.5233
2023-02-13 10:56:45,028 - mmselfsup - INFO - Epoch [4][100/427]	lr: 1.548e+00, eta: 7:43:41, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.4843
2023-02-13 10:57:19,202 - mmselfsup - INFO - Epoch [4][150/427]	lr: 1.604e+00, eta: 7:43:21, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.4708
2023-02-13 10:57:53,359 - mmselfsup - INFO - Epoch [4][200/427]	lr: 1.660e+00, eta: 7:43:01, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.4619
2023-02-13 10:58:27,345 - mmselfsup - INFO - Epoch [4][250/427]	lr: 1.716e+00, eta: 7:42:35, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.4526
2023-02-13 10:59:01,392 - mmselfsup - INFO - Epoch [4][300/427]	lr: 1.772e+00, eta: 7:42:10, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.4297
2023-02-13 10:59:35,566 - mmselfsup - INFO - Epoch [4][350/427]	lr: 1.829e+00, eta: 7:41:48, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.4215
2023-02-13 11:00:09,650 - mmselfsup - INFO - Epoch [4][400/427]	lr: 1.885e+00, eta: 7:41:22, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.4165
2023-02-13 11:01:08,915 - mmselfsup - INFO - Epoch [5][50/427]	lr: 1.968e+00, eta: 7:36:27, time: 0.830, data_time: 0.128, memory: 25346, loss: 0.4120
2023-02-13 11:01:42,977 - mmselfsup - INFO - Epoch [5][100/427]	lr: 2.024e+00, eta: 7:36:07, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.3912
2023-02-13 11:02:16,995 - mmselfsup - INFO - Epoch [5][150/427]	lr: 2.080e+00, eta: 7:35:46, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.3955
2023-02-13 11:02:51,058 - mmselfsup - INFO - Epoch [5][200/427]	lr: 2.136e+00, eta: 7:35:24, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.3910
2023-02-13 11:03:25,131 - mmselfsup - INFO - Epoch [5][250/427]	lr: 2.191e+00, eta: 7:35:03, time: 0.682, data_time: 0.002, memory: 25346, loss: 0.3834
2023-02-13 11:03:59,306 - mmselfsup - INFO - Epoch [5][300/427]	lr: 2.247e+00, eta: 7:34:43, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3852
2023-02-13 11:04:33,366 - mmselfsup - INFO - Epoch [5][350/427]	lr: 2.303e+00, eta: 7:34:20, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.3741
2023-02-13 11:05:07,530 - mmselfsup - INFO - Epoch [5][400/427]	lr: 2.359e+00, eta: 7:33:59, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.3749
2023-02-13 11:06:06,661 - mmselfsup - INFO - Epoch [6][50/427]	lr: 2.440e+00, eta: 7:29:58, time: 0.829, data_time: 0.138, memory: 25346, loss: 0.3700
2023-02-13 11:06:40,830 - mmselfsup - INFO - Epoch [6][100/427]	lr: 2.496e+00, eta: 7:29:40, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.3674
2023-02-13 11:07:14,791 - mmselfsup - INFO - Epoch [6][150/427]	lr: 2.552e+00, eta: 7:29:18, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.3711
2023-02-13 11:07:48,958 - mmselfsup - INFO - Epoch [6][200/427]	lr: 2.608e+00, eta: 7:28:58, time: 0.683, data_time: 0.002, memory: 25346, loss: 0.3673
2023-02-13 11:08:23,132 - mmselfsup - INFO - Epoch [6][250/427]	lr: 2.664e+00, eta: 7:28:38, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.3711
2023-02-13 11:08:57,294 - mmselfsup - INFO - Epoch [6][300/427]	lr: 2.719e+00, eta: 7:28:18, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.3669
2023-02-13 11:09:31,471 - mmselfsup - INFO - Epoch [6][350/427]	lr: 2.775e+00, eta: 7:27:56, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.3723
2023-02-13 11:10:05,654 - mmselfsup - INFO - Epoch [6][400/427]	lr: 2.831e+00, eta: 7:27:36, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3675
2023-02-13 11:11:05,161 - mmselfsup - INFO - Epoch [7][50/427]	lr: 2.909e+00, eta: 7:24:15, time: 0.836, data_time: 0.114, memory: 25346, loss: 0.3716
2023-02-13 11:11:39,345 - mmselfsup - INFO - Epoch [7][100/427]	lr: 2.965e+00, eta: 7:23:56, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3721
2023-02-13 11:12:13,439 - mmselfsup - INFO - Epoch [7][150/427]	lr: 3.021e+00, eta: 7:23:35, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.3746
2023-02-13 11:12:47,522 - mmselfsup - INFO - Epoch [7][200/427]	lr: 3.076e+00, eta: 7:23:13, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.3764
2023-02-13 11:13:21,627 - mmselfsup - INFO - Epoch [7][250/427]	lr: 3.132e+00, eta: 7:22:51, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.3795
2023-02-13 11:13:55,921 - mmselfsup - INFO - Epoch [7][300/427]	lr: 3.188e+00, eta: 7:22:32, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3786
2023-02-13 11:14:29,981 - mmselfsup - INFO - Epoch [7][350/427]	lr: 3.243e+00, eta: 7:22:08, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.3790
2023-02-13 11:15:04,082 - mmselfsup - INFO - Epoch [7][400/427]	lr: 3.299e+00, eta: 7:21:45, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.3847
2023-02-13 11:16:03,627 - mmselfsup - INFO - Epoch [8][50/427]	lr: 3.374e+00, eta: 7:18:49, time: 0.835, data_time: 0.120, memory: 25346, loss: 0.3872
2023-02-13 11:16:37,804 - mmselfsup - INFO - Epoch [8][100/427]	lr: 3.430e+00, eta: 7:18:28, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3887
2023-02-13 11:17:11,985 - mmselfsup - INFO - Epoch [8][150/427]	lr: 3.485e+00, eta: 7:18:07, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.3914
2023-02-13 11:17:46,192 - mmselfsup - INFO - Epoch [8][200/427]	lr: 3.541e+00, eta: 7:17:46, time: 0.685, data_time: 0.004, memory: 25346, loss: 0.3943
2023-02-13 11:18:20,349 - mmselfsup - INFO - Epoch [8][250/427]	lr: 3.596e+00, eta: 7:17:24, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.3944
2023-02-13 11:18:54,615 - mmselfsup - INFO - Epoch [8][300/427]	lr: 3.652e+00, eta: 7:17:03, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3994
2023-02-13 11:19:28,957 - mmselfsup - INFO - Epoch [8][350/427]	lr: 3.707e+00, eta: 7:16:43, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.4040
2023-02-13 11:20:03,142 - mmselfsup - INFO - Epoch [8][400/427]	lr: 3.763e+00, eta: 7:16:20, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.4085
2023-02-13 11:21:02,872 - mmselfsup - INFO - Epoch [9][50/427]	lr: 3.834e+00, eta: 7:13:43, time: 0.839, data_time: 0.127, memory: 25346, loss: 0.4135
2023-02-13 11:21:37,148 - mmselfsup - INFO - Epoch [9][100/427]	lr: 3.889e+00, eta: 7:13:22, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.4176
2023-02-13 11:22:11,310 - mmselfsup - INFO - Epoch [9][150/427]	lr: 3.945e+00, eta: 7:12:59, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.4200
2023-02-13 11:22:45,431 - mmselfsup - INFO - Epoch [9][200/427]	lr: 4.000e+00, eta: 7:12:36, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.4224
2023-02-13 11:23:19,579 - mmselfsup - INFO - Epoch [9][250/427]	lr: 4.055e+00, eta: 7:12:13, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.4287
2023-02-13 11:23:53,818 - mmselfsup - INFO - Epoch [9][300/427]	lr: 4.111e+00, eta: 7:11:51, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4366
2023-02-13 11:24:28,089 - mmselfsup - INFO - Epoch [9][350/427]	lr: 4.166e+00, eta: 7:11:28, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4406
2023-02-13 11:25:02,333 - mmselfsup - INFO - Epoch [9][400/427]	lr: 4.221e+00, eta: 7:11:05, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.4386
2023-02-13 11:26:02,228 - mmselfsup - INFO - Epoch [10][50/427]	lr: 4.288e+00, eta: 7:08:42, time: 0.841, data_time: 0.128, memory: 25346, loss: 0.4473
2023-02-13 11:26:36,510 - mmselfsup - INFO - Epoch [10][100/427]	lr: 4.343e+00, eta: 7:08:20, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4486
2023-02-13 11:27:10,793 - mmselfsup - INFO - Epoch [10][150/427]	lr: 4.398e+00, eta: 7:07:58, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4555
2023-02-13 11:27:45,056 - mmselfsup - INFO - Epoch [10][200/427]	lr: 4.454e+00, eta: 7:07:35, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.4609
2023-02-13 11:28:19,527 - mmselfsup - INFO - Epoch [10][250/427]	lr: 4.509e+00, eta: 7:07:14, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.4613
2023-02-13 11:28:53,804 - mmselfsup - INFO - Epoch [10][300/427]	lr: 4.564e+00, eta: 7:06:51, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4657
2023-02-13 11:29:28,120 - mmselfsup - INFO - Epoch [10][350/427]	lr: 4.619e+00, eta: 7:06:28, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4680
2023-02-13 11:30:02,533 - mmselfsup - INFO - Epoch [10][400/427]	lr: 4.674e+00, eta: 7:06:05, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.4727
2023-02-13 11:30:20,341 - mmselfsup - INFO - Saving checkpoint at 10 epochs
2023-02-13 11:31:03,079 - mmselfsup - INFO - Epoch [11][50/427]	lr: 4.683e+00, eta: 7:03:51, time: 0.836, data_time: 0.145, memory: 25346, loss: 0.4832
2023-02-13 11:31:37,303 - mmselfsup - INFO - Epoch [11][100/427]	lr: 4.683e+00, eta: 7:03:27, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4897
2023-02-13 11:32:11,560 - mmselfsup - INFO - Epoch [11][150/427]	lr: 4.683e+00, eta: 7:03:04, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4865
2023-02-13 11:32:45,701 - mmselfsup - INFO - Epoch [11][200/427]	lr: 4.683e+00, eta: 7:02:39, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.4915
2023-02-13 11:33:20,060 - mmselfsup - INFO - Epoch [11][250/427]	lr: 4.683e+00, eta: 7:02:16, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.4945
2023-02-13 11:33:54,345 - mmselfsup - INFO - Epoch [11][300/427]	lr: 4.683e+00, eta: 7:01:52, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.5008
2023-02-13 11:34:28,605 - mmselfsup - INFO - Epoch [11][350/427]	lr: 4.683e+00, eta: 7:01:28, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.5067
2023-02-13 11:35:02,877 - mmselfsup - INFO - Epoch [11][400/427]	lr: 4.683e+00, eta: 7:01:03, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.5080
2023-02-13 11:36:02,348 - mmselfsup - INFO - Epoch [12][50/427]	lr: 4.658e+00, eta: 6:58:57, time: 0.834, data_time: 0.095, memory: 25346, loss: 0.5156
2023-02-13 11:36:36,680 - mmselfsup - INFO - Epoch [12][100/427]	lr: 4.658e+00, eta: 6:58:33, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.5160
2023-02-13 11:37:11,004 - mmselfsup - INFO - Epoch [12][150/427]	lr: 4.658e+00, eta: 6:58:09, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5203
2023-02-13 11:37:45,487 - mmselfsup - INFO - Epoch [12][200/427]	lr: 4.658e+00, eta: 6:57:46, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.5250
2023-02-13 11:38:19,894 - mmselfsup - INFO - Epoch [12][250/427]	lr: 4.658e+00, eta: 6:57:23, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.5284
2023-02-13 11:38:54,243 - mmselfsup - INFO - Epoch [12][300/427]	lr: 4.658e+00, eta: 6:56:59, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.5369
2023-02-13 11:39:28,563 - mmselfsup - INFO - Epoch [12][350/427]	lr: 4.658e+00, eta: 6:56:34, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5408
2023-02-13 11:40:02,882 - mmselfsup - INFO - Epoch [12][400/427]	lr: 4.658e+00, eta: 6:56:09, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5394
2023-02-13 11:41:02,878 - mmselfsup - INFO - Epoch [13][50/427]	lr: 4.631e+00, eta: 6:54:13, time: 0.843, data_time: 0.096, memory: 25346, loss: 0.5471
2023-02-13 11:41:37,297 - mmselfsup - INFO - Epoch [13][100/427]	lr: 4.631e+00, eta: 6:53:49, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.5513
2023-02-13 11:42:11,693 - mmselfsup - INFO - Epoch [13][150/427]	lr: 4.631e+00, eta: 6:53:25, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.5618
2023-02-13 11:42:46,122 - mmselfsup - INFO - Epoch [13][200/427]	lr: 4.631e+00, eta: 6:53:01, time: 0.689, data_time: 0.001, memory: 25346, loss: 0.5612
2023-02-13 11:43:20,590 - mmselfsup - INFO - Epoch [13][250/427]	lr: 4.631e+00, eta: 6:52:37, time: 0.689, data_time: 0.001, memory: 25346, loss: 0.5649
2023-02-13 11:43:54,968 - mmselfsup - INFO - Epoch [13][300/427]	lr: 4.631e+00, eta: 6:52:12, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.5731
2023-02-13 11:44:29,315 - mmselfsup - INFO - Epoch [13][350/427]	lr: 4.631e+00, eta: 6:51:47, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.5754
2023-02-13 11:45:03,741 - mmselfsup - INFO - Epoch [13][400/427]	lr: 4.631e+00, eta: 6:51:22, time: 0.689, data_time: 0.001, memory: 25346, loss: 0.5737
2023-02-13 11:46:03,489 - mmselfsup - INFO - Epoch [14][50/427]	lr: 4.603e+00, eta: 6:49:30, time: 0.838, data_time: 0.126, memory: 25346, loss: 0.5892
2023-02-13 11:46:38,091 - mmselfsup - INFO - Epoch [14][100/427]	lr: 4.603e+00, eta: 6:49:06, time: 0.693, data_time: 0.004, memory: 25346, loss: 0.6062
2023-02-13 11:47:12,566 - mmselfsup - INFO - Epoch [14][150/427]	lr: 4.603e+00, eta: 6:48:42, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.5978
2023-02-13 11:47:47,043 - mmselfsup - INFO - Epoch [14][200/427]	lr: 4.603e+00, eta: 6:48:17, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.6009
2023-02-13 11:48:21,571 - mmselfsup - INFO - Epoch [14][250/427]	lr: 4.603e+00, eta: 6:47:53, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.6001
2023-02-13 11:48:56,080 - mmselfsup - INFO - Epoch [14][300/427]	lr: 4.603e+00, eta: 6:47:28, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.6086
2023-02-13 11:49:30,453 - mmselfsup - INFO - Epoch [14][350/427]	lr: 4.603e+00, eta: 6:47:03, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.6061
2023-02-13 11:50:04,805 - mmselfsup - INFO - Epoch [14][400/427]	lr: 4.603e+00, eta: 6:46:37, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.6188
2023-02-13 11:51:04,769 - mmselfsup - INFO - Epoch [15][50/427]	lr: 4.572e+00, eta: 6:44:51, time: 0.843, data_time: 0.117, memory: 25346, loss: 0.6144
2023-02-13 11:51:39,267 - mmselfsup - INFO - Epoch [15][100/427]	lr: 4.572e+00, eta: 6:44:26, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.6158
2023-02-13 11:52:13,743 - mmselfsup - INFO - Epoch [15][150/427]	lr: 4.572e+00, eta: 6:44:01, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.6209
2023-02-13 11:52:48,070 - mmselfsup - INFO - Epoch [15][200/427]	lr: 4.572e+00, eta: 6:43:35, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.6246
2023-02-13 11:53:22,529 - mmselfsup - INFO - Epoch [15][250/427]	lr: 4.572e+00, eta: 6:43:09, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.6322
2023-02-13 11:53:57,091 - mmselfsup - INFO - Epoch [15][300/427]	lr: 4.572e+00, eta: 6:42:44, time: 0.691, data_time: 0.001, memory: 25346, loss: 0.6347
2023-02-13 11:54:31,402 - mmselfsup - INFO - Epoch [15][350/427]	lr: 4.572e+00, eta: 6:42:18, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.6384
2023-02-13 11:55:05,669 - mmselfsup - INFO - Epoch [15][400/427]	lr: 4.572e+00, eta: 6:41:51, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.6394
2023-02-13 11:56:05,447 - mmselfsup - INFO - Epoch [16][50/427]	lr: 4.538e+00, eta: 6:40:09, time: 0.840, data_time: 0.089, memory: 25346, loss: 0.6406
2023-02-13 11:56:40,004 - mmselfsup - INFO - Epoch [16][100/427]	lr: 4.538e+00, eta: 6:39:43, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.6513
2023-02-13 11:57:14,591 - mmselfsup - INFO - Epoch [16][150/427]	lr: 4.538e+00, eta: 6:39:18, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.6531
2023-02-13 11:57:49,058 - mmselfsup - INFO - Epoch [16][200/427]	lr: 4.538e+00, eta: 6:38:52, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.6506
2023-02-13 11:58:23,539 - mmselfsup - INFO - Epoch [16][250/427]	lr: 4.538e+00, eta: 6:38:26, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.6575
2023-02-13 11:58:57,975 - mmselfsup - INFO - Epoch [16][300/427]	lr: 4.538e+00, eta: 6:38:00, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.6652
2023-02-13 11:59:32,464 - mmselfsup - INFO - Epoch [16][350/427]	lr: 4.538e+00, eta: 6:37:34, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.6742
2023-02-13 12:00:06,784 - mmselfsup - INFO - Epoch [16][400/427]	lr: 4.538e+00, eta: 6:37:07, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.6792
2023-02-13 12:01:06,822 - mmselfsup - INFO - Epoch [17][50/427]	lr: 4.503e+00, eta: 6:35:29, time: 0.845, data_time: 0.130, memory: 25346, loss: 0.6788
2023-02-13 12:01:41,319 - mmselfsup - INFO - Epoch [17][100/427]	lr: 4.503e+00, eta: 6:35:03, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.6978
2023-02-13 12:02:15,703 - mmselfsup - INFO - Epoch [17][150/427]	lr: 4.503e+00, eta: 6:34:37, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.6940
2023-02-13 12:02:50,219 - mmselfsup - INFO - Epoch [17][200/427]	lr: 4.503e+00, eta: 6:34:10, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.6938
2023-02-13 12:03:24,629 - mmselfsup - INFO - Epoch [17][250/427]	lr: 4.503e+00, eta: 6:33:43, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.6927
2023-02-13 12:03:59,099 - mmselfsup - INFO - Epoch [17][300/427]	lr: 4.503e+00, eta: 6:33:17, time: 0.689, data_time: 0.001, memory: 25346, loss: 0.6854
2023-02-13 12:04:33,484 - mmselfsup - INFO - Epoch [17][350/427]	lr: 4.503e+00, eta: 6:32:50, time: 0.689, data_time: 0.001, memory: 25346, loss: 0.7087
2023-02-13 12:05:07,812 - mmselfsup - INFO - Epoch [17][400/427]	lr: 4.503e+00, eta: 6:32:22, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.7163
2023-02-13 12:06:07,717 - mmselfsup - INFO - Epoch [18][50/427]	lr: 4.466e+00, eta: 6:30:47, time: 0.841, data_time: 0.130, memory: 25346, loss: 0.6954
2023-02-13 12:06:42,405 - mmselfsup - INFO - Epoch [18][100/427]	lr: 4.466e+00, eta: 6:30:21, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.6923
2023-02-13 12:07:16,872 - mmselfsup - INFO - Epoch [18][150/427]	lr: 4.466e+00, eta: 6:29:55, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.6986
2023-02-13 12:07:51,509 - mmselfsup - INFO - Epoch [18][200/427]	lr: 4.466e+00, eta: 6:29:29, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.6998
2023-02-13 12:08:26,024 - mmselfsup - INFO - Epoch [18][250/427]	lr: 4.466e+00, eta: 6:29:02, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7016
2023-02-13 12:09:00,544 - mmselfsup - INFO - Epoch [18][300/427]	lr: 4.466e+00, eta: 6:28:35, time: 0.690, data_time: 0.002, memory: 25346, loss: 0.7048
2023-02-13 12:09:35,053 - mmselfsup - INFO - Epoch [18][350/427]	lr: 4.466e+00, eta: 6:28:08, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7139
2023-02-13 12:10:09,681 - mmselfsup - INFO - Epoch [18][400/427]	lr: 4.466e+00, eta: 6:27:41, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.7185
2023-02-13 12:11:09,350 - mmselfsup - INFO - Epoch [19][50/427]	lr: 4.426e+00, eta: 6:26:09, time: 0.838, data_time: 0.119, memory: 25346, loss: 0.7116
2023-02-13 12:11:43,930 - mmselfsup - INFO - Epoch [19][100/427]	lr: 4.426e+00, eta: 6:25:42, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7049
2023-02-13 12:12:18,491 - mmselfsup - INFO - Epoch [19][150/427]	lr: 4.426e+00, eta: 6:25:15, time: 0.691, data_time: 0.001, memory: 25346, loss: 0.7140
2023-02-13 12:12:53,083 - mmselfsup - INFO - Epoch [19][200/427]	lr: 4.426e+00, eta: 6:24:48, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7111
2023-02-13 12:13:27,696 - mmselfsup - INFO - Epoch [19][250/427]	lr: 4.426e+00, eta: 6:24:21, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7170
2023-02-13 12:14:02,260 - mmselfsup - INFO - Epoch [19][300/427]	lr: 4.426e+00, eta: 6:23:54, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7103
2023-02-13 12:14:36,805 - mmselfsup - INFO - Epoch [19][350/427]	lr: 4.426e+00, eta: 6:23:27, time: 0.691, data_time: 0.001, memory: 25346, loss: 0.7133
2023-02-13 12:15:11,162 - mmselfsup - INFO - Epoch [19][400/427]	lr: 4.426e+00, eta: 6:22:59, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.7226
2023-02-13 12:16:10,817 - mmselfsup - INFO - Epoch [20][50/427]	lr: 4.385e+00, eta: 6:21:28, time: 0.836, data_time: 0.095, memory: 25346, loss: 0.7186
2023-02-13 12:16:45,401 - mmselfsup - INFO - Epoch [20][100/427]	lr: 4.385e+00, eta: 6:21:01, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7243
2023-02-13 12:17:19,701 - mmselfsup - INFO - Epoch [20][150/427]	lr: 4.385e+00, eta: 6:20:33, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.7247
2023-02-13 12:17:54,277 - mmselfsup - INFO - Epoch [20][200/427]	lr: 4.385e+00, eta: 6:20:06, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7314
2023-02-13 12:18:28,756 - mmselfsup - INFO - Epoch [20][250/427]	lr: 4.385e+00, eta: 6:19:38, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.7541
2023-02-13 12:19:03,135 - mmselfsup - INFO - Epoch [20][300/427]	lr: 4.385e+00, eta: 6:19:10, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.7345
2023-02-13 12:19:37,520 - mmselfsup - INFO - Epoch [20][350/427]	lr: 4.385e+00, eta: 6:18:42, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.7344
2023-02-13 12:20:12,006 - mmselfsup - INFO - Epoch [20][400/427]	lr: 4.385e+00, eta: 6:18:14, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.7502
2023-02-13 12:20:29,793 - mmselfsup - INFO - Saving checkpoint at 20 epochs
2023-02-13 12:21:11,762 - mmselfsup - INFO - Epoch [21][50/427]	lr: 4.342e+00, eta: 6:16:42, time: 0.820, data_time: 0.116, memory: 25346, loss: 0.7441
2023-02-13 12:21:46,245 - mmselfsup - INFO - Epoch [21][100/427]	lr: 4.342e+00, eta: 6:16:15, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.7413
2023-02-13 12:22:20,655 - mmselfsup - INFO - Epoch [21][150/427]	lr: 4.342e+00, eta: 6:15:46, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.7375
2023-02-13 12:22:55,131 - mmselfsup - INFO - Epoch [21][200/427]	lr: 4.342e+00, eta: 6:15:18, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.7397
2023-02-13 12:23:29,476 - mmselfsup - INFO - Epoch [21][250/427]	lr: 4.342e+00, eta: 6:14:50, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.7390
2023-02-13 12:24:03,886 - mmselfsup - INFO - Epoch [21][300/427]	lr: 4.342e+00, eta: 6:14:21, time: 0.688, data_time: 0.000, memory: 25346, loss: 0.7442
2023-02-13 12:24:38,312 - mmselfsup - INFO - Epoch [21][350/427]	lr: 4.342e+00, eta: 6:13:53, time: 0.689, data_time: 0.001, memory: 25346, loss: 0.7532
2023-02-13 12:25:12,796 - mmselfsup - INFO - Epoch [21][400/427]	lr: 4.342e+00, eta: 6:13:25, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.7499
2023-02-13 12:26:12,863 - mmselfsup - INFO - Epoch [22][50/427]	lr: 4.296e+00, eta: 6:12:01, time: 0.845, data_time: 0.091, memory: 25346, loss: 0.7502
2023-02-13 12:26:47,537 - mmselfsup - INFO - Epoch [22][100/427]	lr: 4.296e+00, eta: 6:11:33, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.7484
2023-02-13 12:27:22,098 - mmselfsup - INFO - Epoch [22][150/427]	lr: 4.296e+00, eta: 6:11:05, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7446
2023-02-13 12:27:56,512 - mmselfsup - INFO - Epoch [22][200/427]	lr: 4.296e+00, eta: 6:10:37, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.7802
2023-02-13 12:28:30,894 - mmselfsup - INFO - Epoch [22][250/427]	lr: 4.296e+00, eta: 6:10:08, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.8356
2023-02-13 12:29:05,347 - mmselfsup - INFO - Epoch [22][300/427]	lr: 4.296e+00, eta: 6:09:39, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.8069
2023-02-13 12:29:39,935 - mmselfsup - INFO - Epoch [22][350/427]	lr: 4.296e+00, eta: 6:09:11, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.7952
2023-02-13 12:30:14,497 - mmselfsup - INFO - Epoch [22][400/427]	lr: 4.296e+00, eta: 6:08:43, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7730
2023-02-13 12:31:14,503 - mmselfsup - INFO - Epoch [23][50/427]	lr: 4.249e+00, eta: 6:07:21, time: 0.844, data_time: 0.137, memory: 25346, loss: 0.7761
2023-02-13 12:31:49,020 - mmselfsup - INFO - Epoch [23][100/427]	lr: 4.249e+00, eta: 6:06:52, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.7818
2023-02-13 12:32:23,604 - mmselfsup - INFO - Epoch [23][150/427]	lr: 4.249e+00, eta: 6:06:24, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7845
2023-02-13 12:32:58,101 - mmselfsup - INFO - Epoch [23][200/427]	lr: 4.249e+00, eta: 6:05:56, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.7772
2023-02-13 12:33:32,647 - mmselfsup - INFO - Epoch [23][250/427]	lr: 4.249e+00, eta: 6:05:27, time: 0.691, data_time: 0.001, memory: 25346, loss: 0.7868
2023-02-13 12:34:07,104 - mmselfsup - INFO - Epoch [23][300/427]	lr: 4.249e+00, eta: 6:04:58, time: 0.689, data_time: 0.001, memory: 25346, loss: 0.8576
2023-02-13 12:34:41,697 - mmselfsup - INFO - Epoch [23][350/427]	lr: 4.249e+00, eta: 6:04:30, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.8414
2023-02-13 12:35:16,272 - mmselfsup - INFO - Epoch [23][400/427]	lr: 4.249e+00, eta: 6:04:02, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7780
2023-02-13 12:36:16,181 - mmselfsup - INFO - Epoch [24][50/427]	lr: 4.200e+00, eta: 6:02:40, time: 0.842, data_time: 0.088, memory: 25346, loss: 0.7901
2023-02-13 12:36:50,680 - mmselfsup - INFO - Epoch [24][100/427]	lr: 4.200e+00, eta: 6:02:12, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.8046
2023-02-13 12:37:25,262 - mmselfsup - INFO - Epoch [24][150/427]	lr: 4.200e+00, eta: 6:01:43, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.7797
2023-02-13 12:37:59,758 - mmselfsup - INFO - Epoch [24][200/427]	lr: 4.200e+00, eta: 6:01:15, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.7731
2023-02-13 12:38:34,307 - mmselfsup - INFO - Epoch [24][250/427]	lr: 4.200e+00, eta: 6:00:46, time: 0.691, data_time: 0.002, memory: 25346, loss: 0.7782
2023-02-13 12:39:08,752 - mmselfsup - INFO - Epoch [24][300/427]	lr: 4.200e+00, eta: 6:00:17, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.7776
2023-02-13 12:39:43,069 - mmselfsup - INFO - Epoch [24][350/427]	lr: 4.200e+00, eta: 5:59:47, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.7639
2023-02-13 12:40:17,537 - mmselfsup - INFO - Epoch [24][400/427]	lr: 4.200e+00, eta: 5:59:18, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.7718
2023-02-13 12:41:17,391 - mmselfsup - INFO - Epoch [25][50/427]	lr: 4.150e+00, eta: 5:57:59, time: 0.841, data_time: 0.136, memory: 25346, loss: 0.7730
2023-02-13 12:41:52,019 - mmselfsup - INFO - Epoch [25][100/427]	lr: 4.150e+00, eta: 5:57:30, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.7942
2023-02-13 12:42:26,505 - mmselfsup - INFO - Epoch [25][150/427]	lr: 4.150e+00, eta: 5:57:01, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.7773
2023-02-13 12:43:00,876 - mmselfsup - INFO - Epoch [25][200/427]	lr: 4.150e+00, eta: 5:56:31, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.7686
2023-02-13 12:43:35,418 - mmselfsup - INFO - Epoch [25][250/427]	lr: 4.150e+00, eta: 5:56:03, time: 0.691, data_time: 0.001, memory: 25346, loss: 0.7600
2023-02-13 12:44:09,819 - mmselfsup - INFO - Epoch [25][300/427]	lr: 4.150e+00, eta: 5:55:33, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.7656
2023-02-13 12:44:44,349 - mmselfsup - INFO - Epoch [25][350/427]	lr: 4.150e+00, eta: 5:55:04, time: 0.690, data_time: 0.002, memory: 25346, loss: 0.7650
2023-02-13 12:45:18,906 - mmselfsup - INFO - Epoch [25][400/427]	lr: 4.150e+00, eta: 5:54:35, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7768
2023-02-13 12:46:18,867 - mmselfsup - INFO - Epoch [26][50/427]	lr: 4.097e+00, eta: 5:53:17, time: 0.844, data_time: 0.132, memory: 25346, loss: 0.7594
2023-02-13 12:46:53,502 - mmselfsup - INFO - Epoch [26][100/427]	lr: 4.097e+00, eta: 5:52:49, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.7688
2023-02-13 12:47:28,029 - mmselfsup - INFO - Epoch [26][150/427]	lr: 4.097e+00, eta: 5:52:19, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7828
2023-02-13 12:48:02,554 - mmselfsup - INFO - Epoch [26][200/427]	lr: 4.097e+00, eta: 5:51:50, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.7686
2023-02-13 12:48:37,074 - mmselfsup - INFO - Epoch [26][250/427]	lr: 4.097e+00, eta: 5:51:21, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.7620
2023-02-13 12:49:11,605 - mmselfsup - INFO - Epoch [26][300/427]	lr: 4.097e+00, eta: 5:50:52, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7489
2023-02-13 12:49:46,137 - mmselfsup - INFO - Epoch [26][350/427]	lr: 4.097e+00, eta: 5:50:23, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7543
2023-02-13 12:50:20,898 - mmselfsup - INFO - Epoch [26][400/427]	lr: 4.097e+00, eta: 5:49:54, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.7484
2023-02-13 12:51:20,835 - mmselfsup - INFO - Epoch [27][50/427]	lr: 4.043e+00, eta: 5:48:37, time: 0.843, data_time: 0.135, memory: 25346, loss: 0.7427
2023-02-13 12:51:55,444 - mmselfsup - INFO - Epoch [27][100/427]	lr: 4.043e+00, eta: 5:48:08, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7524
2023-02-13 12:52:30,089 - mmselfsup - INFO - Epoch [27][150/427]	lr: 4.043e+00, eta: 5:47:39, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.7490
2023-02-13 12:53:04,756 - mmselfsup - INFO - Epoch [27][200/427]	lr: 4.043e+00, eta: 5:47:10, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.7313
2023-02-13 12:53:39,337 - mmselfsup - INFO - Epoch [27][250/427]	lr: 4.043e+00, eta: 5:46:41, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7338
2023-02-13 12:54:13,938 - mmselfsup - INFO - Epoch [27][300/427]	lr: 4.043e+00, eta: 5:46:12, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7360
2023-02-13 12:54:48,590 - mmselfsup - INFO - Epoch [27][350/427]	lr: 4.043e+00, eta: 5:45:42, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7282
2023-02-13 12:55:23,257 - mmselfsup - INFO - Epoch [27][400/427]	lr: 4.043e+00, eta: 5:45:13, time: 0.694, data_time: 0.002, memory: 25346, loss: 0.7302
2023-02-13 12:56:23,192 - mmselfsup - INFO - Epoch [28][50/427]	lr: 3.987e+00, eta: 5:43:58, time: 0.843, data_time: 0.130, memory: 25346, loss: 0.7358
2023-02-13 12:56:57,719 - mmselfsup - INFO - Epoch [28][100/427]	lr: 3.987e+00, eta: 5:43:29, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.7297
2023-02-13 12:57:32,429 - mmselfsup - INFO - Epoch [28][150/427]	lr: 3.987e+00, eta: 5:42:59, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.7204
2023-02-13 12:58:06,990 - mmselfsup - INFO - Epoch [28][200/427]	lr: 3.987e+00, eta: 5:42:30, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.7264
2023-02-13 12:58:41,673 - mmselfsup - INFO - Epoch [28][250/427]	lr: 3.987e+00, eta: 5:42:01, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.7216
2023-02-13 12:59:16,323 - mmselfsup - INFO - Epoch [28][300/427]	lr: 3.987e+00, eta: 5:41:31, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.7314
2023-02-13 12:59:50,959 - mmselfsup - INFO - Epoch [28][350/427]	lr: 3.987e+00, eta: 5:41:02, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.7313
2023-02-13 13:00:25,686 - mmselfsup - INFO - Epoch [28][400/427]	lr: 3.987e+00, eta: 5:40:33, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.7140
2023-02-13 13:01:25,877 - mmselfsup - INFO - Epoch [29][50/427]	lr: 3.930e+00, eta: 5:39:19, time: 0.846, data_time: 0.113, memory: 25346, loss: 0.7288
2023-02-13 13:02:00,328 - mmselfsup - INFO - Epoch [29][100/427]	lr: 3.930e+00, eta: 5:38:49, time: 0.689, data_time: 0.001, memory: 25346, loss: 0.8582
2023-02-13 13:02:34,986 - mmselfsup - INFO - Epoch [29][150/427]	lr: 3.930e+00, eta: 5:38:20, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.7754
2023-02-13 13:03:09,583 - mmselfsup - INFO - Epoch [29][200/427]	lr: 3.930e+00, eta: 5:37:50, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.7302
2023-02-13 13:03:44,248 - mmselfsup - INFO - Epoch [29][250/427]	lr: 3.930e+00, eta: 5:37:20, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.7240
2023-02-13 13:04:18,725 - mmselfsup - INFO - Epoch [29][300/427]	lr: 3.930e+00, eta: 5:36:51, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.7891
2023-02-13 13:04:53,318 - mmselfsup - INFO - Epoch [29][350/427]	lr: 3.930e+00, eta: 5:36:21, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7493
2023-02-13 13:05:27,936 - mmselfsup - INFO - Epoch [29][400/427]	lr: 3.930e+00, eta: 5:35:51, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7488
2023-02-13 13:06:27,733 - mmselfsup - INFO - Epoch [30][50/427]	lr: 3.871e+00, eta: 5:34:38, time: 0.838, data_time: 0.090, memory: 25346, loss: 0.7322
2023-02-13 13:07:02,376 - mmselfsup - INFO - Epoch [30][100/427]	lr: 3.871e+00, eta: 5:34:08, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.7055
2023-02-13 13:07:37,044 - mmselfsup - INFO - Epoch [30][150/427]	lr: 3.871e+00, eta: 5:33:38, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.7063
2023-02-13 13:08:11,893 - mmselfsup - INFO - Epoch [30][200/427]	lr: 3.871e+00, eta: 5:33:09, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.6957
2023-02-13 13:08:46,568 - mmselfsup - INFO - Epoch [30][250/427]	lr: 3.871e+00, eta: 5:32:39, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.7444
2023-02-13 13:09:21,181 - mmselfsup - INFO - Epoch [30][300/427]	lr: 3.871e+00, eta: 5:32:10, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.7407
2023-02-13 13:09:55,803 - mmselfsup - INFO - Epoch [30][350/427]	lr: 3.871e+00, eta: 5:31:40, time: 0.692, data_time: 0.004, memory: 25346, loss: 0.7309
2023-02-13 13:10:30,412 - mmselfsup - INFO - Epoch [30][400/427]	lr: 3.871e+00, eta: 5:31:10, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.7276
2023-02-13 13:10:48,247 - mmselfsup - INFO - Saving checkpoint at 30 epochs
2023-02-13 13:11:31,193 - mmselfsup - INFO - Epoch [31][50/427]	lr: 3.811e+00, eta: 5:29:57, time: 0.840, data_time: 0.139, memory: 25346, loss: 0.7194
2023-02-13 13:12:05,773 - mmselfsup - INFO - Epoch [31][100/427]	lr: 3.811e+00, eta: 5:29:28, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.6826
2023-02-13 13:12:40,601 - mmselfsup - INFO - Epoch [31][150/427]	lr: 3.811e+00, eta: 5:28:58, time: 0.697, data_time: 0.000, memory: 25346, loss: 0.6928
2023-02-13 13:13:15,298 - mmselfsup - INFO - Epoch [31][200/427]	lr: 3.811e+00, eta: 5:28:28, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.6971
2023-02-13 13:13:50,028 - mmselfsup - INFO - Epoch [31][250/427]	lr: 3.811e+00, eta: 5:27:59, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.7041
2023-02-13 13:14:24,655 - mmselfsup - INFO - Epoch [31][300/427]	lr: 3.811e+00, eta: 5:27:29, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.6759
2023-02-13 13:14:59,202 - mmselfsup - INFO - Epoch [31][350/427]	lr: 3.811e+00, eta: 5:26:59, time: 0.692, data_time: 0.002, memory: 25346, loss: 0.6970
2023-02-13 13:15:33,778 - mmselfsup - INFO - Epoch [31][400/427]	lr: 3.811e+00, eta: 5:26:28, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.6754
2023-02-13 13:16:33,624 - mmselfsup - INFO - Epoch [32][50/427]	lr: 3.749e+00, eta: 5:25:17, time: 0.840, data_time: 0.091, memory: 25346, loss: 0.6892
2023-02-13 13:17:08,240 - mmselfsup - INFO - Epoch [32][100/427]	lr: 3.749e+00, eta: 5:24:47, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.7117
2023-02-13 13:17:43,067 - mmselfsup - INFO - Epoch [32][150/427]	lr: 3.749e+00, eta: 5:24:17, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.6836
2023-02-13 13:18:17,750 - mmselfsup - INFO - Epoch [32][200/427]	lr: 3.749e+00, eta: 5:23:48, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.6763
2023-02-13 13:18:52,404 - mmselfsup - INFO - Epoch [32][250/427]	lr: 3.749e+00, eta: 5:23:17, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.6655
2023-02-13 13:19:26,998 - mmselfsup - INFO - Epoch [32][300/427]	lr: 3.749e+00, eta: 5:22:47, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.6731
2023-02-13 13:20:01,685 - mmselfsup - INFO - Epoch [32][350/427]	lr: 3.749e+00, eta: 5:22:17, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.6944
2023-02-13 13:20:36,299 - mmselfsup - INFO - Epoch [32][400/427]	lr: 3.749e+00, eta: 5:21:47, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.6971
2023-02-13 13:21:36,222 - mmselfsup - INFO - Epoch [33][50/427]	lr: 3.686e+00, eta: 5:20:37, time: 0.841, data_time: 0.126, memory: 25346, loss: 0.7056
2023-02-13 13:22:10,951 - mmselfsup - INFO - Epoch [33][100/427]	lr: 3.686e+00, eta: 5:20:07, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.6689
2023-02-13 13:22:45,584 - mmselfsup - INFO - Epoch [33][150/427]	lr: 3.686e+00, eta: 5:19:37, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.6701
2023-02-13 13:23:20,169 - mmselfsup - INFO - Epoch [33][200/427]	lr: 3.686e+00, eta: 5:19:06, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.6721
2023-02-13 13:23:54,823 - mmselfsup - INFO - Epoch [33][250/427]	lr: 3.686e+00, eta: 5:18:36, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.7047
2023-02-13 13:24:29,407 - mmselfsup - INFO - Epoch [33][300/427]	lr: 3.686e+00, eta: 5:18:06, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.6663
2023-02-13 13:25:04,020 - mmselfsup - INFO - Epoch [33][350/427]	lr: 3.686e+00, eta: 5:17:35, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.6529
2023-02-13 13:25:38,660 - mmselfsup - INFO - Epoch [33][400/427]	lr: 3.686e+00, eta: 5:17:05, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.6466
2023-02-13 13:26:38,177 - mmselfsup - INFO - Epoch [34][50/427]	lr: 3.622e+00, eta: 5:15:55, time: 0.834, data_time: 0.125, memory: 25346, loss: 0.6731
2023-02-13 13:27:12,872 - mmselfsup - INFO - Epoch [34][100/427]	lr: 3.622e+00, eta: 5:15:25, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.6395
2023-02-13 13:27:47,526 - mmselfsup - INFO - Epoch [34][150/427]	lr: 3.622e+00, eta: 5:14:54, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.6866
2023-02-13 13:28:22,185 - mmselfsup - INFO - Epoch [34][200/427]	lr: 3.622e+00, eta: 5:14:24, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.6629
2023-02-13 13:28:56,751 - mmselfsup - INFO - Epoch [34][250/427]	lr: 3.622e+00, eta: 5:13:53, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.6666
2023-02-13 13:29:31,341 - mmselfsup - INFO - Epoch [34][300/427]	lr: 3.622e+00, eta: 5:13:23, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.6343
2023-02-13 13:30:05,629 - mmselfsup - INFO - Epoch [34][350/427]	lr: 3.622e+00, eta: 5:12:52, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.6324
2023-02-13 13:30:40,163 - mmselfsup - INFO - Epoch [34][400/427]	lr: 3.622e+00, eta: 5:12:21, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.6392
2023-02-13 13:31:40,234 - mmselfsup - INFO - Epoch [35][50/427]	lr: 3.556e+00, eta: 5:11:13, time: 0.844, data_time: 0.132, memory: 25346, loss: 0.6346
2023-02-13 13:32:15,016 - mmselfsup - INFO - Epoch [35][100/427]	lr: 3.556e+00, eta: 5:10:43, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.6141
2023-02-13 13:32:49,922 - mmselfsup - INFO - Epoch [35][150/427]	lr: 3.556e+00, eta: 5:10:13, time: 0.699, data_time: 0.002, memory: 25346, loss: 0.6197
2023-02-13 13:33:24,583 - mmselfsup - INFO - Epoch [35][200/427]	lr: 3.556e+00, eta: 5:09:42, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.6562
2023-02-13 13:33:59,378 - mmselfsup - INFO - Epoch [35][250/427]	lr: 3.556e+00, eta: 5:09:12, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.6597
2023-02-13 13:34:34,070 - mmselfsup - INFO - Epoch [35][300/427]	lr: 3.556e+00, eta: 5:08:42, time: 0.694, data_time: 0.002, memory: 25346, loss: 0.6572
2023-02-13 13:35:08,830 - mmselfsup - INFO - Epoch [35][350/427]	lr: 3.556e+00, eta: 5:08:11, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.6618
2023-02-13 13:35:43,433 - mmselfsup - INFO - Epoch [35][400/427]	lr: 3.556e+00, eta: 5:07:41, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.6315
2023-02-13 13:36:43,247 - mmselfsup - INFO - Epoch [36][50/427]	lr: 3.490e+00, eta: 5:06:33, time: 0.839, data_time: 0.117, memory: 25346, loss: 0.6174
2023-02-13 13:37:17,989 - mmselfsup - INFO - Epoch [36][100/427]	lr: 3.490e+00, eta: 5:06:02, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.6293
2023-02-13 13:37:52,722 - mmselfsup - INFO - Epoch [36][150/427]	lr: 3.490e+00, eta: 5:05:32, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.6495
2023-02-13 13:38:27,524 - mmselfsup - INFO - Epoch [36][200/427]	lr: 3.490e+00, eta: 5:05:01, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.6145
2023-02-13 13:39:02,118 - mmselfsup - INFO - Epoch [36][250/427]	lr: 3.490e+00, eta: 5:04:31, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.6069
2023-02-13 13:39:36,706 - mmselfsup - INFO - Epoch [36][300/427]	lr: 3.490e+00, eta: 5:04:00, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.6549
2023-02-13 13:40:11,330 - mmselfsup - INFO - Epoch [36][350/427]	lr: 3.490e+00, eta: 5:03:29, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.6141
2023-02-13 13:40:45,930 - mmselfsup - INFO - Epoch [36][400/427]	lr: 3.490e+00, eta: 5:02:58, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.6049
2023-02-13 13:41:46,089 - mmselfsup - INFO - Epoch [37][50/427]	lr: 3.422e+00, eta: 5:01:52, time: 0.845, data_time: 0.103, memory: 25346, loss: 0.5936
2023-02-13 13:42:20,763 - mmselfsup - INFO - Epoch [37][100/427]	lr: 3.422e+00, eta: 5:01:21, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.5916
2023-02-13 13:42:55,452 - mmselfsup - INFO - Epoch [37][150/427]	lr: 3.422e+00, eta: 5:00:50, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.5842
2023-02-13 13:43:30,054 - mmselfsup - INFO - Epoch [37][200/427]	lr: 3.422e+00, eta: 5:00:19, time: 0.691, data_time: 0.001, memory: 25346, loss: 0.5905
2023-02-13 13:44:04,799 - mmselfsup - INFO - Epoch [37][250/427]	lr: 3.422e+00, eta: 4:59:49, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.6000
2023-02-13 13:44:39,440 - mmselfsup - INFO - Epoch [37][300/427]	lr: 3.422e+00, eta: 4:59:18, time: 0.693, data_time: 0.002, memory: 25346, loss: 0.5788
2023-02-13 13:45:14,052 - mmselfsup - INFO - Epoch [37][350/427]	lr: 3.422e+00, eta: 4:58:47, time: 0.692, data_time: 0.002, memory: 25346, loss: 0.6040
2023-02-13 13:45:48,678 - mmselfsup - INFO - Epoch [37][400/427]	lr: 3.422e+00, eta: 4:58:16, time: 0.693, data_time: 0.002, memory: 25346, loss: 0.6000
2023-02-13 13:46:48,731 - mmselfsup - INFO - Epoch [38][50/427]	lr: 3.353e+00, eta: 4:57:10, time: 0.843, data_time: 0.090, memory: 25346, loss: 0.5701
2023-02-13 13:47:23,417 - mmselfsup - INFO - Epoch [38][100/427]	lr: 3.353e+00, eta: 4:56:39, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.5716
2023-02-13 13:47:58,040 - mmselfsup - INFO - Epoch [38][150/427]	lr: 3.353e+00, eta: 4:56:08, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.5610
2023-02-13 13:48:32,607 - mmselfsup - INFO - Epoch [38][200/427]	lr: 3.353e+00, eta: 4:55:38, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.5730
2023-02-13 13:49:07,459 - mmselfsup - INFO - Epoch [38][250/427]	lr: 3.353e+00, eta: 4:55:07, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.5778
2023-02-13 13:49:42,226 - mmselfsup - INFO - Epoch [38][300/427]	lr: 3.353e+00, eta: 4:54:36, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.5612
2023-02-13 13:50:16,986 - mmselfsup - INFO - Epoch [38][350/427]	lr: 3.353e+00, eta: 4:54:06, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.5522
2023-02-13 13:50:51,746 - mmselfsup - INFO - Epoch [38][400/427]	lr: 3.353e+00, eta: 4:53:35, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.5623
2023-02-13 13:51:51,678 - mmselfsup - INFO - Epoch [39][50/427]	lr: 3.283e+00, eta: 4:52:29, time: 0.841, data_time: 0.096, memory: 25346, loss: 0.5487
2023-02-13 13:52:26,268 - mmselfsup - INFO - Epoch [39][100/427]	lr: 3.283e+00, eta: 4:51:58, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.5450
2023-02-13 13:53:00,768 - mmselfsup - INFO - Epoch [39][150/427]	lr: 3.283e+00, eta: 4:51:27, time: 0.690, data_time: 0.001, memory: 25346, loss: 0.5497
2023-02-13 13:53:35,450 - mmselfsup - INFO - Epoch [39][200/427]	lr: 3.283e+00, eta: 4:50:56, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.5755
2023-02-13 13:54:09,978 - mmselfsup - INFO - Epoch [39][250/427]	lr: 3.283e+00, eta: 4:50:25, time: 0.691, data_time: 0.002, memory: 25346, loss: 0.5526
2023-02-13 13:54:44,579 - mmselfsup - INFO - Epoch [39][300/427]	lr: 3.283e+00, eta: 4:49:54, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.5604
2023-02-13 13:55:19,162 - mmselfsup - INFO - Epoch [39][350/427]	lr: 3.283e+00, eta: 4:49:23, time: 0.691, data_time: 0.001, memory: 25346, loss: 0.5512
2023-02-13 13:55:53,856 - mmselfsup - INFO - Epoch [39][400/427]	lr: 3.283e+00, eta: 4:48:52, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.5390
2023-02-13 13:56:53,659 - mmselfsup - INFO - Epoch [40][50/427]	lr: 3.213e+00, eta: 4:47:46, time: 0.840, data_time: 0.134, memory: 25346, loss: 0.5308
2023-02-13 13:57:28,570 - mmselfsup - INFO - Epoch [40][100/427]	lr: 3.213e+00, eta: 4:47:16, time: 0.698, data_time: 0.003, memory: 25346, loss: 0.5317
2023-02-13 13:58:03,301 - mmselfsup - INFO - Epoch [40][150/427]	lr: 3.213e+00, eta: 4:46:45, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.5542
2023-02-13 13:58:37,992 - mmselfsup - INFO - Epoch [40][200/427]	lr: 3.213e+00, eta: 4:46:14, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.5356
2023-02-13 13:59:12,647 - mmselfsup - INFO - Epoch [40][250/427]	lr: 3.213e+00, eta: 4:45:43, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.5473
2023-02-13 13:59:47,302 - mmselfsup - INFO - Epoch [40][300/427]	lr: 3.213e+00, eta: 4:45:12, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.5765
2023-02-13 14:00:21,965 - mmselfsup - INFO - Epoch [40][350/427]	lr: 3.213e+00, eta: 4:44:41, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.6997
2023-02-13 14:00:56,694 - mmselfsup - INFO - Epoch [40][400/427]	lr: 3.213e+00, eta: 4:44:10, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.7282
2023-02-13 14:01:14,510 - mmselfsup - INFO - Saving checkpoint at 40 epochs
2023-02-13 14:01:57,275 - mmselfsup - INFO - Epoch [41][50/427]	lr: 3.142e+00, eta: 4:43:04, time: 0.835, data_time: 0.127, memory: 25346, loss: 0.6083
2023-02-13 14:02:31,951 - mmselfsup - INFO - Epoch [41][100/427]	lr: 3.142e+00, eta: 4:42:33, time: 0.694, data_time: 0.002, memory: 25346, loss: 0.5473
2023-02-13 14:03:06,655 - mmselfsup - INFO - Epoch [41][150/427]	lr: 3.142e+00, eta: 4:42:02, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.5603
2023-02-13 14:03:41,371 - mmselfsup - INFO - Epoch [41][200/427]	lr: 3.142e+00, eta: 4:41:31, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.5497
2023-02-13 14:04:16,022 - mmselfsup - INFO - Epoch [41][250/427]	lr: 3.142e+00, eta: 4:41:00, time: 0.694, data_time: 0.002, memory: 25346, loss: 0.5307
2023-02-13 14:04:50,801 - mmselfsup - INFO - Epoch [41][300/427]	lr: 3.142e+00, eta: 4:40:29, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.5272
2023-02-13 14:05:25,448 - mmselfsup - INFO - Epoch [41][350/427]	lr: 3.142e+00, eta: 4:39:58, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.5232
2023-02-13 14:05:59,966 - mmselfsup - INFO - Epoch [41][400/427]	lr: 3.142e+00, eta: 4:39:27, time: 0.691, data_time: 0.001, memory: 25346, loss: 0.5245
2023-02-13 14:06:59,902 - mmselfsup - INFO - Epoch [42][50/427]	lr: 3.070e+00, eta: 4:38:22, time: 0.841, data_time: 0.088, memory: 25346, loss: 0.5195
2023-02-13 14:07:34,796 - mmselfsup - INFO - Epoch [42][100/427]	lr: 3.070e+00, eta: 4:37:51, time: 0.698, data_time: 0.003, memory: 25346, loss: 0.5047
2023-02-13 14:08:09,842 - mmselfsup - INFO - Epoch [42][150/427]	lr: 3.070e+00, eta: 4:37:21, time: 0.701, data_time: 0.003, memory: 25346, loss: 0.5127
2023-02-13 14:08:44,540 - mmselfsup - INFO - Epoch [42][200/427]	lr: 3.070e+00, eta: 4:36:50, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.5171
2023-02-13 14:09:19,325 - mmselfsup - INFO - Epoch [42][250/427]	lr: 3.070e+00, eta: 4:36:19, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.5083
2023-02-13 14:09:54,035 - mmselfsup - INFO - Epoch [42][300/427]	lr: 3.070e+00, eta: 4:35:47, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.5083
2023-02-13 14:10:28,769 - mmselfsup - INFO - Epoch [42][350/427]	lr: 3.070e+00, eta: 4:35:16, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.5022
2023-02-13 14:11:03,467 - mmselfsup - INFO - Epoch [42][400/427]	lr: 3.070e+00, eta: 4:34:45, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.4945
2023-02-13 14:12:03,719 - mmselfsup - INFO - Epoch [43][50/427]	lr: 2.997e+00, eta: 4:33:42, time: 0.846, data_time: 0.096, memory: 25346, loss: 0.5010
2023-02-13 14:12:38,594 - mmselfsup - INFO - Epoch [43][100/427]	lr: 2.997e+00, eta: 4:33:11, time: 0.699, data_time: 0.002, memory: 25346, loss: 0.4909
2023-02-13 14:13:13,329 - mmselfsup - INFO - Epoch [43][150/427]	lr: 2.997e+00, eta: 4:32:40, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.4928
2023-02-13 14:13:48,144 - mmselfsup - INFO - Epoch [43][200/427]	lr: 2.997e+00, eta: 4:32:08, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4861
2023-02-13 14:14:23,033 - mmselfsup - INFO - Epoch [43][250/427]	lr: 2.997e+00, eta: 4:31:37, time: 0.698, data_time: 0.001, memory: 25346, loss: 0.5017
2023-02-13 14:14:57,811 - mmselfsup - INFO - Epoch [43][300/427]	lr: 2.997e+00, eta: 4:31:06, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4854
2023-02-13 14:15:32,554 - mmselfsup - INFO - Epoch [43][350/427]	lr: 2.997e+00, eta: 4:30:35, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.4906
2023-02-13 14:16:07,476 - mmselfsup - INFO - Epoch [43][400/427]	lr: 2.997e+00, eta: 4:30:04, time: 0.699, data_time: 0.001, memory: 25346, loss: 0.4931
2023-02-13 14:17:07,539 - mmselfsup - INFO - Epoch [44][50/427]	lr: 2.924e+00, eta: 4:29:01, time: 0.844, data_time: 0.096, memory: 25346, loss: 0.4793
2023-02-13 14:17:42,403 - mmselfsup - INFO - Epoch [44][100/427]	lr: 2.924e+00, eta: 4:28:30, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.4793
2023-02-13 14:18:17,316 - mmselfsup - INFO - Epoch [44][150/427]	lr: 2.924e+00, eta: 4:27:59, time: 0.698, data_time: 0.003, memory: 25346, loss: 0.4828
2023-02-13 14:18:52,162 - mmselfsup - INFO - Epoch [44][200/427]	lr: 2.924e+00, eta: 4:27:28, time: 0.698, data_time: 0.003, memory: 25346, loss: 0.4853
2023-02-13 14:19:27,173 - mmselfsup - INFO - Epoch [44][250/427]	lr: 2.924e+00, eta: 4:26:57, time: 0.700, data_time: 0.003, memory: 25346, loss: 0.4727
2023-02-13 14:20:02,099 - mmselfsup - INFO - Epoch [44][300/427]	lr: 2.924e+00, eta: 4:26:26, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.4677
2023-02-13 14:20:36,898 - mmselfsup - INFO - Epoch [44][350/427]	lr: 2.924e+00, eta: 4:25:54, time: 0.697, data_time: 0.004, memory: 25346, loss: 0.4745
2023-02-13 14:21:11,761 - mmselfsup - INFO - Epoch [44][400/427]	lr: 2.924e+00, eta: 4:25:23, time: 0.698, data_time: 0.003, memory: 25346, loss: 0.4681
2023-02-13 14:22:11,384 - mmselfsup - INFO - Epoch [45][50/427]	lr: 2.850e+00, eta: 4:24:20, time: 0.835, data_time: 0.131, memory: 25346, loss: 0.4616
2023-02-13 14:22:46,104 - mmselfsup - INFO - Epoch [45][100/427]	lr: 2.850e+00, eta: 4:23:49, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.4613
2023-02-13 14:23:20,847 - mmselfsup - INFO - Epoch [45][150/427]	lr: 2.850e+00, eta: 4:23:17, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.4576
2023-02-13 14:23:55,650 - mmselfsup - INFO - Epoch [45][200/427]	lr: 2.850e+00, eta: 4:22:46, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4558
2023-02-13 14:24:30,354 - mmselfsup - INFO - Epoch [45][250/427]	lr: 2.850e+00, eta: 4:22:15, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.4658
2023-02-13 14:25:05,211 - mmselfsup - INFO - Epoch [45][300/427]	lr: 2.850e+00, eta: 4:21:43, time: 0.697, data_time: 0.001, memory: 25346, loss: 0.4670
2023-02-13 14:25:40,018 - mmselfsup - INFO - Epoch [45][350/427]	lr: 2.850e+00, eta: 4:21:12, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4846
2023-02-13 14:26:14,681 - mmselfsup - INFO - Epoch [45][400/427]	lr: 2.850e+00, eta: 4:20:40, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.4646
2023-02-13 14:27:14,766 - mmselfsup - INFO - Epoch [46][50/427]	lr: 2.775e+00, eta: 4:19:38, time: 0.844, data_time: 0.123, memory: 25346, loss: 0.4755
2023-02-13 14:27:49,461 - mmselfsup - INFO - Epoch [46][100/427]	lr: 2.775e+00, eta: 4:19:07, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.4729
2023-02-13 14:28:24,235 - mmselfsup - INFO - Epoch [46][150/427]	lr: 2.775e+00, eta: 4:18:35, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.4674
2023-02-13 14:28:59,037 - mmselfsup - INFO - Epoch [46][200/427]	lr: 2.775e+00, eta: 4:18:04, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.4675
2023-02-13 14:29:33,822 - mmselfsup - INFO - Epoch [46][250/427]	lr: 2.775e+00, eta: 4:17:32, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.4701
2023-02-13 14:30:08,601 - mmselfsup - INFO - Epoch [46][300/427]	lr: 2.775e+00, eta: 4:17:01, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.4445
2023-02-13 14:30:43,338 - mmselfsup - INFO - Epoch [46][350/427]	lr: 2.775e+00, eta: 4:16:30, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.4524
2023-02-13 14:31:18,105 - mmselfsup - INFO - Epoch [46][400/427]	lr: 2.775e+00, eta: 4:15:58, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.4570
2023-02-13 14:32:18,053 - mmselfsup - INFO - Epoch [47][50/427]	lr: 2.701e+00, eta: 4:14:56, time: 0.843, data_time: 0.101, memory: 25346, loss: 0.4477
2023-02-13 14:32:52,876 - mmselfsup - INFO - Epoch [47][100/427]	lr: 2.701e+00, eta: 4:14:25, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4436
2023-02-13 14:33:27,510 - mmselfsup - INFO - Epoch [47][150/427]	lr: 2.701e+00, eta: 4:13:53, time: 0.693, data_time: 0.000, memory: 25346, loss: 0.4431
2023-02-13 14:34:02,293 - mmselfsup - INFO - Epoch [47][200/427]	lr: 2.701e+00, eta: 4:13:22, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4402
2023-02-13 14:34:37,065 - mmselfsup - INFO - Epoch [47][250/427]	lr: 2.701e+00, eta: 4:12:50, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.4404
2023-02-13 14:35:11,878 - mmselfsup - INFO - Epoch [47][300/427]	lr: 2.701e+00, eta: 4:12:19, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4401
2023-02-13 14:35:46,594 - mmselfsup - INFO - Epoch [47][350/427]	lr: 2.701e+00, eta: 4:11:47, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.4384
2023-02-13 14:36:21,402 - mmselfsup - INFO - Epoch [47][400/427]	lr: 2.701e+00, eta: 4:11:16, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4404
2023-02-13 14:37:21,188 - mmselfsup - INFO - Epoch [48][50/427]	lr: 2.626e+00, eta: 4:10:14, time: 0.839, data_time: 0.126, memory: 25346, loss: 0.4299
2023-02-13 14:37:56,005 - mmselfsup - INFO - Epoch [48][100/427]	lr: 2.626e+00, eta: 4:09:42, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.4317
2023-02-13 14:38:30,775 - mmselfsup - INFO - Epoch [48][150/427]	lr: 2.626e+00, eta: 4:09:11, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.4549
2023-02-13 14:39:05,508 - mmselfsup - INFO - Epoch [48][200/427]	lr: 2.626e+00, eta: 4:08:39, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.4277
2023-02-13 14:39:40,152 - mmselfsup - INFO - Epoch [48][250/427]	lr: 2.626e+00, eta: 4:08:07, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.4234
2023-02-13 14:40:14,812 - mmselfsup - INFO - Epoch [48][300/427]	lr: 2.626e+00, eta: 4:07:36, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.4291
2023-02-13 14:40:49,517 - mmselfsup - INFO - Epoch [48][350/427]	lr: 2.626e+00, eta: 4:07:04, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.4347
2023-02-13 14:41:24,370 - mmselfsup - INFO - Epoch [48][400/427]	lr: 2.626e+00, eta: 4:06:33, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.4236
2023-02-13 14:42:24,533 - mmselfsup - INFO - Epoch [49][50/427]	lr: 2.551e+00, eta: 4:05:32, time: 0.845, data_time: 0.142, memory: 25346, loss: 0.4236
2023-02-13 14:42:59,220 - mmselfsup - INFO - Epoch [49][100/427]	lr: 2.551e+00, eta: 4:05:00, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.4224
2023-02-13 14:43:34,129 - mmselfsup - INFO - Epoch [49][150/427]	lr: 2.551e+00, eta: 4:04:28, time: 0.698, data_time: 0.001, memory: 25346, loss: 0.4175
2023-02-13 14:44:08,900 - mmselfsup - INFO - Epoch [49][200/427]	lr: 2.551e+00, eta: 4:03:57, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.4266
2023-02-13 14:44:43,757 - mmselfsup - INFO - Epoch [49][250/427]	lr: 2.551e+00, eta: 4:03:25, time: 0.698, data_time: 0.001, memory: 25346, loss: 0.4234
2023-02-13 14:45:18,491 - mmselfsup - INFO - Epoch [49][300/427]	lr: 2.551e+00, eta: 4:02:53, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.4228
2023-02-13 14:45:53,142 - mmselfsup - INFO - Epoch [49][350/427]	lr: 2.551e+00, eta: 4:02:22, time: 0.692, data_time: 0.001, memory: 25346, loss: 0.4301
2023-02-13 14:46:27,934 - mmselfsup - INFO - Epoch [49][400/427]	lr: 2.551e+00, eta: 4:01:50, time: 0.697, data_time: 0.002, memory: 25346, loss: 0.4143
2023-02-13 14:47:27,923 - mmselfsup - INFO - Epoch [50][50/427]	lr: 2.475e+00, eta: 4:00:49, time: 0.841, data_time: 0.128, memory: 25346, loss: 0.4102
2023-02-13 14:48:02,716 - mmselfsup - INFO - Epoch [50][100/427]	lr: 2.475e+00, eta: 4:00:18, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.4122
2023-02-13 14:48:37,570 - mmselfsup - INFO - Epoch [50][150/427]	lr: 2.475e+00, eta: 3:59:46, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.4065
2023-02-13 14:49:12,289 - mmselfsup - INFO - Epoch [50][200/427]	lr: 2.475e+00, eta: 3:59:14, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.4114
2023-02-13 14:49:47,128 - mmselfsup - INFO - Epoch [50][250/427]	lr: 2.475e+00, eta: 3:58:42, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.4249
2023-02-13 14:50:22,017 - mmselfsup - INFO - Epoch [50][300/427]	lr: 2.475e+00, eta: 3:58:11, time: 0.698, data_time: 0.004, memory: 25346, loss: 0.4157
2023-02-13 14:50:56,893 - mmselfsup - INFO - Epoch [50][350/427]	lr: 2.475e+00, eta: 3:57:39, time: 0.698, data_time: 0.003, memory: 25346, loss: 0.4044
2023-02-13 14:51:31,736 - mmselfsup - INFO - Epoch [50][400/427]	lr: 2.475e+00, eta: 3:57:07, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.4048
2023-02-13 14:51:49,600 - mmselfsup - INFO - Saving checkpoint at 50 epochs
2023-02-13 14:52:32,292 - mmselfsup - INFO - Epoch [51][50/427]	lr: 2.400e+00, eta: 3:56:07, time: 0.834, data_time: 0.122, memory: 25346, loss: 0.3981
2023-02-13 14:53:06,966 - mmselfsup - INFO - Epoch [51][100/427]	lr: 2.400e+00, eta: 3:55:35, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.4012
2023-02-13 14:53:41,788 - mmselfsup - INFO - Epoch [51][150/427]	lr: 2.400e+00, eta: 3:55:03, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4136
2023-02-13 14:54:16,482 - mmselfsup - INFO - Epoch [51][200/427]	lr: 2.400e+00, eta: 3:54:31, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.4056
2023-02-13 14:54:51,167 - mmselfsup - INFO - Epoch [51][250/427]	lr: 2.400e+00, eta: 3:53:59, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.4193
2023-02-13 14:55:25,870 - mmselfsup - INFO - Epoch [51][300/427]	lr: 2.400e+00, eta: 3:53:28, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.4247
2023-02-13 14:56:00,632 - mmselfsup - INFO - Epoch [51][350/427]	lr: 2.400e+00, eta: 3:52:56, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.4060
2023-02-13 14:56:35,277 - mmselfsup - INFO - Epoch [51][400/427]	lr: 2.400e+00, eta: 3:52:24, time: 0.693, data_time: 0.002, memory: 25346, loss: 0.4002
2023-02-13 14:57:35,212 - mmselfsup - INFO - Epoch [52][50/427]	lr: 2.325e+00, eta: 3:51:24, time: 0.842, data_time: 0.116, memory: 25346, loss: 0.4015
2023-02-13 14:58:09,945 - mmselfsup - INFO - Epoch [52][100/427]	lr: 2.325e+00, eta: 3:50:52, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.4061
2023-02-13 14:58:44,759 - mmselfsup - INFO - Epoch [52][150/427]	lr: 2.325e+00, eta: 3:50:20, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.3966
2023-02-13 14:59:19,410 - mmselfsup - INFO - Epoch [52][200/427]	lr: 2.325e+00, eta: 3:49:48, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.3889
2023-02-13 14:59:54,204 - mmselfsup - INFO - Epoch [52][250/427]	lr: 2.325e+00, eta: 3:49:16, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.3918
2023-02-13 15:00:28,922 - mmselfsup - INFO - Epoch [52][300/427]	lr: 2.325e+00, eta: 3:48:44, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.3976
2023-02-13 15:01:03,636 - mmselfsup - INFO - Epoch [52][350/427]	lr: 2.325e+00, eta: 3:48:12, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.3924
2023-02-13 15:01:38,352 - mmselfsup - INFO - Epoch [52][400/427]	lr: 2.325e+00, eta: 3:47:40, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.3990
2023-02-13 15:02:38,618 - mmselfsup - INFO - Epoch [53][50/427]	lr: 2.249e+00, eta: 3:46:41, time: 0.847, data_time: 0.101, memory: 25346, loss: 0.3814
2023-02-13 15:03:13,349 - mmselfsup - INFO - Epoch [53][100/427]	lr: 2.249e+00, eta: 3:46:09, time: 0.696, data_time: 0.002, memory: 25346, loss: 0.3997
2023-02-13 15:03:47,989 - mmselfsup - INFO - Epoch [53][150/427]	lr: 2.249e+00, eta: 3:45:37, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.3961
2023-02-13 15:04:22,712 - mmselfsup - INFO - Epoch [53][200/427]	lr: 2.249e+00, eta: 3:45:05, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.3857
2023-02-13 15:04:57,564 - mmselfsup - INFO - Epoch [53][250/427]	lr: 2.249e+00, eta: 3:44:33, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.4042
2023-02-13 15:05:32,252 - mmselfsup - INFO - Epoch [53][300/427]	lr: 2.249e+00, eta: 3:44:01, time: 0.695, data_time: 0.002, memory: 25346, loss: 0.3829
2023-02-13 15:06:06,971 - mmselfsup - INFO - Epoch [53][350/427]	lr: 2.249e+00, eta: 3:43:29, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.3770
2023-02-13 15:06:41,762 - mmselfsup - INFO - Epoch [53][400/427]	lr: 2.249e+00, eta: 3:42:57, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.3822
2023-02-13 15:07:41,492 - mmselfsup - INFO - Epoch [54][50/427]	lr: 2.174e+00, eta: 3:41:58, time: 0.838, data_time: 0.122, memory: 25346, loss: 0.3740
2023-02-13 15:08:16,157 - mmselfsup - INFO - Epoch [54][100/427]	lr: 2.174e+00, eta: 3:41:26, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.4319
2023-02-13 15:08:50,940 - mmselfsup - INFO - Epoch [54][150/427]	lr: 2.174e+00, eta: 3:40:54, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.3966
2023-02-13 15:09:25,669 - mmselfsup - INFO - Epoch [54][200/427]	lr: 2.174e+00, eta: 3:40:22, time: 0.695, data_time: 0.004, memory: 25346, loss: 0.3771
2023-02-13 15:10:00,476 - mmselfsup - INFO - Epoch [54][250/427]	lr: 2.174e+00, eta: 3:39:50, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.3835
2023-02-13 15:10:35,246 - mmselfsup - INFO - Epoch [54][300/427]	lr: 2.174e+00, eta: 3:39:18, time: 0.696, data_time: 0.004, memory: 25346, loss: 0.3849
2023-02-13 15:11:10,092 - mmselfsup - INFO - Epoch [54][350/427]	lr: 2.174e+00, eta: 3:38:46, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.3779
2023-02-13 15:11:44,826 - mmselfsup - INFO - Epoch [54][400/427]	lr: 2.174e+00, eta: 3:38:14, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.3789
2023-02-13 15:12:45,171 - mmselfsup - INFO - Epoch [55][50/427]	lr: 2.099e+00, eta: 3:37:15, time: 0.849, data_time: 0.142, memory: 25346, loss: 0.3732
2023-02-13 15:13:19,891 - mmselfsup - INFO - Epoch [55][100/427]	lr: 2.099e+00, eta: 3:36:43, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.3670
2023-02-13 15:13:54,618 - mmselfsup - INFO - Epoch [55][150/427]	lr: 2.099e+00, eta: 3:36:11, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.3572
2023-02-13 15:14:29,278 - mmselfsup - INFO - Epoch [55][200/427]	lr: 2.099e+00, eta: 3:35:39, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.3746
2023-02-13 15:15:03,968 - mmselfsup - INFO - Epoch [55][250/427]	lr: 2.099e+00, eta: 3:35:07, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.3774
2023-02-13 15:15:38,677 - mmselfsup - INFO - Epoch [55][300/427]	lr: 2.099e+00, eta: 3:34:35, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.3755
2023-02-13 15:16:13,416 - mmselfsup - INFO - Epoch [55][350/427]	lr: 2.099e+00, eta: 3:34:03, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.3836
2023-02-13 15:16:48,245 - mmselfsup - INFO - Epoch [55][400/427]	lr: 2.099e+00, eta: 3:33:31, time: 0.697, data_time: 0.001, memory: 25346, loss: 0.3926
2023-02-13 15:17:48,371 - mmselfsup - INFO - Epoch [56][50/427]	lr: 2.025e+00, eta: 3:32:32, time: 0.844, data_time: 0.092, memory: 25346, loss: 0.4297
2023-02-13 15:18:23,217 - mmselfsup - INFO - Epoch [56][100/427]	lr: 2.025e+00, eta: 3:32:00, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.3862
2023-02-13 15:18:58,111 - mmselfsup - INFO - Epoch [56][150/427]	lr: 2.025e+00, eta: 3:31:28, time: 0.698, data_time: 0.003, memory: 25346, loss: 0.3664
2023-02-13 15:19:33,004 - mmselfsup - INFO - Epoch [56][200/427]	lr: 2.025e+00, eta: 3:30:56, time: 0.698, data_time: 0.003, memory: 25346, loss: 0.4654
2023-02-13 15:20:07,664 - mmselfsup - INFO - Epoch [56][250/427]	lr: 2.025e+00, eta: 3:30:24, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.4156
2023-02-13 15:20:42,406 - mmselfsup - INFO - Epoch [56][300/427]	lr: 2.025e+00, eta: 3:29:52, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.3989
2023-02-13 15:21:17,221 - mmselfsup - INFO - Epoch [56][350/427]	lr: 2.025e+00, eta: 3:29:20, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.3810
2023-02-13 15:21:52,041 - mmselfsup - INFO - Epoch [56][400/427]	lr: 2.025e+00, eta: 3:28:48, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.3681
2023-02-13 15:22:51,998 - mmselfsup - INFO - Epoch [57][50/427]	lr: 1.950e+00, eta: 3:27:49, time: 0.842, data_time: 0.095, memory: 25346, loss: 0.3674
2023-02-13 15:23:26,908 - mmselfsup - INFO - Epoch [57][100/427]	lr: 1.950e+00, eta: 3:27:17, time: 0.698, data_time: 0.001, memory: 25346, loss: 0.3916
2023-02-13 15:24:01,734 - mmselfsup - INFO - Epoch [57][150/427]	lr: 1.950e+00, eta: 3:26:45, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.3507
2023-02-13 15:24:36,494 - mmselfsup - INFO - Epoch [57][200/427]	lr: 1.950e+00, eta: 3:26:13, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.3604
2023-02-13 15:25:11,287 - mmselfsup - INFO - Epoch [57][250/427]	lr: 1.950e+00, eta: 3:25:41, time: 0.697, data_time: 0.001, memory: 25346, loss: 0.3550
2023-02-13 15:25:46,189 - mmselfsup - INFO - Epoch [57][300/427]	lr: 1.950e+00, eta: 3:25:09, time: 0.698, data_time: 0.001, memory: 25346, loss: 0.3533
2023-02-13 15:26:21,106 - mmselfsup - INFO - Epoch [57][350/427]	lr: 1.950e+00, eta: 3:24:37, time: 0.697, data_time: 0.001, memory: 25346, loss: 0.3598
2023-02-13 15:26:55,878 - mmselfsup - INFO - Epoch [57][400/427]	lr: 1.950e+00, eta: 3:24:05, time: 0.696, data_time: 0.002, memory: 25346, loss: 0.3687
2023-02-13 15:27:56,060 - mmselfsup - INFO - Epoch [58][50/427]	lr: 1.876e+00, eta: 3:23:07, time: 0.845, data_time: 0.092, memory: 25346, loss: 0.3529
2023-02-13 15:28:30,854 - mmselfsup - INFO - Epoch [58][100/427]	lr: 1.876e+00, eta: 3:22:34, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.3605
2023-02-13 15:29:05,746 - mmselfsup - INFO - Epoch [58][150/427]	lr: 1.876e+00, eta: 3:22:02, time: 0.698, data_time: 0.003, memory: 25346, loss: 0.3577
2023-02-13 15:29:40,523 - mmselfsup - INFO - Epoch [58][200/427]	lr: 1.876e+00, eta: 3:21:30, time: 0.696, data_time: 0.003, memory: 25346, loss: 0.3412
2023-02-13 15:30:15,249 - mmselfsup - INFO - Epoch [58][250/427]	lr: 1.876e+00, eta: 3:20:58, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.3435
2023-02-13 15:30:50,189 - mmselfsup - INFO - Epoch [58][300/427]	lr: 1.876e+00, eta: 3:20:26, time: 0.699, data_time: 0.003, memory: 25346, loss: 0.3511
2023-02-13 15:31:25,041 - mmselfsup - INFO - Epoch [58][350/427]	lr: 1.876e+00, eta: 3:19:54, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.3680
2023-02-13 15:31:59,682 - mmselfsup - INFO - Epoch [58][400/427]	lr: 1.876e+00, eta: 3:19:21, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.3516
2023-02-13 15:32:59,819 - mmselfsup - INFO - Epoch [59][50/427]	lr: 1.803e+00, eta: 3:18:24, time: 0.846, data_time: 0.094, memory: 25346, loss: 0.3406
2023-02-13 15:33:34,575 - mmselfsup - INFO - Epoch [59][100/427]	lr: 1.803e+00, eta: 3:17:51, time: 0.696, data_time: 0.001, memory: 25346, loss: 0.3265
2023-02-13 15:34:09,256 - mmselfsup - INFO - Epoch [59][150/427]	lr: 1.803e+00, eta: 3:17:19, time: 0.693, data_time: 0.001, memory: 25346, loss: 0.3446
2023-02-13 15:34:44,012 - mmselfsup - INFO - Epoch [59][200/427]	lr: 1.803e+00, eta: 3:16:47, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.3449
2023-02-13 15:35:18,722 - mmselfsup - INFO - Epoch [59][250/427]	lr: 1.803e+00, eta: 3:16:15, time: 0.694, data_time: 0.002, memory: 25346, loss: 0.3577
2023-02-13 15:35:53,410 - mmselfsup - INFO - Epoch [59][300/427]	lr: 1.803e+00, eta: 3:15:42, time: 0.695, data_time: 0.002, memory: 25346, loss: 0.3396
2023-02-13 15:36:28,297 - mmselfsup - INFO - Epoch [59][350/427]	lr: 1.803e+00, eta: 3:15:10, time: 0.698, data_time: 0.001, memory: 25346, loss: 0.3358
2023-02-13 15:37:02,991 - mmselfsup - INFO - Epoch [59][400/427]	lr: 1.803e+00, eta: 3:14:38, time: 0.694, data_time: 0.001, memory: 25346, loss: 0.3454
2023-02-13 15:38:04,002 - mmselfsup - INFO - Epoch [60][50/427]	lr: 1.730e+00, eta: 3:13:41, time: 0.861, data_time: 0.144, memory: 25346, loss: 0.3334
2023-02-13 15:38:39,021 - mmselfsup - INFO - Epoch [60][100/427]	lr: 1.730e+00, eta: 3:13:09, time: 0.701, data_time: 0.003, memory: 25346, loss: 0.3433
2023-02-13 15:39:13,570 - mmselfsup - INFO - Epoch [60][150/427]	lr: 1.730e+00, eta: 3:12:36, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.3445
2023-02-13 15:39:48,261 - mmselfsup - INFO - Epoch [60][200/427]	lr: 1.730e+00, eta: 3:12:04, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.3403
2023-02-13 15:40:22,908 - mmselfsup - INFO - Epoch [60][250/427]	lr: 1.730e+00, eta: 3:11:32, time: 0.693, data_time: 0.003, memory: 25346, loss: 0.3312
2023-02-13 15:40:56,977 - mmselfsup - INFO - Epoch [60][300/427]	lr: 1.730e+00, eta: 3:10:59, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.3358
2023-02-13 15:41:30,823 - mmselfsup - INFO - Epoch [60][350/427]	lr: 1.730e+00, eta: 3:10:26, time: 0.678, data_time: 0.004, memory: 25346, loss: 0.3167
2023-02-13 15:42:05,108 - mmselfsup - INFO - Epoch [60][400/427]	lr: 1.730e+00, eta: 3:09:53, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3280
2023-02-13 15:42:22,921 - mmselfsup - INFO - Saving checkpoint at 60 epochs
2023-02-13 15:43:05,857 - mmselfsup - INFO - Epoch [61][50/427]	lr: 1.658e+00, eta: 3:08:56, time: 0.839, data_time: 0.139, memory: 25346, loss: 0.3358
2023-02-13 15:43:39,757 - mmselfsup - INFO - Epoch [61][100/427]	lr: 1.658e+00, eta: 3:08:23, time: 0.677, data_time: 0.002, memory: 25346, loss: 0.3532
2023-02-13 15:44:13,536 - mmselfsup - INFO - Epoch [61][150/427]	lr: 1.658e+00, eta: 3:07:50, time: 0.677, data_time: 0.002, memory: 25346, loss: 0.3233
2023-02-13 15:44:47,150 - mmselfsup - INFO - Epoch [61][200/427]	lr: 1.658e+00, eta: 3:07:17, time: 0.672, data_time: 0.001, memory: 25346, loss: 0.3100
2023-02-13 15:45:20,664 - mmselfsup - INFO - Epoch [61][250/427]	lr: 1.658e+00, eta: 3:06:44, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.3259
2023-02-13 15:45:54,308 - mmselfsup - INFO - Epoch [61][300/427]	lr: 1.658e+00, eta: 3:06:11, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.3281
2023-02-13 15:46:27,986 - mmselfsup - INFO - Epoch [61][350/427]	lr: 1.658e+00, eta: 3:05:38, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.3302
2023-02-13 15:47:01,575 - mmselfsup - INFO - Epoch [61][400/427]	lr: 1.658e+00, eta: 3:05:05, time: 0.672, data_time: 0.001, memory: 25346, loss: 0.3276
2023-02-13 15:48:01,681 - mmselfsup - INFO - Epoch [62][50/427]	lr: 1.587e+00, eta: 3:04:08, time: 0.850, data_time: 0.134, memory: 25346, loss: 0.3203
2023-02-13 15:48:35,312 - mmselfsup - INFO - Epoch [62][100/427]	lr: 1.587e+00, eta: 3:03:35, time: 0.673, data_time: 0.003, memory: 25346, loss: 0.3318
2023-02-13 15:49:08,820 - mmselfsup - INFO - Epoch [62][150/427]	lr: 1.587e+00, eta: 3:03:01, time: 0.670, data_time: 0.003, memory: 25346, loss: 0.3176
2023-02-13 15:49:42,444 - mmselfsup - INFO - Epoch [62][200/427]	lr: 1.587e+00, eta: 3:02:28, time: 0.672, data_time: 0.003, memory: 25346, loss: 0.3059
2023-02-13 15:50:16,359 - mmselfsup - INFO - Epoch [62][250/427]	lr: 1.587e+00, eta: 3:01:56, time: 0.680, data_time: 0.004, memory: 25346, loss: 0.3055
2023-02-13 15:50:50,083 - mmselfsup - INFO - Epoch [62][300/427]	lr: 1.587e+00, eta: 3:01:23, time: 0.674, data_time: 0.003, memory: 25346, loss: 0.3075
2023-02-13 15:51:24,073 - mmselfsup - INFO - Epoch [62][350/427]	lr: 1.587e+00, eta: 3:00:50, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.2956
2023-02-13 15:51:57,829 - mmselfsup - INFO - Epoch [62][400/427]	lr: 1.587e+00, eta: 3:00:17, time: 0.675, data_time: 0.003, memory: 25346, loss: 0.3057
2023-02-13 15:52:58,104 - mmselfsup - INFO - Epoch [63][50/427]	lr: 1.517e+00, eta: 2:59:20, time: 0.851, data_time: 0.149, memory: 25346, loss: 0.3350
2023-02-13 15:53:31,515 - mmselfsup - INFO - Epoch [63][100/427]	lr: 1.517e+00, eta: 2:58:47, time: 0.668, data_time: 0.001, memory: 25346, loss: 0.3036
2023-02-13 15:54:05,008 - mmselfsup - INFO - Epoch [63][150/427]	lr: 1.517e+00, eta: 2:58:14, time: 0.669, data_time: 0.001, memory: 25346, loss: 0.3107
2023-02-13 15:54:38,590 - mmselfsup - INFO - Epoch [63][200/427]	lr: 1.517e+00, eta: 2:57:41, time: 0.672, data_time: 0.002, memory: 25346, loss: 0.3109
2023-02-13 15:55:12,075 - mmselfsup - INFO - Epoch [63][250/427]	lr: 1.517e+00, eta: 2:57:08, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.3137
2023-02-13 15:55:45,654 - mmselfsup - INFO - Epoch [63][300/427]	lr: 1.517e+00, eta: 2:56:35, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.3090
2023-02-13 15:56:19,255 - mmselfsup - INFO - Epoch [63][350/427]	lr: 1.517e+00, eta: 2:56:02, time: 0.672, data_time: 0.002, memory: 25346, loss: 0.3051
2023-02-13 15:56:52,849 - mmselfsup - INFO - Epoch [63][400/427]	lr: 1.517e+00, eta: 2:55:28, time: 0.673, data_time: 0.002, memory: 25346, loss: 0.2963
2023-02-13 15:57:53,487 - mmselfsup - INFO - Epoch [64][50/427]	lr: 1.447e+00, eta: 2:54:33, time: 0.862, data_time: 0.147, memory: 25346, loss: 0.3228
2023-02-13 15:58:28,824 - mmselfsup - INFO - Epoch [64][100/427]	lr: 1.447e+00, eta: 2:54:00, time: 0.707, data_time: 0.003, memory: 25346, loss: 0.3158
2023-02-13 15:59:03,992 - mmselfsup - INFO - Epoch [64][150/427]	lr: 1.447e+00, eta: 2:53:28, time: 0.703, data_time: 0.003, memory: 25346, loss: 0.2967
2023-02-13 15:59:39,193 - mmselfsup - INFO - Epoch [64][200/427]	lr: 1.447e+00, eta: 2:52:56, time: 0.704, data_time: 0.003, memory: 25346, loss: 0.2876
2023-02-13 16:00:14,256 - mmselfsup - INFO - Epoch [64][250/427]	lr: 1.447e+00, eta: 2:52:24, time: 0.701, data_time: 0.003, memory: 25346, loss: 0.2948
2023-02-13 16:00:49,447 - mmselfsup - INFO - Epoch [64][300/427]	lr: 1.447e+00, eta: 2:51:52, time: 0.704, data_time: 0.003, memory: 25346, loss: 0.3019
2023-02-13 16:01:24,591 - mmselfsup - INFO - Epoch [64][350/427]	lr: 1.447e+00, eta: 2:51:20, time: 0.702, data_time: 0.003, memory: 25346, loss: 0.3318
2023-02-13 16:01:59,708 - mmselfsup - INFO - Epoch [64][400/427]	lr: 1.447e+00, eta: 2:50:47, time: 0.703, data_time: 0.004, memory: 25346, loss: 0.3593
2023-02-13 16:03:00,589 - mmselfsup - INFO - Epoch [65][50/427]	lr: 1.378e+00, eta: 2:49:51, time: 0.859, data_time: 0.137, memory: 25346, loss: 0.3030
2023-02-13 16:03:35,335 - mmselfsup - INFO - Epoch [65][100/427]	lr: 1.378e+00, eta: 2:49:19, time: 0.695, data_time: 0.001, memory: 25346, loss: 0.3022
2023-02-13 16:04:10,582 - mmselfsup - INFO - Epoch [65][150/427]	lr: 1.378e+00, eta: 2:48:47, time: 0.705, data_time: 0.001, memory: 25346, loss: 0.2974
2023-02-13 16:04:45,456 - mmselfsup - INFO - Epoch [65][200/427]	lr: 1.378e+00, eta: 2:48:14, time: 0.697, data_time: 0.001, memory: 25346, loss: 0.2957
2023-02-13 16:05:20,055 - mmselfsup - INFO - Epoch [65][250/427]	lr: 1.378e+00, eta: 2:47:42, time: 0.692, data_time: 0.002, memory: 25346, loss: 0.2946
2023-02-13 16:05:54,523 - mmselfsup - INFO - Epoch [65][300/427]	lr: 1.378e+00, eta: 2:47:09, time: 0.689, data_time: 0.001, memory: 25346, loss: 0.2911
2023-02-13 16:06:28,588 - mmselfsup - INFO - Epoch [65][350/427]	lr: 1.378e+00, eta: 2:46:36, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.2962
2023-02-13 16:07:02,807 - mmselfsup - INFO - Epoch [65][400/427]	lr: 1.378e+00, eta: 2:46:04, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3024
2023-02-13 16:08:02,853 - mmselfsup - INFO - Epoch [66][50/427]	lr: 1.310e+00, eta: 2:45:08, time: 0.842, data_time: 0.141, memory: 25346, loss: 0.2965
2023-02-13 16:08:37,590 - mmselfsup - INFO - Epoch [66][100/427]	lr: 1.310e+00, eta: 2:44:35, time: 0.695, data_time: 0.003, memory: 25346, loss: 0.3117
2023-02-13 16:09:12,285 - mmselfsup - INFO - Epoch [66][150/427]	lr: 1.310e+00, eta: 2:44:03, time: 0.694, data_time: 0.003, memory: 25346, loss: 0.3068
2023-02-13 16:09:47,329 - mmselfsup - INFO - Epoch [66][200/427]	lr: 1.310e+00, eta: 2:43:30, time: 0.701, data_time: 0.003, memory: 25346, loss: 0.2834
2023-02-13 16:10:22,653 - mmselfsup - INFO - Epoch [66][250/427]	lr: 1.310e+00, eta: 2:42:58, time: 0.706, data_time: 0.003, memory: 25346, loss: 0.2891
2023-02-13 16:10:57,790 - mmselfsup - INFO - Epoch [66][300/427]	lr: 1.310e+00, eta: 2:42:26, time: 0.702, data_time: 0.003, memory: 25346, loss: 0.3172
2023-02-13 16:11:32,971 - mmselfsup - INFO - Epoch [66][350/427]	lr: 1.310e+00, eta: 2:41:54, time: 0.705, data_time: 0.004, memory: 25346, loss: 0.3028
2023-02-13 16:12:08,090 - mmselfsup - INFO - Epoch [66][400/427]	lr: 1.310e+00, eta: 2:41:21, time: 0.702, data_time: 0.002, memory: 25346, loss: 0.2947
2023-02-13 16:13:09,255 - mmselfsup - INFO - Epoch [67][50/427]	lr: 1.244e+00, eta: 2:40:26, time: 0.864, data_time: 0.104, memory: 25346, loss: 0.2704
2023-02-13 16:13:44,294 - mmselfsup - INFO - Epoch [67][100/427]	lr: 1.244e+00, eta: 2:39:53, time: 0.701, data_time: 0.001, memory: 25346, loss: 0.3061
2023-02-13 16:14:19,405 - mmselfsup - INFO - Epoch [67][150/427]	lr: 1.244e+00, eta: 2:39:21, time: 0.702, data_time: 0.001, memory: 25346, loss: 0.2967
2023-02-13 16:14:54,531 - mmselfsup - INFO - Epoch [67][200/427]	lr: 1.244e+00, eta: 2:38:49, time: 0.702, data_time: 0.001, memory: 25346, loss: 0.3100
2023-02-13 16:15:29,541 - mmselfsup - INFO - Epoch [67][250/427]	lr: 1.244e+00, eta: 2:38:16, time: 0.700, data_time: 0.001, memory: 25346, loss: 0.2845
2023-02-13 16:16:04,565 - mmselfsup - INFO - Epoch [67][300/427]	lr: 1.244e+00, eta: 2:37:44, time: 0.700, data_time: 0.001, memory: 25346, loss: 0.2817
2023-02-13 16:16:39,609 - mmselfsup - INFO - Epoch [67][350/427]	lr: 1.244e+00, eta: 2:37:12, time: 0.700, data_time: 0.001, memory: 25346, loss: 0.2837
2023-02-13 16:17:14,649 - mmselfsup - INFO - Epoch [67][400/427]	lr: 1.244e+00, eta: 2:36:39, time: 0.701, data_time: 0.001, memory: 25346, loss: 0.2835
2023-02-13 16:18:15,522 - mmselfsup - INFO - Epoch [68][50/427]	lr: 1.178e+00, eta: 2:35:44, time: 0.859, data_time: 0.127, memory: 25346, loss: 0.2645
2023-02-13 16:18:50,562 - mmselfsup - INFO - Epoch [68][100/427]	lr: 1.178e+00, eta: 2:35:11, time: 0.701, data_time: 0.003, memory: 25346, loss: 0.2821
2023-02-13 16:19:25,581 - mmselfsup - INFO - Epoch [68][150/427]	lr: 1.178e+00, eta: 2:34:39, time: 0.700, data_time: 0.003, memory: 25346, loss: 0.2765
2023-02-13 16:20:00,459 - mmselfsup - INFO - Epoch [68][200/427]	lr: 1.178e+00, eta: 2:34:06, time: 0.697, data_time: 0.003, memory: 25346, loss: 0.2659
2023-02-13 16:20:35,036 - mmselfsup - INFO - Epoch [68][250/427]	lr: 1.178e+00, eta: 2:33:34, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.2792
2023-02-13 16:21:09,631 - mmselfsup - INFO - Epoch [68][300/427]	lr: 1.178e+00, eta: 2:33:01, time: 0.692, data_time: 0.003, memory: 25346, loss: 0.2866
2023-02-13 16:21:44,586 - mmselfsup - INFO - Epoch [68][350/427]	lr: 1.178e+00, eta: 2:32:29, time: 0.700, data_time: 0.004, memory: 25346, loss: 0.3111
2023-02-13 16:22:18,674 - mmselfsup - INFO - Epoch [68][400/427]	lr: 1.178e+00, eta: 2:31:56, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.2944
2023-02-13 16:23:18,619 - mmselfsup - INFO - Epoch [69][50/427]	lr: 1.114e+00, eta: 2:31:00, time: 0.845, data_time: 0.128, memory: 25346, loss: 0.2735
2023-02-13 16:23:52,297 - mmselfsup - INFO - Epoch [69][100/427]	lr: 1.114e+00, eta: 2:30:27, time: 0.674, data_time: 0.002, memory: 25346, loss: 0.2765
2023-02-13 16:24:25,720 - mmselfsup - INFO - Epoch [69][150/427]	lr: 1.114e+00, eta: 2:29:54, time: 0.668, data_time: 0.001, memory: 25346, loss: 0.2827
2023-02-13 16:24:59,267 - mmselfsup - INFO - Epoch [69][200/427]	lr: 1.114e+00, eta: 2:29:21, time: 0.672, data_time: 0.002, memory: 25346, loss: 0.2648
2023-02-13 16:25:32,746 - mmselfsup - INFO - Epoch [69][250/427]	lr: 1.114e+00, eta: 2:28:48, time: 0.669, data_time: 0.001, memory: 25346, loss: 0.2805
2023-02-13 16:26:06,405 - mmselfsup - INFO - Epoch [69][300/427]	lr: 1.114e+00, eta: 2:28:14, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.2834
2023-02-13 16:26:39,807 - mmselfsup - INFO - Epoch [69][350/427]	lr: 1.114e+00, eta: 2:27:41, time: 0.668, data_time: 0.001, memory: 25346, loss: 0.2889
2023-02-13 16:27:13,385 - mmselfsup - INFO - Epoch [69][400/427]	lr: 1.114e+00, eta: 2:27:08, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2748
2023-02-13 16:28:13,652 - mmselfsup - INFO - Epoch [70][50/427]	lr: 1.051e+00, eta: 2:26:13, time: 0.853, data_time: 0.139, memory: 25346, loss: 0.2753
2023-02-13 16:28:47,537 - mmselfsup - INFO - Epoch [70][100/427]	lr: 1.051e+00, eta: 2:25:40, time: 0.677, data_time: 0.003, memory: 25346, loss: 0.2627
2023-02-13 16:29:21,212 - mmselfsup - INFO - Epoch [70][150/427]	lr: 1.051e+00, eta: 2:25:07, time: 0.675, data_time: 0.004, memory: 25346, loss: 0.2649
2023-02-13 16:29:54,876 - mmselfsup - INFO - Epoch [70][200/427]	lr: 1.051e+00, eta: 2:24:34, time: 0.672, data_time: 0.002, memory: 25346, loss: 0.2687
2023-02-13 16:30:28,582 - mmselfsup - INFO - Epoch [70][250/427]	lr: 1.051e+00, eta: 2:24:01, time: 0.675, data_time: 0.003, memory: 25346, loss: 0.2690
2023-02-13 16:31:02,323 - mmselfsup - INFO - Epoch [70][300/427]	lr: 1.051e+00, eta: 2:23:28, time: 0.674, data_time: 0.003, memory: 25346, loss: 0.2708
2023-02-13 16:31:35,949 - mmselfsup - INFO - Epoch [70][350/427]	lr: 1.051e+00, eta: 2:22:55, time: 0.672, data_time: 0.003, memory: 25346, loss: 0.2659
2023-02-13 16:32:09,841 - mmselfsup - INFO - Epoch [70][400/427]	lr: 1.051e+00, eta: 2:22:22, time: 0.678, data_time: 0.003, memory: 25346, loss: 0.2605
2023-02-13 16:32:27,660 - mmselfsup - INFO - Saving checkpoint at 70 epochs
2023-02-13 16:33:10,443 - mmselfsup - INFO - Epoch [71][50/427]	lr: 9.893e-01, eta: 2:21:26, time: 0.836, data_time: 0.139, memory: 25346, loss: 0.2586
2023-02-13 16:33:44,133 - mmselfsup - INFO - Epoch [71][100/427]	lr: 9.893e-01, eta: 2:20:53, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.2511
2023-02-13 16:34:17,710 - mmselfsup - INFO - Epoch [71][150/427]	lr: 9.893e-01, eta: 2:20:20, time: 0.672, data_time: 0.001, memory: 25346, loss: 0.2595
2023-02-13 16:34:51,220 - mmselfsup - INFO - Epoch [71][200/427]	lr: 9.893e-01, eta: 2:19:47, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2666
2023-02-13 16:35:24,725 - mmselfsup - INFO - Epoch [71][250/427]	lr: 9.893e-01, eta: 2:19:14, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.2510
2023-02-13 16:35:58,257 - mmselfsup - INFO - Epoch [71][300/427]	lr: 9.893e-01, eta: 2:18:41, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.2585
2023-02-13 16:36:31,696 - mmselfsup - INFO - Epoch [71][350/427]	lr: 9.893e-01, eta: 2:18:07, time: 0.669, data_time: 0.001, memory: 25346, loss: 0.2540
2023-02-13 16:37:05,286 - mmselfsup - INFO - Epoch [71][400/427]	lr: 9.893e-01, eta: 2:17:34, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.2628
2023-02-13 16:38:05,520 - mmselfsup - INFO - Epoch [72][50/427]	lr: 9.290e-01, eta: 2:16:40, time: 0.853, data_time: 0.132, memory: 25346, loss: 0.2567
2023-02-13 16:38:39,179 - mmselfsup - INFO - Epoch [72][100/427]	lr: 9.290e-01, eta: 2:16:07, time: 0.673, data_time: 0.003, memory: 25346, loss: 0.2571
2023-02-13 16:39:13,042 - mmselfsup - INFO - Epoch [72][150/427]	lr: 9.290e-01, eta: 2:15:34, time: 0.677, data_time: 0.003, memory: 25346, loss: 0.2496
2023-02-13 16:39:46,749 - mmselfsup - INFO - Epoch [72][200/427]	lr: 9.290e-01, eta: 2:15:00, time: 0.675, data_time: 0.003, memory: 25346, loss: 0.2608
2023-02-13 16:40:20,759 - mmselfsup - INFO - Epoch [72][250/427]	lr: 9.290e-01, eta: 2:14:28, time: 0.679, data_time: 0.002, memory: 25346, loss: 0.2536
2023-02-13 16:40:54,253 - mmselfsup - INFO - Epoch [72][300/427]	lr: 9.290e-01, eta: 2:13:54, time: 0.670, data_time: 0.003, memory: 25346, loss: 0.2571
2023-02-13 16:41:27,999 - mmselfsup - INFO - Epoch [72][350/427]	lr: 9.290e-01, eta: 2:13:21, time: 0.675, data_time: 0.003, memory: 25346, loss: 0.2541
2023-02-13 16:42:01,528 - mmselfsup - INFO - Epoch [72][400/427]	lr: 9.290e-01, eta: 2:12:48, time: 0.671, data_time: 0.003, memory: 25346, loss: 0.2505
2023-02-13 16:43:01,461 - mmselfsup - INFO - Epoch [73][50/427]	lr: 8.702e-01, eta: 2:11:54, time: 0.846, data_time: 0.147, memory: 25346, loss: 0.2595
2023-02-13 16:43:35,026 - mmselfsup - INFO - Epoch [73][100/427]	lr: 8.702e-01, eta: 2:11:20, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2461
2023-02-13 16:44:08,588 - mmselfsup - INFO - Epoch [73][150/427]	lr: 8.702e-01, eta: 2:10:47, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2598
2023-02-13 16:44:42,252 - mmselfsup - INFO - Epoch [73][200/427]	lr: 8.702e-01, eta: 2:10:14, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2528
2023-02-13 16:45:15,813 - mmselfsup - INFO - Epoch [73][250/427]	lr: 8.702e-01, eta: 2:09:41, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2497
2023-02-13 16:45:49,385 - mmselfsup - INFO - Epoch [73][300/427]	lr: 8.702e-01, eta: 2:09:08, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.2497
2023-02-13 16:46:22,840 - mmselfsup - INFO - Epoch [73][350/427]	lr: 8.702e-01, eta: 2:08:35, time: 0.670, data_time: 0.002, memory: 25346, loss: 0.2470
2023-02-13 16:46:56,385 - mmselfsup - INFO - Epoch [73][400/427]	lr: 8.702e-01, eta: 2:08:02, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2614
2023-02-13 16:47:56,373 - mmselfsup - INFO - Epoch [74][50/427]	lr: 8.129e-01, eta: 2:07:07, time: 0.848, data_time: 0.126, memory: 25346, loss: 0.2445
2023-02-13 16:48:29,993 - mmselfsup - INFO - Epoch [74][100/427]	lr: 8.129e-01, eta: 2:06:34, time: 0.672, data_time: 0.003, memory: 25346, loss: 0.2519
2023-02-13 16:49:03,582 - mmselfsup - INFO - Epoch [74][150/427]	lr: 8.129e-01, eta: 2:06:01, time: 0.672, data_time: 0.003, memory: 25346, loss: 0.2432
2023-02-13 16:49:37,277 - mmselfsup - INFO - Epoch [74][200/427]	lr: 8.129e-01, eta: 2:05:28, time: 0.673, data_time: 0.003, memory: 25346, loss: 0.2527
2023-02-13 16:50:10,774 - mmselfsup - INFO - Epoch [74][250/427]	lr: 8.129e-01, eta: 2:04:55, time: 0.671, data_time: 0.004, memory: 25346, loss: 0.2400
2023-02-13 16:50:44,547 - mmselfsup - INFO - Epoch [74][300/427]	lr: 8.129e-01, eta: 2:04:22, time: 0.676, data_time: 0.003, memory: 25346, loss: 0.2468
2023-02-13 16:51:18,043 - mmselfsup - INFO - Epoch [74][350/427]	lr: 8.129e-01, eta: 2:03:49, time: 0.670, data_time: 0.003, memory: 25346, loss: 0.2471
2023-02-13 16:51:51,642 - mmselfsup - INFO - Epoch [74][400/427]	lr: 8.129e-01, eta: 2:03:16, time: 0.673, data_time: 0.003, memory: 25346, loss: 0.2369
2023-02-13 16:52:51,690 - mmselfsup - INFO - Epoch [75][50/427]	lr: 7.571e-01, eta: 2:02:21, time: 0.848, data_time: 0.125, memory: 25346, loss: 0.2446
2023-02-13 16:53:25,236 - mmselfsup - INFO - Epoch [75][100/427]	lr: 7.571e-01, eta: 2:01:48, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.2397
2023-02-13 16:53:58,719 - mmselfsup - INFO - Epoch [75][150/427]	lr: 7.571e-01, eta: 2:01:15, time: 0.669, data_time: 0.001, memory: 25346, loss: 0.2343
2023-02-13 16:54:32,267 - mmselfsup - INFO - Epoch [75][200/427]	lr: 7.571e-01, eta: 2:00:42, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2464
2023-02-13 16:55:05,770 - mmselfsup - INFO - Epoch [75][250/427]	lr: 7.571e-01, eta: 2:00:09, time: 0.671, data_time: 0.002, memory: 25346, loss: 0.2410
2023-02-13 16:55:39,305 - mmselfsup - INFO - Epoch [75][300/427]	lr: 7.571e-01, eta: 1:59:36, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2400
2023-02-13 16:56:12,860 - mmselfsup - INFO - Epoch [75][350/427]	lr: 7.571e-01, eta: 1:59:03, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.2468
2023-02-13 16:56:46,353 - mmselfsup - INFO - Epoch [75][400/427]	lr: 7.571e-01, eta: 1:58:30, time: 0.670, data_time: 0.002, memory: 25346, loss: 0.2312
2023-02-13 16:57:46,428 - mmselfsup - INFO - Epoch [76][50/427]	lr: 7.029e-01, eta: 1:57:36, time: 0.849, data_time: 0.137, memory: 25346, loss: 0.2336
2023-02-13 16:58:20,037 - mmselfsup - INFO - Epoch [76][100/427]	lr: 7.029e-01, eta: 1:57:03, time: 0.672, data_time: 0.004, memory: 25346, loss: 0.2348
2023-02-13 16:58:53,517 - mmselfsup - INFO - Epoch [76][150/427]	lr: 7.029e-01, eta: 1:56:29, time: 0.669, data_time: 0.003, memory: 25346, loss: 0.2357
2023-02-13 16:59:27,321 - mmselfsup - INFO - Epoch [76][200/427]	lr: 7.029e-01, eta: 1:55:56, time: 0.676, data_time: 0.004, memory: 25346, loss: 0.2438
2023-02-13 17:00:00,911 - mmselfsup - INFO - Epoch [76][250/427]	lr: 7.029e-01, eta: 1:55:23, time: 0.672, data_time: 0.003, memory: 25346, loss: 0.2294
2023-02-13 17:00:34,679 - mmselfsup - INFO - Epoch [76][300/427]	lr: 7.029e-01, eta: 1:54:50, time: 0.676, data_time: 0.003, memory: 25346, loss: 0.2333
2023-02-13 17:01:08,268 - mmselfsup - INFO - Epoch [76][350/427]	lr: 7.029e-01, eta: 1:54:17, time: 0.671, data_time: 0.002, memory: 25346, loss: 0.2290
2023-02-13 17:01:41,905 - mmselfsup - INFO - Epoch [76][400/427]	lr: 7.029e-01, eta: 1:53:44, time: 0.672, data_time: 0.003, memory: 25346, loss: 0.2302
2023-02-13 17:02:42,141 - mmselfsup - INFO - Epoch [77][50/427]	lr: 6.505e-01, eta: 1:52:50, time: 0.852, data_time: 0.129, memory: 25346, loss: 0.2372
2023-02-13 17:03:15,828 - mmselfsup - INFO - Epoch [77][100/427]	lr: 6.505e-01, eta: 1:52:17, time: 0.675, data_time: 0.002, memory: 25346, loss: 0.2367
2023-02-13 17:03:49,377 - mmselfsup - INFO - Epoch [77][150/427]	lr: 6.505e-01, eta: 1:51:44, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2416
2023-02-13 17:04:23,051 - mmselfsup - INFO - Epoch [77][200/427]	lr: 6.505e-01, eta: 1:51:11, time: 0.673, data_time: 0.000, memory: 25346, loss: 0.2163
2023-02-13 17:04:56,594 - mmselfsup - INFO - Epoch [77][250/427]	lr: 6.505e-01, eta: 1:50:38, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.2282
2023-02-13 17:05:30,227 - mmselfsup - INFO - Epoch [77][300/427]	lr: 6.505e-01, eta: 1:50:05, time: 0.674, data_time: 0.002, memory: 25346, loss: 0.2410
2023-02-13 17:06:03,772 - mmselfsup - INFO - Epoch [77][350/427]	lr: 6.505e-01, eta: 1:49:32, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.2433
2023-02-13 17:06:37,483 - mmselfsup - INFO - Epoch [77][400/427]	lr: 6.505e-01, eta: 1:48:59, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2215
2023-02-13 17:07:37,512 - mmselfsup - INFO - Epoch [78][50/427]	lr: 5.997e-01, eta: 1:48:05, time: 0.850, data_time: 0.124, memory: 25346, loss: 0.2255
2023-02-13 17:08:11,320 - mmselfsup - INFO - Epoch [78][100/427]	lr: 5.997e-01, eta: 1:47:32, time: 0.676, data_time: 0.003, memory: 25346, loss: 0.2263
2023-02-13 17:08:45,081 - mmselfsup - INFO - Epoch [78][150/427]	lr: 5.997e-01, eta: 1:46:59, time: 0.675, data_time: 0.003, memory: 25346, loss: 0.2289
2023-02-13 17:09:18,691 - mmselfsup - INFO - Epoch [78][200/427]	lr: 5.997e-01, eta: 1:46:26, time: 0.672, data_time: 0.003, memory: 25346, loss: 0.2243
2023-02-13 17:09:52,319 - mmselfsup - INFO - Epoch [78][250/427]	lr: 5.997e-01, eta: 1:45:53, time: 0.673, data_time: 0.003, memory: 25346, loss: 0.2305
2023-02-13 17:10:25,878 - mmselfsup - INFO - Epoch [78][300/427]	lr: 5.997e-01, eta: 1:45:20, time: 0.670, data_time: 0.003, memory: 25346, loss: 0.2269
2023-02-13 17:10:59,469 - mmselfsup - INFO - Epoch [78][350/427]	lr: 5.997e-01, eta: 1:44:47, time: 0.672, data_time: 0.004, memory: 25346, loss: 0.2232
2023-02-13 17:11:32,889 - mmselfsup - INFO - Epoch [78][400/427]	lr: 5.997e-01, eta: 1:44:14, time: 0.670, data_time: 0.004, memory: 25346, loss: 0.2259
2023-02-13 17:12:33,214 - mmselfsup - INFO - Epoch [79][50/427]	lr: 5.508e-01, eta: 1:43:20, time: 0.853, data_time: 0.128, memory: 25346, loss: 0.2194
2023-02-13 17:13:06,775 - mmselfsup - INFO - Epoch [79][100/427]	lr: 5.508e-01, eta: 1:42:47, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2257
2023-02-13 17:13:40,124 - mmselfsup - INFO - Epoch [79][150/427]	lr: 5.508e-01, eta: 1:42:14, time: 0.666, data_time: 0.001, memory: 25346, loss: 0.2197
2023-02-13 17:14:13,487 - mmselfsup - INFO - Epoch [79][200/427]	lr: 5.508e-01, eta: 1:41:41, time: 0.668, data_time: 0.001, memory: 25346, loss: 0.2293
2023-02-13 17:14:47,125 - mmselfsup - INFO - Epoch [79][250/427]	lr: 5.508e-01, eta: 1:41:08, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.2214
2023-02-13 17:15:20,812 - mmselfsup - INFO - Epoch [79][300/427]	lr: 5.508e-01, eta: 1:40:35, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.2270
2023-02-13 17:15:54,345 - mmselfsup - INFO - Epoch [79][350/427]	lr: 5.508e-01, eta: 1:40:02, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.2187
2023-02-13 17:16:27,845 - mmselfsup - INFO - Epoch [79][400/427]	lr: 5.508e-01, eta: 1:39:29, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.2320
2023-02-13 17:17:27,894 - mmselfsup - INFO - Epoch [80][50/427]	lr: 5.036e-01, eta: 1:38:35, time: 0.849, data_time: 0.148, memory: 25346, loss: 0.2264
2023-02-13 17:18:01,598 - mmselfsup - INFO - Epoch [80][100/427]	lr: 5.036e-01, eta: 1:38:02, time: 0.673, data_time: 0.003, memory: 25346, loss: 0.2186
2023-02-13 17:18:35,158 - mmselfsup - INFO - Epoch [80][150/427]	lr: 5.036e-01, eta: 1:37:29, time: 0.671, data_time: 0.004, memory: 25346, loss: 0.2337
2023-02-13 17:19:08,953 - mmselfsup - INFO - Epoch [80][200/427]	lr: 5.036e-01, eta: 1:36:56, time: 0.676, data_time: 0.004, memory: 25346, loss: 0.2254
2023-02-13 17:19:42,632 - mmselfsup - INFO - Epoch [80][250/427]	lr: 5.036e-01, eta: 1:36:23, time: 0.673, data_time: 0.004, memory: 25346, loss: 0.2293
2023-02-13 17:20:16,258 - mmselfsup - INFO - Epoch [80][300/427]	lr: 5.036e-01, eta: 1:35:50, time: 0.672, data_time: 0.004, memory: 25346, loss: 0.2206
2023-02-13 17:20:50,107 - mmselfsup - INFO - Epoch [80][350/427]	lr: 5.036e-01, eta: 1:35:17, time: 0.678, data_time: 0.004, memory: 25346, loss: 0.2204
2023-02-13 17:21:23,815 - mmselfsup - INFO - Epoch [80][400/427]	lr: 5.036e-01, eta: 1:34:44, time: 0.675, data_time: 0.003, memory: 25346, loss: 0.2169
2023-02-13 17:21:41,407 - mmselfsup - INFO - Saving checkpoint at 80 epochs
2023-02-13 17:22:24,082 - mmselfsup - INFO - Epoch [81][50/427]	lr: 4.584e-01, eta: 1:33:50, time: 0.834, data_time: 0.145, memory: 25346, loss: 0.2243
2023-02-13 17:22:57,809 - mmselfsup - INFO - Epoch [81][100/427]	lr: 4.584e-01, eta: 1:33:17, time: 0.675, data_time: 0.001, memory: 25346, loss: 0.2181
2023-02-13 17:23:31,432 - mmselfsup - INFO - Epoch [81][150/427]	lr: 4.584e-01, eta: 1:32:44, time: 0.672, data_time: 0.001, memory: 25346, loss: 0.2284
2023-02-13 17:24:05,117 - mmselfsup - INFO - Epoch [81][200/427]	lr: 4.584e-01, eta: 1:32:11, time: 0.674, data_time: 0.002, memory: 25346, loss: 0.2111
2023-02-13 17:24:38,738 - mmselfsup - INFO - Epoch [81][250/427]	lr: 4.584e-01, eta: 1:31:38, time: 0.673, data_time: 0.002, memory: 25346, loss: 0.2199
2023-02-13 17:25:12,372 - mmselfsup - INFO - Epoch [81][300/427]	lr: 4.584e-01, eta: 1:31:05, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.2213
2023-02-13 17:25:46,149 - mmselfsup - INFO - Epoch [81][350/427]	lr: 4.584e-01, eta: 1:30:32, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2237
2023-02-13 17:26:19,695 - mmselfsup - INFO - Epoch [81][400/427]	lr: 4.584e-01, eta: 1:29:59, time: 0.671, data_time: 0.002, memory: 25346, loss: 0.2142
2023-02-13 17:27:19,868 - mmselfsup - INFO - Epoch [82][50/427]	lr: 4.150e-01, eta: 1:29:06, time: 0.851, data_time: 0.164, memory: 25346, loss: 0.2204
2023-02-13 17:27:54,190 - mmselfsup - INFO - Epoch [82][100/427]	lr: 4.150e-01, eta: 1:28:33, time: 0.687, data_time: 0.004, memory: 25346, loss: 0.2192
2023-02-13 17:28:27,973 - mmselfsup - INFO - Epoch [82][150/427]	lr: 4.150e-01, eta: 1:28:00, time: 0.676, data_time: 0.004, memory: 25346, loss: 0.2205
2023-02-13 17:29:01,772 - mmselfsup - INFO - Epoch [82][200/427]	lr: 4.150e-01, eta: 1:27:27, time: 0.677, data_time: 0.004, memory: 25346, loss: 0.2181
2023-02-13 17:29:35,698 - mmselfsup - INFO - Epoch [82][250/427]	lr: 4.150e-01, eta: 1:26:54, time: 0.679, data_time: 0.003, memory: 25346, loss: 0.2036
2023-02-13 17:30:09,394 - mmselfsup - INFO - Epoch [82][300/427]	lr: 4.150e-01, eta: 1:26:21, time: 0.672, data_time: 0.002, memory: 25346, loss: 0.2093
2023-02-13 17:30:43,084 - mmselfsup - INFO - Epoch [82][350/427]	lr: 4.150e-01, eta: 1:25:48, time: 0.674, data_time: 0.004, memory: 25346, loss: 0.2152
2023-02-13 17:31:16,787 - mmselfsup - INFO - Epoch [82][400/427]	lr: 4.150e-01, eta: 1:25:15, time: 0.675, data_time: 0.004, memory: 25346, loss: 0.2071
2023-02-13 17:32:17,582 - mmselfsup - INFO - Epoch [83][50/427]	lr: 3.736e-01, eta: 1:24:22, time: 0.863, data_time: 0.153, memory: 25346, loss: 0.2176
2023-02-13 17:32:51,390 - mmselfsup - INFO - Epoch [83][100/427]	lr: 3.736e-01, eta: 1:23:49, time: 0.676, data_time: 0.001, memory: 25346, loss: 0.2126
2023-02-13 17:33:25,394 - mmselfsup - INFO - Epoch [83][150/427]	lr: 3.736e-01, eta: 1:23:16, time: 0.679, data_time: 0.001, memory: 25346, loss: 0.2150
2023-02-13 17:33:59,185 - mmselfsup - INFO - Epoch [83][200/427]	lr: 3.736e-01, eta: 1:22:43, time: 0.676, data_time: 0.002, memory: 25346, loss: 0.2137
2023-02-13 17:34:33,114 - mmselfsup - INFO - Epoch [83][250/427]	lr: 3.736e-01, eta: 1:22:10, time: 0.678, data_time: 0.001, memory: 25346, loss: 0.2244
2023-02-13 17:35:06,890 - mmselfsup - INFO - Epoch [83][300/427]	lr: 3.736e-01, eta: 1:21:37, time: 0.676, data_time: 0.002, memory: 25346, loss: 0.2166
2023-02-13 17:35:40,627 - mmselfsup - INFO - Epoch [83][350/427]	lr: 3.736e-01, eta: 1:21:04, time: 0.675, data_time: 0.001, memory: 25346, loss: 0.2205
2023-02-13 17:36:14,311 - mmselfsup - INFO - Epoch [83][400/427]	lr: 3.736e-01, eta: 1:20:31, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2177
2023-02-13 17:37:14,628 - mmselfsup - INFO - Epoch [84][50/427]	lr: 3.342e-01, eta: 1:19:38, time: 0.853, data_time: 0.148, memory: 25346, loss: 0.2099
2023-02-13 17:37:48,431 - mmselfsup - INFO - Epoch [84][100/427]	lr: 3.342e-01, eta: 1:19:05, time: 0.676, data_time: 0.004, memory: 25346, loss: 0.2077
2023-02-13 17:38:22,620 - mmselfsup - INFO - Epoch [84][150/427]	lr: 3.342e-01, eta: 1:18:32, time: 0.684, data_time: 0.004, memory: 25346, loss: 0.2169
2023-02-13 17:38:56,474 - mmselfsup - INFO - Epoch [84][200/427]	lr: 3.342e-01, eta: 1:17:59, time: 0.676, data_time: 0.003, memory: 25346, loss: 0.2059
2023-02-13 17:39:30,287 - mmselfsup - INFO - Epoch [84][250/427]	lr: 3.342e-01, eta: 1:17:26, time: 0.677, data_time: 0.004, memory: 25346, loss: 0.2119
2023-02-13 17:40:04,076 - mmselfsup - INFO - Epoch [84][300/427]	lr: 3.342e-01, eta: 1:16:53, time: 0.676, data_time: 0.004, memory: 25346, loss: 0.2097
2023-02-13 17:40:37,869 - mmselfsup - INFO - Epoch [84][350/427]	lr: 3.342e-01, eta: 1:16:20, time: 0.675, data_time: 0.003, memory: 25346, loss: 0.2204
2023-02-13 17:41:11,657 - mmselfsup - INFO - Epoch [84][400/427]	lr: 3.342e-01, eta: 1:15:47, time: 0.676, data_time: 0.004, memory: 25346, loss: 0.2124
2023-02-13 17:42:12,198 - mmselfsup - INFO - Epoch [85][50/427]	lr: 2.969e-01, eta: 1:14:55, time: 0.857, data_time: 0.142, memory: 25346, loss: 0.2034
2023-02-13 17:42:45,877 - mmselfsup - INFO - Epoch [85][100/427]	lr: 2.969e-01, eta: 1:14:22, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2053
2023-02-13 17:43:19,582 - mmselfsup - INFO - Epoch [85][150/427]	lr: 2.969e-01, eta: 1:13:49, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2131
2023-02-13 17:43:53,427 - mmselfsup - INFO - Epoch [85][200/427]	lr: 2.969e-01, eta: 1:13:16, time: 0.677, data_time: 0.001, memory: 25346, loss: 0.2091
2023-02-13 17:44:27,141 - mmselfsup - INFO - Epoch [85][250/427]	lr: 2.969e-01, eta: 1:12:43, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2153
2023-02-13 17:45:00,842 - mmselfsup - INFO - Epoch [85][300/427]	lr: 2.969e-01, eta: 1:12:10, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2056
2023-02-13 17:45:34,512 - mmselfsup - INFO - Epoch [85][350/427]	lr: 2.969e-01, eta: 1:11:37, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.2056
2023-02-13 17:46:08,228 - mmselfsup - INFO - Epoch [85][400/427]	lr: 2.969e-01, eta: 1:11:03, time: 0.675, data_time: 0.002, memory: 25346, loss: 0.2101
2023-02-13 17:47:08,666 - mmselfsup - INFO - Epoch [86][50/427]	lr: 2.616e-01, eta: 1:10:11, time: 0.854, data_time: 0.098, memory: 25346, loss: 0.2136
2023-02-13 17:47:42,475 - mmselfsup - INFO - Epoch [86][100/427]	lr: 2.616e-01, eta: 1:09:38, time: 0.677, data_time: 0.004, memory: 25346, loss: 0.2092
2023-02-13 17:48:16,148 - mmselfsup - INFO - Epoch [86][150/427]	lr: 2.616e-01, eta: 1:09:05, time: 0.674, data_time: 0.004, memory: 25346, loss: 0.2121
2023-02-13 17:48:49,896 - mmselfsup - INFO - Epoch [86][200/427]	lr: 2.616e-01, eta: 1:08:32, time: 0.675, data_time: 0.003, memory: 25346, loss: 0.2063
2023-02-13 17:49:23,732 - mmselfsup - INFO - Epoch [86][250/427]	lr: 2.616e-01, eta: 1:07:59, time: 0.677, data_time: 0.004, memory: 25346, loss: 0.2112
2023-02-13 17:49:57,531 - mmselfsup - INFO - Epoch [86][300/427]	lr: 2.616e-01, eta: 1:07:26, time: 0.675, data_time: 0.003, memory: 25346, loss: 0.2050
2023-02-13 17:50:31,242 - mmselfsup - INFO - Epoch [86][350/427]	lr: 2.616e-01, eta: 1:06:53, time: 0.675, data_time: 0.004, memory: 25346, loss: 0.2070
2023-02-13 17:51:05,259 - mmselfsup - INFO - Epoch [86][400/427]	lr: 2.616e-01, eta: 1:06:20, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.2144
2023-02-13 17:52:05,792 - mmselfsup - INFO - Epoch [87][50/427]	lr: 2.284e-01, eta: 1:05:27, time: 0.856, data_time: 0.126, memory: 25346, loss: 0.2125
2023-02-13 17:52:39,382 - mmselfsup - INFO - Epoch [87][100/427]	lr: 2.284e-01, eta: 1:04:54, time: 0.672, data_time: 0.001, memory: 25346, loss: 0.2081
2023-02-13 17:53:13,081 - mmselfsup - INFO - Epoch [87][150/427]	lr: 2.284e-01, eta: 1:04:21, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.1970
2023-02-13 17:53:46,767 - mmselfsup - INFO - Epoch [87][200/427]	lr: 2.284e-01, eta: 1:03:48, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.2061
2023-02-13 17:54:20,493 - mmselfsup - INFO - Epoch [87][250/427]	lr: 2.284e-01, eta: 1:03:15, time: 0.675, data_time: 0.001, memory: 25346, loss: 0.2036
2023-02-13 17:54:54,145 - mmselfsup - INFO - Epoch [87][300/427]	lr: 2.284e-01, eta: 1:02:42, time: 0.672, data_time: 0.001, memory: 25346, loss: 0.2049
2023-02-13 17:55:27,768 - mmselfsup - INFO - Epoch [87][350/427]	lr: 2.284e-01, eta: 1:02:09, time: 0.673, data_time: 0.002, memory: 25346, loss: 0.2137
2023-02-13 17:56:01,441 - mmselfsup - INFO - Epoch [87][400/427]	lr: 2.284e-01, eta: 1:01:36, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.2089
2023-02-13 17:57:02,181 - mmselfsup - INFO - Epoch [88][50/427]	lr: 1.974e-01, eta: 1:00:44, time: 0.861, data_time: 0.135, memory: 25346, loss: 0.2094
2023-02-13 17:57:35,923 - mmselfsup - INFO - Epoch [88][100/427]	lr: 1.974e-01, eta: 1:00:11, time: 0.676, data_time: 0.005, memory: 25346, loss: 0.2062
2023-02-13 17:58:09,725 - mmselfsup - INFO - Epoch [88][150/427]	lr: 1.974e-01, eta: 0:59:38, time: 0.676, data_time: 0.003, memory: 25346, loss: 0.2129
2023-02-13 17:58:43,410 - mmselfsup - INFO - Epoch [88][200/427]	lr: 1.974e-01, eta: 0:59:05, time: 0.674, data_time: 0.003, memory: 25346, loss: 0.2050
2023-02-13 17:59:17,131 - mmselfsup - INFO - Epoch [88][250/427]	lr: 1.974e-01, eta: 0:58:32, time: 0.674, data_time: 0.003, memory: 25346, loss: 0.1992
2023-02-13 17:59:51,046 - mmselfsup - INFO - Epoch [88][300/427]	lr: 1.974e-01, eta: 0:57:59, time: 0.679, data_time: 0.003, memory: 25346, loss: 0.2065
2023-02-13 18:00:24,809 - mmselfsup - INFO - Epoch [88][350/427]	lr: 1.974e-01, eta: 0:57:26, time: 0.674, data_time: 0.002, memory: 25346, loss: 0.2042
2023-02-13 18:00:58,637 - mmselfsup - INFO - Epoch [88][400/427]	lr: 1.974e-01, eta: 0:56:53, time: 0.677, data_time: 0.004, memory: 25346, loss: 0.2032
2023-02-13 18:01:59,100 - mmselfsup - INFO - Epoch [89][50/427]	lr: 1.685e-01, eta: 0:56:01, time: 0.856, data_time: 0.152, memory: 25346, loss: 0.2046
2023-02-13 18:02:32,672 - mmselfsup - INFO - Epoch [89][100/427]	lr: 1.685e-01, eta: 0:55:27, time: 0.672, data_time: 0.001, memory: 25346, loss: 0.2096
2023-02-13 18:03:06,454 - mmselfsup - INFO - Epoch [89][150/427]	lr: 1.685e-01, eta: 0:54:54, time: 0.675, data_time: 0.001, memory: 25346, loss: 0.2080
2023-02-13 18:03:40,277 - mmselfsup - INFO - Epoch [89][200/427]	lr: 1.685e-01, eta: 0:54:21, time: 0.677, data_time: 0.002, memory: 25346, loss: 0.2064
2023-02-13 18:04:14,257 - mmselfsup - INFO - Epoch [89][250/427]	lr: 1.685e-01, eta: 0:53:48, time: 0.679, data_time: 0.001, memory: 25346, loss: 0.2035
2023-02-13 18:04:47,941 - mmselfsup - INFO - Epoch [89][300/427]	lr: 1.685e-01, eta: 0:53:15, time: 0.674, data_time: 0.002, memory: 25346, loss: 0.1947
2023-02-13 18:05:21,697 - mmselfsup - INFO - Epoch [89][350/427]	lr: 1.685e-01, eta: 0:52:42, time: 0.675, data_time: 0.001, memory: 25346, loss: 0.2125
2023-02-13 18:05:55,485 - mmselfsup - INFO - Epoch [89][400/427]	lr: 1.685e-01, eta: 0:52:09, time: 0.676, data_time: 0.001, memory: 25346, loss: 0.1996
2023-02-13 18:06:56,478 - mmselfsup - INFO - Epoch [90][50/427]	lr: 1.419e-01, eta: 0:51:17, time: 0.867, data_time: 0.142, memory: 25346, loss: 0.2044
2023-02-13 18:07:30,175 - mmselfsup - INFO - Epoch [90][100/427]	lr: 1.419e-01, eta: 0:50:44, time: 0.675, data_time: 0.004, memory: 25346, loss: 0.2014
2023-02-13 18:08:04,080 - mmselfsup - INFO - Epoch [90][150/427]	lr: 1.419e-01, eta: 0:50:11, time: 0.678, data_time: 0.003, memory: 25346, loss: 0.2091
2023-02-13 18:08:37,848 - mmselfsup - INFO - Epoch [90][200/427]	lr: 1.419e-01, eta: 0:49:38, time: 0.674, data_time: 0.003, memory: 25346, loss: 0.2072
2023-02-13 18:09:11,793 - mmselfsup - INFO - Epoch [90][250/427]	lr: 1.419e-01, eta: 0:49:05, time: 0.678, data_time: 0.004, memory: 25346, loss: 0.2026
2023-02-13 18:09:45,803 - mmselfsup - INFO - Epoch [90][300/427]	lr: 1.419e-01, eta: 0:48:32, time: 0.681, data_time: 0.004, memory: 25346, loss: 0.2104
2023-02-13 18:10:19,681 - mmselfsup - INFO - Epoch [90][350/427]	lr: 1.419e-01, eta: 0:47:59, time: 0.677, data_time: 0.004, memory: 25346, loss: 0.2007
2023-02-13 18:10:53,419 - mmselfsup - INFO - Epoch [90][400/427]	lr: 1.419e-01, eta: 0:47:26, time: 0.675, data_time: 0.004, memory: 25346, loss: 0.2020
2023-02-13 18:11:11,116 - mmselfsup - INFO - Saving checkpoint at 90 epochs
2023-02-13 18:11:54,259 - mmselfsup - INFO - Epoch [91][50/427]	lr: 1.175e-01, eta: 0:46:34, time: 0.844, data_time: 0.156, memory: 25346, loss: 0.2103
2023-02-13 18:12:28,050 - mmselfsup - INFO - Epoch [91][100/427]	lr: 1.175e-01, eta: 0:46:01, time: 0.676, data_time: 0.001, memory: 25346, loss: 0.1991
2023-02-13 18:13:01,690 - mmselfsup - INFO - Epoch [91][150/427]	lr: 1.175e-01, eta: 0:45:28, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.1959
2023-02-13 18:13:35,455 - mmselfsup - INFO - Epoch [91][200/427]	lr: 1.175e-01, eta: 0:44:55, time: 0.675, data_time: 0.001, memory: 25346, loss: 0.2102
2023-02-13 18:14:09,247 - mmselfsup - INFO - Epoch [91][250/427]	lr: 1.175e-01, eta: 0:44:22, time: 0.676, data_time: 0.001, memory: 25346, loss: 0.2048
2023-02-13 18:14:42,945 - mmselfsup - INFO - Epoch [91][300/427]	lr: 1.175e-01, eta: 0:43:49, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2045
2023-02-13 18:15:16,653 - mmselfsup - INFO - Epoch [91][350/427]	lr: 1.175e-01, eta: 0:43:16, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.2071
2023-02-13 18:15:50,389 - mmselfsup - INFO - Epoch [91][400/427]	lr: 1.175e-01, eta: 0:42:43, time: 0.675, data_time: 0.001, memory: 25346, loss: 0.2005
2023-02-13 18:16:50,853 - mmselfsup - INFO - Epoch [92][50/427]	lr: 9.530e-02, eta: 0:41:51, time: 0.857, data_time: 0.159, memory: 25346, loss: 0.1950
2023-02-13 18:17:24,813 - mmselfsup - INFO - Epoch [92][100/427]	lr: 9.530e-02, eta: 0:41:18, time: 0.678, data_time: 0.003, memory: 25346, loss: 0.1981
2023-02-13 18:17:58,644 - mmselfsup - INFO - Epoch [92][150/427]	lr: 9.530e-02, eta: 0:40:45, time: 0.677, data_time: 0.004, memory: 25346, loss: 0.1978
2023-02-13 18:18:32,800 - mmselfsup - INFO - Epoch [92][200/427]	lr: 9.530e-02, eta: 0:40:12, time: 0.683, data_time: 0.004, memory: 25346, loss: 0.1993
2023-02-13 18:19:06,794 - mmselfsup - INFO - Epoch [92][250/427]	lr: 9.530e-02, eta: 0:39:39, time: 0.679, data_time: 0.004, memory: 25346, loss: 0.2122
2023-02-13 18:19:40,724 - mmselfsup - INFO - Epoch [92][300/427]	lr: 9.530e-02, eta: 0:39:06, time: 0.679, data_time: 0.004, memory: 25346, loss: 0.2011
2023-02-13 18:20:15,339 - mmselfsup - INFO - Epoch [92][350/427]	lr: 9.530e-02, eta: 0:38:33, time: 0.693, data_time: 0.004, memory: 25346, loss: 0.2068
2023-02-13 18:20:50,854 - mmselfsup - INFO - Epoch [92][400/427]	lr: 9.530e-02, eta: 0:38:00, time: 0.710, data_time: 0.003, memory: 25346, loss: 0.2015
2023-02-13 18:21:51,500 - mmselfsup - INFO - Epoch [93][50/427]	lr: 7.540e-02, eta: 0:37:08, time: 0.853, data_time: 0.110, memory: 25346, loss: 0.2066
2023-02-13 18:22:27,023 - mmselfsup - INFO - Epoch [93][100/427]	lr: 7.540e-02, eta: 0:36:35, time: 0.710, data_time: 0.001, memory: 25346, loss: 0.2027
2023-02-13 18:23:02,313 - mmselfsup - INFO - Epoch [93][150/427]	lr: 7.540e-02, eta: 0:36:02, time: 0.706, data_time: 0.001, memory: 25346, loss: 0.2078
2023-02-13 18:23:37,685 - mmselfsup - INFO - Epoch [93][200/427]	lr: 7.540e-02, eta: 0:35:29, time: 0.707, data_time: 0.001, memory: 25346, loss: 0.1992
2023-02-13 18:24:13,036 - mmselfsup - INFO - Epoch [93][250/427]	lr: 7.540e-02, eta: 0:34:56, time: 0.707, data_time: 0.001, memory: 25346, loss: 0.2024
2023-02-13 18:24:48,202 - mmselfsup - INFO - Epoch [93][300/427]	lr: 7.540e-02, eta: 0:34:24, time: 0.703, data_time: 0.001, memory: 25346, loss: 0.1999
2023-02-13 18:25:23,406 - mmselfsup - INFO - Epoch [93][350/427]	lr: 7.540e-02, eta: 0:33:51, time: 0.704, data_time: 0.001, memory: 25346, loss: 0.2025
2023-02-13 18:25:58,636 - mmselfsup - INFO - Epoch [93][400/427]	lr: 7.540e-02, eta: 0:33:18, time: 0.705, data_time: 0.001, memory: 25346, loss: 0.1956
2023-02-13 18:26:59,392 - mmselfsup - INFO - Epoch [94][50/427]	lr: 5.780e-02, eta: 0:32:26, time: 0.853, data_time: 0.125, memory: 25346, loss: 0.1987
2023-02-13 18:27:34,605 - mmselfsup - INFO - Epoch [94][100/427]	lr: 5.780e-02, eta: 0:31:53, time: 0.705, data_time: 0.004, memory: 25346, loss: 0.2045
2023-02-13 18:28:09,837 - mmselfsup - INFO - Epoch [94][150/427]	lr: 5.780e-02, eta: 0:31:20, time: 0.705, data_time: 0.003, memory: 25346, loss: 0.2017
2023-02-13 18:28:45,174 - mmselfsup - INFO - Epoch [94][200/427]	lr: 5.780e-02, eta: 0:30:47, time: 0.706, data_time: 0.002, memory: 25346, loss: 0.2059
2023-02-13 18:29:20,414 - mmselfsup - INFO - Epoch [94][250/427]	lr: 5.780e-02, eta: 0:30:14, time: 0.705, data_time: 0.003, memory: 25346, loss: 0.1980
2023-02-13 18:29:55,804 - mmselfsup - INFO - Epoch [94][300/427]	lr: 5.780e-02, eta: 0:29:41, time: 0.708, data_time: 0.003, memory: 25346, loss: 0.1929
2023-02-13 18:30:31,165 - mmselfsup - INFO - Epoch [94][350/427]	lr: 5.780e-02, eta: 0:29:08, time: 0.707, data_time: 0.003, memory: 25346, loss: 0.2034
2023-02-13 18:31:06,333 - mmselfsup - INFO - Epoch [94][400/427]	lr: 5.780e-02, eta: 0:28:35, time: 0.703, data_time: 0.003, memory: 25346, loss: 0.1988
2023-02-13 18:32:07,555 - mmselfsup - INFO - Epoch [95][50/427]	lr: 4.251e-02, eta: 0:27:44, time: 0.864, data_time: 0.121, memory: 25346, loss: 0.1942
2023-02-13 18:32:42,859 - mmselfsup - INFO - Epoch [95][100/427]	lr: 4.251e-02, eta: 0:27:11, time: 0.705, data_time: 0.001, memory: 25346, loss: 0.1951
2023-02-13 18:33:18,161 - mmselfsup - INFO - Epoch [95][150/427]	lr: 4.251e-02, eta: 0:26:38, time: 0.707, data_time: 0.001, memory: 25346, loss: 0.1978
2023-02-13 18:33:53,312 - mmselfsup - INFO - Epoch [95][200/427]	lr: 4.251e-02, eta: 0:26:05, time: 0.702, data_time: 0.001, memory: 25346, loss: 0.2046
2023-02-13 18:34:28,426 - mmselfsup - INFO - Epoch [95][250/427]	lr: 4.251e-02, eta: 0:25:32, time: 0.703, data_time: 0.002, memory: 25346, loss: 0.1927
2023-02-13 18:35:03,617 - mmselfsup - INFO - Epoch [95][300/427]	lr: 4.251e-02, eta: 0:24:59, time: 0.704, data_time: 0.001, memory: 25346, loss: 0.2011
2023-02-13 18:35:38,930 - mmselfsup - INFO - Epoch [95][350/427]	lr: 4.251e-02, eta: 0:24:26, time: 0.706, data_time: 0.001, memory: 25346, loss: 0.2018
2023-02-13 18:36:14,218 - mmselfsup - INFO - Epoch [95][400/427]	lr: 4.251e-02, eta: 0:23:53, time: 0.706, data_time: 0.001, memory: 25346, loss: 0.2016
2023-02-13 18:37:15,216 - mmselfsup - INFO - Epoch [96][50/427]	lr: 2.955e-02, eta: 0:23:01, time: 0.861, data_time: 0.097, memory: 25346, loss: 0.2069
2023-02-13 18:37:50,638 - mmselfsup - INFO - Epoch [96][100/427]	lr: 2.955e-02, eta: 0:22:28, time: 0.708, data_time: 0.003, memory: 25346, loss: 0.1996
2023-02-13 18:38:25,916 - mmselfsup - INFO - Epoch [96][150/427]	lr: 2.955e-02, eta: 0:21:55, time: 0.706, data_time: 0.003, memory: 25346, loss: 0.1934
2023-02-13 18:39:01,158 - mmselfsup - INFO - Epoch [96][200/427]	lr: 2.955e-02, eta: 0:21:22, time: 0.705, data_time: 0.003, memory: 25346, loss: 0.2067
2023-02-13 18:39:36,513 - mmselfsup - INFO - Epoch [96][250/427]	lr: 2.955e-02, eta: 0:20:49, time: 0.707, data_time: 0.003, memory: 25346, loss: 0.1969
2023-02-13 18:40:11,666 - mmselfsup - INFO - Epoch [96][300/427]	lr: 2.955e-02, eta: 0:20:16, time: 0.703, data_time: 0.003, memory: 25346, loss: 0.1923
2023-02-13 18:40:46,792 - mmselfsup - INFO - Epoch [96][350/427]	lr: 2.955e-02, eta: 0:19:43, time: 0.703, data_time: 0.003, memory: 25346, loss: 0.2032
2023-02-13 18:41:22,031 - mmselfsup - INFO - Epoch [96][400/427]	lr: 2.955e-02, eta: 0:19:10, time: 0.705, data_time: 0.003, memory: 25346, loss: 0.1952
2023-02-13 18:42:23,313 - mmselfsup - INFO - Epoch [97][50/427]	lr: 1.892e-02, eta: 0:18:18, time: 0.866, data_time: 0.130, memory: 25346, loss: 0.2049
2023-02-13 18:42:58,525 - mmselfsup - INFO - Epoch [97][100/427]	lr: 1.892e-02, eta: 0:17:45, time: 0.704, data_time: 0.001, memory: 25346, loss: 0.2017
2023-02-13 18:43:33,789 - mmselfsup - INFO - Epoch [97][150/427]	lr: 1.892e-02, eta: 0:17:12, time: 0.705, data_time: 0.001, memory: 25346, loss: 0.2094
2023-02-13 18:44:08,918 - mmselfsup - INFO - Epoch [97][200/427]	lr: 1.892e-02, eta: 0:16:39, time: 0.702, data_time: 0.001, memory: 25346, loss: 0.1991
2023-02-13 18:44:43,933 - mmselfsup - INFO - Epoch [97][250/427]	lr: 1.892e-02, eta: 0:16:06, time: 0.700, data_time: 0.001, memory: 25346, loss: 0.2012
2023-02-13 18:45:19,185 - mmselfsup - INFO - Epoch [97][300/427]	lr: 1.892e-02, eta: 0:15:33, time: 0.705, data_time: 0.001, memory: 25346, loss: 0.2038
2023-02-13 18:45:54,204 - mmselfsup - INFO - Epoch [97][350/427]	lr: 1.892e-02, eta: 0:15:00, time: 0.700, data_time: 0.001, memory: 25346, loss: 0.1945
2023-02-13 18:46:29,464 - mmselfsup - INFO - Epoch [97][400/427]	lr: 1.892e-02, eta: 0:14:27, time: 0.705, data_time: 0.001, memory: 25346, loss: 0.2034
2023-02-13 18:47:30,271 - mmselfsup - INFO - Epoch [98][50/427]	lr: 1.065e-02, eta: 0:13:36, time: 0.857, data_time: 0.120, memory: 25346, loss: 0.1890
2023-02-13 18:48:05,354 - mmselfsup - INFO - Epoch [98][100/427]	lr: 1.065e-02, eta: 0:13:03, time: 0.701, data_time: 0.002, memory: 25346, loss: 0.2040
2023-02-13 18:48:40,541 - mmselfsup - INFO - Epoch [98][150/427]	lr: 1.065e-02, eta: 0:12:29, time: 0.704, data_time: 0.003, memory: 25346, loss: 0.1961
2023-02-13 18:49:15,622 - mmselfsup - INFO - Epoch [98][200/427]	lr: 1.065e-02, eta: 0:11:56, time: 0.701, data_time: 0.003, memory: 25346, loss: 0.1927
2023-02-13 18:49:50,616 - mmselfsup - INFO - Epoch [98][250/427]	lr: 1.065e-02, eta: 0:11:23, time: 0.700, data_time: 0.003, memory: 25346, loss: 0.1989
2023-02-13 18:50:25,898 - mmselfsup - INFO - Epoch [98][300/427]	lr: 1.065e-02, eta: 0:10:50, time: 0.706, data_time: 0.003, memory: 25346, loss: 0.1997
2023-02-13 18:51:01,008 - mmselfsup - INFO - Epoch [98][350/427]	lr: 1.065e-02, eta: 0:10:17, time: 0.703, data_time: 0.003, memory: 25346, loss: 0.1981
2023-02-13 18:51:36,174 - mmselfsup - INFO - Epoch [98][400/427]	lr: 1.065e-02, eta: 0:09:44, time: 0.703, data_time: 0.003, memory: 25346, loss: 0.1983
2023-02-13 18:52:37,009 - mmselfsup - INFO - Epoch [99][50/427]	lr: 4.736e-03, eta: 0:08:53, time: 0.856, data_time: 0.132, memory: 25346, loss: 0.1940
2023-02-13 18:53:12,127 - mmselfsup - INFO - Epoch [99][100/427]	lr: 4.736e-03, eta: 0:08:20, time: 0.702, data_time: 0.001, memory: 25346, loss: 0.1990
2023-02-13 18:53:47,187 - mmselfsup - INFO - Epoch [99][150/427]	lr: 4.736e-03, eta: 0:07:46, time: 0.701, data_time: 0.001, memory: 25346, loss: 0.2008
2023-02-13 18:54:22,350 - mmselfsup - INFO - Epoch [99][200/427]	lr: 4.736e-03, eta: 0:07:13, time: 0.703, data_time: 0.001, memory: 25346, loss: 0.1971
2023-02-13 18:54:57,564 - mmselfsup - INFO - Epoch [99][250/427]	lr: 4.736e-03, eta: 0:06:40, time: 0.705, data_time: 0.001, memory: 25346, loss: 0.1958
2023-02-13 18:55:32,589 - mmselfsup - INFO - Epoch [99][300/427]	lr: 4.736e-03, eta: 0:06:07, time: 0.701, data_time: 0.001, memory: 25346, loss: 0.1967
2023-02-13 18:56:07,758 - mmselfsup - INFO - Epoch [99][350/427]	lr: 4.736e-03, eta: 0:05:34, time: 0.703, data_time: 0.001, memory: 25346, loss: 0.1953
2023-02-13 18:56:42,858 - mmselfsup - INFO - Epoch [99][400/427]	lr: 4.736e-03, eta: 0:05:01, time: 0.702, data_time: 0.001, memory: 25346, loss: 0.1967
2023-02-13 18:57:43,358 - mmselfsup - INFO - Epoch [100][50/427]	lr: 1.184e-03, eta: 0:04:10, time: 0.852, data_time: 0.107, memory: 25346, loss: 0.1953
2023-02-13 18:58:18,543 - mmselfsup - INFO - Epoch [100][100/427]	lr: 1.184e-03, eta: 0:03:36, time: 0.704, data_time: 0.003, memory: 25346, loss: 0.1964
2023-02-13 18:58:53,862 - mmselfsup - INFO - Epoch [100][150/427]	lr: 1.184e-03, eta: 0:03:03, time: 0.706, data_time: 0.003, memory: 25346, loss: 0.1965
2023-02-13 18:59:29,058 - mmselfsup - INFO - Epoch [100][200/427]	lr: 1.184e-03, eta: 0:02:30, time: 0.704, data_time: 0.003, memory: 25346, loss: 0.1972
2023-02-13 19:00:04,253 - mmselfsup - INFO - Epoch [100][250/427]	lr: 1.184e-03, eta: 0:01:57, time: 0.704, data_time: 0.003, memory: 25346, loss: 0.1986
2023-02-13 19:00:39,323 - mmselfsup - INFO - Epoch [100][300/427]	lr: 1.184e-03, eta: 0:01:24, time: 0.701, data_time: 0.003, memory: 25346, loss: 0.1938
2023-02-13 19:01:14,772 - mmselfsup - INFO - Epoch [100][350/427]	lr: 1.184e-03, eta: 0:00:51, time: 0.709, data_time: 0.003, memory: 25346, loss: 0.2041
2023-02-13 19:01:49,783 - mmselfsup - INFO - Epoch [100][400/427]	lr: 1.184e-03, eta: 0:00:17, time: 0.700, data_time: 0.003, memory: 25346, loss: 0.2057
2023-02-13 19:02:07,746 - mmselfsup - INFO - Saving checkpoint at 100 epochs
