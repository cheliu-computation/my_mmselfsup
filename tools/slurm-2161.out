Mon Feb 13 21:03:11 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                  Off |
| N/A   23C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |                  Off |
| N/A   22C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                  Off |
| N/A   34C    P0    62W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |                  Off |
| N/A   24C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA A100-SXM...  On   | 00000000:87:00.0 Off |                  Off |
| N/A   27C    P0    53W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA A100-SXM...  On   | 00000000:90:00.0 Off |                  Off |
| N/A   26C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                  Off |
| N/A   35C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                  Off |
| N/A   27C    P0    53W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Thu_Nov_18_09:45:30_PST_2021
Cuda compilation tools, release 11.5, V11.5.119
Build cuda_11.5.r11.5/compiler.30672275_0
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/cl522/github_repo/my_mmselfsup/mmselfsup/utils/setup_env.py:42: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
2023-02-13 21:03:22,387 - mmselfsup - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.11 (main, Mar 29 2022, 19:08:29) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-40GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.5, V11.5.119
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.11.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0
OpenCV: 4.7.0
MMCV: 1.7.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSelfSup: 0.10.1+c3f655a
------------------------------------------------------------

2023-02-13 21:03:23,039 - mmselfsup - INFO - Distributed training: True
2023-02-13 21:03:23,592 - mmselfsup - INFO - Config:
model = dict(
    type='BYOL',
    base_momentum=0.99,
    backbone=dict(
        type='ResNet',
        depth=50,
        in_channels=3,
        out_indices=[4],
        norm_cfg=dict(type='SyncBN')),
    neck=dict(
        type='NonLinearNeck',
        in_channels=2048,
        hid_channels=4096,
        out_channels=256,
        num_layers=2,
        with_bias=True,
        with_last_bn=False,
        with_avg_pool=True),
    head=dict(
        type='LatentPredictHead',
        predictor=dict(
            type='NonLinearNeck',
            in_channels=256,
            hid_channels=4096,
            out_channels=256,
            num_layers=2,
            with_bias=True,
            with_last_bn=False,
            with_avg_pool=False)))
data_source = 'CXR'
dataset_type_mimic = 'MultiViewDatasetMIMIC'
dataset_type_cxr14 = 'MultiViewDatasetNIH'
dataset_type_cxp = 'MultiViewDatasetCXP'
dataset_type_pdc = 'MultiViewDatasetPDC'
img_norm_cfg = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
train_pipeline1 = [
    dict(
        type='Normalize',
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]),
    dict(type='RandomResizedCrop', size=224, interpolation=3),
    dict(type='RandomHorizontalFlip')
]
train_pipeline2 = [
    dict(
        type='Normalize',
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]),
    dict(type='RandomResizedCrop', size=224, interpolation=3),
    dict(type='RandomHorizontalFlip')
]
prefetch = False
data = dict(
    samples_per_gpu=256,
    workers_per_gpu=8,
    train=[
        dict(
            type='MultiViewDatasetPDC',
            data_source=dict(type='CXR', data_prefix='cxr', ann_file='cxr'),
            num_views=[1, 1],
            pipelines=[[{
                'type': 'Normalize',
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            }, {
                'type': 'RandomResizedCrop',
                'size': 224,
                'interpolation': 3
            }, {
                'type': 'RandomHorizontalFlip'
            }],
                       [{
                           'type': 'Normalize',
                           'mean': [0.485, 0.456, 0.406],
                           'std': [0.229, 0.224, 0.225]
                       }, {
                           'type': 'RandomResizedCrop',
                           'size': 224,
                           'interpolation': 3
                       }, {
                           'type': 'RandomHorizontalFlip'
                       }]],
            prefetch=False),
        dict(
            type='MultiViewDatasetNIH',
            data_source=dict(type='CXR', data_prefix='cxr', ann_file='cxr'),
            num_views=[1, 1],
            pipelines=[[{
                'type': 'Normalize',
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            }, {
                'type': 'RandomResizedCrop',
                'size': 224,
                'interpolation': 3
            }, {
                'type': 'RandomHorizontalFlip'
            }],
                       [{
                           'type': 'Normalize',
                           'mean': [0.485, 0.456, 0.406],
                           'std': [0.229, 0.224, 0.225]
                       }, {
                           'type': 'RandomResizedCrop',
                           'size': 224,
                           'interpolation': 3
                       }, {
                           'type': 'RandomHorizontalFlip'
                       }]],
            prefetch=False),
        dict(
            type='MultiViewDatasetCXP',
            data_source=dict(type='CXR', data_prefix='cxr', ann_file='cxr'),
            num_views=[1, 1],
            pipelines=[[{
                'type': 'Normalize',
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            }, {
                'type': 'RandomResizedCrop',
                'size': 224,
                'interpolation': 3
            }, {
                'type': 'RandomHorizontalFlip'
            }],
                       [{
                           'type': 'Normalize',
                           'mean': [0.485, 0.456, 0.406],
                           'std': [0.229, 0.224, 0.225]
                       }, {
                           'type': 'RandomResizedCrop',
                           'size': 224,
                           'interpolation': 3
                       }, {
                           'type': 'RandomHorizontalFlip'
                       }]],
            prefetch=False),
        dict(
            type='MultiViewDatasetMIMIC',
            data_source=dict(type='CXR', data_prefix='cxr', ann_file='cxr'),
            num_views=[1, 1],
            pipelines=[[{
                'type': 'Normalize',
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            }, {
                'type': 'RandomResizedCrop',
                'size': 224,
                'interpolation': 3
            }, {
                'type': 'RandomHorizontalFlip'
            }],
                       [{
                           'type': 'Normalize',
                           'mean': [0.485, 0.456, 0.406],
                           'std': [0.229, 0.224, 0.225]
                       }, {
                           'type': 'RandomResizedCrop',
                           'size': 224,
                           'interpolation': 3
                       }, {
                           'type': 'RandomHorizontalFlip'
                       }]],
            prefetch=False)
    ])
optimizer = dict(
    type='LARS',
    lr=4.8,
    weight_decay=1e-06,
    momentum=0.9,
    paramwise_options=dict({
        '(bn|gn)(\d+)?.(weight|bias)':
        dict(weight_decay=0.0, lars_exclude=True),
        'bias':
        dict(weight_decay=0.0, lars_exclude=True)
    }))
optimizer_config = dict(update_interval=2)
lr_config = dict(
    policy='CosineAnnealing',
    min_lr=0.0,
    warmup='linear',
    warmup_iters=10,
    warmup_ratio=0.0001,
    warmup_by_epoch=True)
runner = dict(type='EpochBasedRunner', max_epochs=100)
checkpoint_config = dict(interval=10, max_keep_ckpts=3)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
cudnn_benchmark = True
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
persistent_workers = True
opencv_num_threads = 0
mp_start_method = 'fork'
update_interval = 2
custom_hooks = [dict(type='BYOLHook', end_momentum=1.0, update_interval=2)]
fp16 = dict(loss_scale=512.0)
work_dir = '/home/cl522/github_repo/res50_allCXR_log/BYOL'
auto_resume = False
gpu_ids = range(0, 8)

2023-02-13 21:03:23,592 - mmselfsup - INFO - Set random seed to 0, deterministic: False
2023-02-13 21:03:24,007 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,020 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,024 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,058 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,068 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,084 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,086 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,097 - mmcv - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': ['Conv2d']}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,147 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,151 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,153 - mmcv - INFO - 
online_net.0.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,154 - mmcv - INFO - 
online_net.0.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,155 - mmcv - INFO - 
online_net.0.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,156 - mmcv - INFO - 
online_net.0.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.0.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.1.fc0.weight - torch.Size([4096, 2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,157 - mmcv - INFO - 
online_net.1.fc0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
online_net.1.bn0.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
online_net.1.bn0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
online_net.1.fc1.weight - torch.Size([256, 4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,158 - mmcv - INFO - 
target_net.0.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,159 - mmcv - INFO - 
target_net.0.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,160 - mmcv - INFO - 
target_net.0.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,161 - mmcv - INFO - 
target_net.0.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.0.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.1.fc0.weight - torch.Size([4096, 2048]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.1.fc0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,162 - mmcv - INFO - 
target_net.1.bn0.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,163 - mmcv - INFO - 
target_net.1.bn0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,163 - mmcv - INFO - 
target_net.1.fc1.weight - torch.Size([256, 4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,163 - mmcv - INFO - 
head.predictor.fc0.weight - torch.Size([4096, 256]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,163 - mmcv - INFO - 
head.predictor.fc0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,163 - mmcv - INFO - 
head.predictor.bn0.weight - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,163 - mmcv - INFO - 
head.predictor.bn0.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
2023-02-13 21:03:24,163 - mmcv - INFO - 
head.predictor.fc1.weight - torch.Size([256, 4096]): 
The value is the same before and after calling `init_weights` of BYOL  
 
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 21:03:24,164 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,164 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 21:03:24,169 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 21:03:24,198 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,203 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 21:03:24,208 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,212 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 21:03:24,225 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,228 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,229 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 21:03:24,232 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
2023-02-13 21:03:24,238 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2023-02-13 21:03:24,243 - mmcv - INFO - initialize NonLinearNeck with init_cfg [{'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
-------
checking cfg!!!
cfg is list or tuple
-------
checking cfg!!!
{'type': 'MultiViewDatasetPDC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
/home/cl522/miniconda3/envs/mmself/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
-------
checking cfg!!!
{'type': 'MultiViewDatasetNIH', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetCXP', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
-------
checking cfg!!!
{'type': 'MultiViewDatasetMIMIC', 'data_source': {'type': 'CXR', 'data_prefix': 'cxr', 'ann_file': 'cxr'}, 'num_views': [1, 1], 'pipelines': [[{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}], [{'type': 'Normalize', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, {'type': 'RandomResizedCrop', 'size': 224, 'interpolation': 3}, {'type': 'RandomHorizontalFlip'}]], 'prefetch': False}
paramwise_options --                                     online_net.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.0.downsample.1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.0.downsample.1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer1.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer1.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.0.downsample.1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.0.downsample.1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer2.3.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer2.3.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.0.downsample.1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.0.downsample.1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.3.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.3.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.4.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.4.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer3.5.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer3.5.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.0.downsample.1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.0.downsample.1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.1.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.1.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn1.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn1.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn1.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn1.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn2.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn2.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn2.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn2.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn3.weight: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn3.weight: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.0.layer4.2.bn3.bias: weight_decay=0.0
paramwise_options --                                     online_net.0.layer4.2.bn3.bias: lars_exclude=True
paramwise_options --                                     online_net.1.fc0.bias: weight_decay=0.0
paramwise_options --                                     online_net.1.fc0.bias: lars_exclude=True
paramwise_options --                                     online_net.1.bn0.weight: weight_decay=0.0
paramwise_options --                                     online_net.1.bn0.weight: lars_exclude=True
paramwise_options --                                     online_net.1.bn0.bias: weight_decay=0.0
paramwise_options --                                     online_net.1.bn0.bias: lars_exclude=True
paramwise_options --                                     online_net.1.bn0.bias: weight_decay=0.0
paramwise_options --                                     online_net.1.bn0.bias: lars_exclude=True
paramwise_options --                                     head.predictor.fc0.bias: weight_decay=0.0
paramwise_options --                                     head.predictor.fc0.bias: lars_exclude=True
paramwise_options --                                     head.predictor.bn0.weight: weight_decay=0.0
paramwise_options --                                     head.predictor.bn0.weight: lars_exclude=True
paramwise_options --                                     head.predictor.bn0.bias: weight_decay=0.0
paramwise_options --                                     head.predictor.bn0.bias: lars_exclude=True
paramwise_options --                                     head.predictor.bn0.bias: weight_decay=0.0
paramwise_options --                                     head.predictor.bn0.bias: lars_exclude=True
2023-02-13 21:03:32,043 - mmselfsup - INFO - Start running, host: cl522@ese-hivemind, work_dir: /home/cl522/github_repo/res50_allCXR_log/BYOL
2023-02-13 21:03:32,043 - mmselfsup - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(ABOVE_NORMAL) GradAccumFp16OptimizerHook         
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) MomentumUpdateHook                 
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) GradAccumFp16OptimizerHook         
(NORMAL      ) CheckpointHook                     
(NORMAL      ) MomentumUpdateHook                 
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-13 21:03:32,043 - mmselfsup - INFO - workflow: [('train', 1)], max: 100 epochs
2023-02-13 21:03:32,044 - mmselfsup - INFO - Checkpoints will be saved to /home/cl522/github_repo/res50_allCXR_log/BYOL by HardDiskBackend.
2023-02-13 21:03:50,984 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 21:03:50,985 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 21:03:50,985 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 21:03:50,985 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 21:03:50,985 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 21:03:50,985 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 21:03:50,985 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 21:03:50,985 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2023-02-13 21:04:23,398 - mmselfsup - INFO - Epoch [1][50/427]	lr: 5.556e-02, eta: 12:10:01, time: 1.027, data_time: 0.134, memory: 25346, loss: 6.7951
2023-02-13 21:04:56,368 - mmselfsup - INFO - Epoch [1][100/427]	lr: 1.118e-01, eta: 9:58:40, time: 0.659, data_time: 0.001, memory: 25346, loss: 3.2116
2023-02-13 21:05:29,224 - mmselfsup - INFO - Epoch [1][150/427]	lr: 1.680e-01, eta: 9:13:58, time: 0.657, data_time: 0.001, memory: 25346, loss: 2.3562
2023-02-13 21:06:02,283 - mmselfsup - INFO - Epoch [1][200/427]	lr: 2.242e-01, eta: 8:52:04, time: 0.661, data_time: 0.001, memory: 25346, loss: 2.0546
2023-02-13 21:06:35,145 - mmselfsup - INFO - Epoch [1][250/427]	lr: 2.804e-01, eta: 8:38:09, time: 0.657, data_time: 0.001, memory: 25346, loss: 1.9109
2023-02-13 21:07:08,033 - mmselfsup - INFO - Epoch [1][300/427]	lr: 3.366e-01, eta: 8:28:45, time: 0.658, data_time: 0.001, memory: 25346, loss: 1.8163
2023-02-13 21:07:40,913 - mmselfsup - INFO - Epoch [1][350/427]	lr: 3.928e-01, eta: 8:21:52, time: 0.658, data_time: 0.001, memory: 25346, loss: 1.7858
2023-02-13 21:08:13,804 - mmselfsup - INFO - Epoch [1][400/427]	lr: 4.490e-01, eta: 8:16:35, time: 0.658, data_time: 0.001, memory: 25346, loss: 1.7296
2023-02-13 21:09:14,110 - mmselfsup - INFO - Epoch [2][50/427]	lr: 5.354e-01, eta: 7:54:27, time: 0.797, data_time: 0.094, memory: 25346, loss: 1.5366
2023-02-13 21:09:47,244 - mmselfsup - INFO - Epoch [2][100/427]	lr: 5.916e-01, eta: 7:53:07, time: 0.663, data_time: 0.003, memory: 25346, loss: 1.4103
2023-02-13 21:10:20,195 - mmselfsup - INFO - Epoch [2][150/427]	lr: 6.477e-01, eta: 7:51:42, time: 0.659, data_time: 0.003, memory: 25346, loss: 1.2996
2023-02-13 21:10:53,230 - mmselfsup - INFO - Epoch [2][200/427]	lr: 7.039e-01, eta: 7:50:31, time: 0.661, data_time: 0.003, memory: 25346, loss: 1.1651
2023-02-13 21:11:26,218 - mmselfsup - INFO - Epoch [2][250/427]	lr: 7.601e-01, eta: 7:49:22, time: 0.660, data_time: 0.003, memory: 25346, loss: 1.0904
2023-02-13 21:11:59,256 - mmselfsup - INFO - Epoch [2][300/427]	lr: 8.163e-01, eta: 7:48:21, time: 0.661, data_time: 0.003, memory: 25346, loss: 0.9678
2023-02-13 21:12:32,274 - mmselfsup - INFO - Epoch [2][350/427]	lr: 8.725e-01, eta: 7:47:23, time: 0.660, data_time: 0.003, memory: 25346, loss: 0.9501
2023-02-13 21:13:05,361 - mmselfsup - INFO - Epoch [2][400/427]	lr: 9.287e-01, eta: 7:46:31, time: 0.662, data_time: 0.003, memory: 25346, loss: 0.9102
2023-02-13 21:14:03,009 - mmselfsup - INFO - Epoch [3][50/427]	lr: 1.014e+00, eta: 7:36:55, time: 0.802, data_time: 0.122, memory: 25346, loss: 0.8702
2023-02-13 21:14:36,166 - mmselfsup - INFO - Epoch [3][100/427]	lr: 1.071e+00, eta: 7:36:38, time: 0.663, data_time: 0.001, memory: 25346, loss: 0.8503
2023-02-13 21:15:09,319 - mmselfsup - INFO - Epoch [3][150/427]	lr: 1.127e+00, eta: 7:36:19, time: 0.663, data_time: 0.001, memory: 25346, loss: 0.7595
2023-02-13 21:15:42,474 - mmselfsup - INFO - Epoch [3][200/427]	lr: 1.183e+00, eta: 7:35:59, time: 0.663, data_time: 0.001, memory: 25346, loss: 0.7039
2023-02-13 21:16:15,750 - mmselfsup - INFO - Epoch [3][250/427]	lr: 1.239e+00, eta: 7:35:42, time: 0.665, data_time: 0.001, memory: 25346, loss: 0.6797
2023-02-13 21:16:48,949 - mmselfsup - INFO - Epoch [3][300/427]	lr: 1.295e+00, eta: 7:35:21, time: 0.664, data_time: 0.001, memory: 25346, loss: 0.6384
2023-02-13 21:17:22,146 - mmselfsup - INFO - Epoch [3][350/427]	lr: 1.351e+00, eta: 7:34:59, time: 0.664, data_time: 0.001, memory: 25346, loss: 0.6094
2023-02-13 21:17:55,368 - mmselfsup - INFO - Epoch [3][400/427]	lr: 1.407e+00, eta: 7:34:37, time: 0.664, data_time: 0.001, memory: 25346, loss: 0.5704
2023-02-13 21:18:52,437 - mmselfsup - INFO - Epoch [4][50/427]	lr: 1.492e+00, eta: 7:28:00, time: 0.791, data_time: 0.113, memory: 25346, loss: 0.5353
2023-02-13 21:19:25,741 - mmselfsup - INFO - Epoch [4][100/427]	lr: 1.548e+00, eta: 7:27:52, time: 0.666, data_time: 0.003, memory: 25346, loss: 0.5017
2023-02-13 21:19:59,064 - mmselfsup - INFO - Epoch [4][150/427]	lr: 1.604e+00, eta: 7:27:43, time: 0.666, data_time: 0.003, memory: 25346, loss: 0.4839
2023-02-13 21:20:32,449 - mmselfsup - INFO - Epoch [4][200/427]	lr: 1.660e+00, eta: 7:27:34, time: 0.668, data_time: 0.003, memory: 25346, loss: 0.4719
2023-02-13 21:21:05,880 - mmselfsup - INFO - Epoch [4][250/427]	lr: 1.716e+00, eta: 7:27:24, time: 0.669, data_time: 0.003, memory: 25346, loss: 0.4627
2023-02-13 21:21:39,258 - mmselfsup - INFO - Epoch [4][300/427]	lr: 1.772e+00, eta: 7:27:12, time: 0.668, data_time: 0.003, memory: 25346, loss: 0.4432
2023-02-13 21:22:12,604 - mmselfsup - INFO - Epoch [4][350/427]	lr: 1.829e+00, eta: 7:26:57, time: 0.667, data_time: 0.003, memory: 25346, loss: 0.4371
2023-02-13 21:22:46,071 - mmselfsup - INFO - Epoch [4][400/427]	lr: 1.885e+00, eta: 7:26:44, time: 0.669, data_time: 0.003, memory: 25346, loss: 0.4258
2023-02-13 21:23:44,879 - mmselfsup - INFO - Epoch [5][50/427]	lr: 1.968e+00, eta: 7:22:22, time: 0.824, data_time: 0.101, memory: 25346, loss: 0.4215
2023-02-13 21:24:18,420 - mmselfsup - INFO - Epoch [5][100/427]	lr: 2.024e+00, eta: 7:22:15, time: 0.671, data_time: 0.001, memory: 25346, loss: 0.4098
2023-02-13 21:24:52,021 - mmselfsup - INFO - Epoch [5][150/427]	lr: 2.080e+00, eta: 7:22:08, time: 0.672, data_time: 0.001, memory: 25346, loss: 0.4066
2023-02-13 21:25:25,496 - mmselfsup - INFO - Epoch [5][200/427]	lr: 2.136e+00, eta: 7:21:57, time: 0.669, data_time: 0.001, memory: 25346, loss: 0.4018
2023-02-13 21:25:59,036 - mmselfsup - INFO - Epoch [5][250/427]	lr: 2.191e+00, eta: 7:21:46, time: 0.671, data_time: 0.000, memory: 25346, loss: 0.3915
2023-02-13 21:26:32,549 - mmselfsup - INFO - Epoch [5][300/427]	lr: 2.247e+00, eta: 7:21:33, time: 0.670, data_time: 0.001, memory: 25346, loss: 0.3958
2023-02-13 21:27:06,191 - mmselfsup - INFO - Epoch [5][350/427]	lr: 2.303e+00, eta: 7:21:22, time: 0.673, data_time: 0.000, memory: 25346, loss: 0.3937
2023-02-13 21:27:39,903 - mmselfsup - INFO - Epoch [5][400/427]	lr: 2.359e+00, eta: 7:21:11, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.3903
2023-02-13 21:28:38,343 - mmselfsup - INFO - Epoch [6][50/427]	lr: 2.440e+00, eta: 7:17:27, time: 0.816, data_time: 0.087, memory: 25346, loss: 0.3837
2023-02-13 21:29:11,933 - mmselfsup - INFO - Epoch [6][100/427]	lr: 2.496e+00, eta: 7:17:16, time: 0.672, data_time: 0.003, memory: 25346, loss: 0.3827
2023-02-13 21:29:45,568 - mmselfsup - INFO - Epoch [6][150/427]	lr: 2.552e+00, eta: 7:17:05, time: 0.673, data_time: 0.003, memory: 25346, loss: 0.3831
2023-02-13 21:30:19,187 - mmselfsup - INFO - Epoch [6][200/427]	lr: 2.608e+00, eta: 7:16:53, time: 0.672, data_time: 0.003, memory: 25346, loss: 0.3790
2023-02-13 21:30:52,854 - mmselfsup - INFO - Epoch [6][250/427]	lr: 2.664e+00, eta: 7:16:41, time: 0.673, data_time: 0.003, memory: 25346, loss: 0.3806
2023-02-13 21:31:26,678 - mmselfsup - INFO - Epoch [6][300/427]	lr: 2.719e+00, eta: 7:16:30, time: 0.676, data_time: 0.003, memory: 25346, loss: 0.3749
2023-02-13 21:32:00,523 - mmselfsup - INFO - Epoch [6][350/427]	lr: 2.775e+00, eta: 7:16:19, time: 0.677, data_time: 0.003, memory: 25346, loss: 0.3783
2023-02-13 21:32:34,243 - mmselfsup - INFO - Epoch [6][400/427]	lr: 2.831e+00, eta: 7:16:05, time: 0.674, data_time: 0.003, memory: 25346, loss: 0.3759
2023-02-13 21:33:32,259 - mmselfsup - INFO - Epoch [7][50/427]	lr: 2.909e+00, eta: 7:12:44, time: 0.807, data_time: 0.086, memory: 25346, loss: 0.3779
2023-02-13 21:34:05,925 - mmselfsup - INFO - Epoch [7][100/427]	lr: 2.965e+00, eta: 7:12:31, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.3823
2023-02-13 21:34:39,650 - mmselfsup - INFO - Epoch [7][150/427]	lr: 3.021e+00, eta: 7:12:18, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.3839
2023-02-13 21:35:13,339 - mmselfsup - INFO - Epoch [7][200/427]	lr: 3.076e+00, eta: 7:12:04, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.3819
2023-02-13 21:35:47,032 - mmselfsup - INFO - Epoch [7][250/427]	lr: 3.132e+00, eta: 7:11:49, time: 0.674, data_time: 0.001, memory: 25346, loss: 0.3836
2023-02-13 21:36:20,667 - mmselfsup - INFO - Epoch [7][300/427]	lr: 3.188e+00, eta: 7:11:33, time: 0.673, data_time: 0.001, memory: 25346, loss: 0.3845
2023-02-13 21:36:54,411 - mmselfsup - INFO - Epoch [7][350/427]	lr: 3.243e+00, eta: 7:11:17, time: 0.675, data_time: 0.001, memory: 25346, loss: 0.3847
2023-02-13 21:37:28,076 - mmselfsup - INFO - Epoch [7][400/427]	lr: 3.299e+00, eta: 7:11:00, time: 0.673, data_time: 0.000, memory: 25346, loss: 0.3897
2023-02-13 21:38:27,219 - mmselfsup - INFO - Epoch [8][50/427]	lr: 3.374e+00, eta: 7:08:18, time: 0.830, data_time: 0.100, memory: 25346, loss: 0.3934
2023-02-13 21:39:00,897 - mmselfsup - INFO - Epoch [8][100/427]	lr: 3.430e+00, eta: 7:08:02, time: 0.674, data_time: 0.003, memory: 25346, loss: 0.3959
2023-02-13 21:39:34,717 - mmselfsup - INFO - Epoch [8][150/427]	lr: 3.485e+00, eta: 7:07:47, time: 0.676, data_time: 0.003, memory: 25346, loss: 0.3991
2023-02-13 21:40:08,496 - mmselfsup - INFO - Epoch [8][200/427]	lr: 3.541e+00, eta: 7:07:31, time: 0.676, data_time: 0.003, memory: 25346, loss: 0.3996
2023-02-13 21:40:42,451 - mmselfsup - INFO - Epoch [8][250/427]	lr: 3.596e+00, eta: 7:07:17, time: 0.679, data_time: 0.003, memory: 25346, loss: 0.3998
2023-02-13 21:41:16,323 - mmselfsup - INFO - Epoch [8][300/427]	lr: 3.652e+00, eta: 7:07:01, time: 0.677, data_time: 0.003, memory: 25346, loss: 0.4106
2023-02-13 21:41:50,191 - mmselfsup - INFO - Epoch [8][350/427]	lr: 3.707e+00, eta: 7:06:45, time: 0.677, data_time: 0.003, memory: 25346, loss: 0.4090
2023-02-13 21:42:24,042 - mmselfsup - INFO - Epoch [8][400/427]	lr: 3.763e+00, eta: 7:06:28, time: 0.677, data_time: 0.003, memory: 25346, loss: 0.4160
2023-02-13 21:43:22,843 - mmselfsup - INFO - Epoch [9][50/427]	lr: 3.834e+00, eta: 7:03:55, time: 0.822, data_time: 0.115, memory: 25346, loss: 0.4175
2023-02-13 21:43:56,637 - mmselfsup - INFO - Epoch [9][100/427]	lr: 3.889e+00, eta: 7:03:38, time: 0.676, data_time: 0.001, memory: 25346, loss: 0.4180
2023-02-13 21:44:30,437 - mmselfsup - INFO - Epoch [9][150/427]	lr: 3.945e+00, eta: 7:03:21, time: 0.676, data_time: 0.001, memory: 25346, loss: 0.4230
2023-02-13 21:45:04,286 - mmselfsup - INFO - Epoch [9][200/427]	lr: 4.000e+00, eta: 7:03:03, time: 0.677, data_time: 0.001, memory: 25346, loss: 0.4299
2023-02-13 21:45:38,214 - mmselfsup - INFO - Epoch [9][250/427]	lr: 4.055e+00, eta: 7:02:46, time: 0.679, data_time: 0.001, memory: 25346, loss: 0.4318
2023-02-13 21:46:12,013 - mmselfsup - INFO - Epoch [9][300/427]	lr: 4.111e+00, eta: 7:02:28, time: 0.676, data_time: 0.001, memory: 25346, loss: 0.4365
2023-02-13 21:46:45,758 - mmselfsup - INFO - Epoch [9][350/427]	lr: 4.166e+00, eta: 7:02:08, time: 0.675, data_time: 0.001, memory: 25346, loss: 0.4396
2023-02-13 21:47:19,578 - mmselfsup - INFO - Epoch [9][400/427]	lr: 4.221e+00, eta: 7:01:49, time: 0.676, data_time: 0.001, memory: 25346, loss: 0.4419
2023-02-13 21:48:17,951 - mmselfsup - INFO - Epoch [10][50/427]	lr: 4.288e+00, eta: 6:59:25, time: 0.814, data_time: 0.084, memory: 25346, loss: 0.4508
2023-02-13 21:48:51,902 - mmselfsup - INFO - Epoch [10][100/427]	lr: 4.343e+00, eta: 6:59:07, time: 0.679, data_time: 0.003, memory: 25346, loss: 0.4567
2023-02-13 21:49:25,811 - mmselfsup - INFO - Epoch [10][150/427]	lr: 4.398e+00, eta: 6:58:49, time: 0.678, data_time: 0.003, memory: 25346, loss: 0.4610
2023-02-13 21:49:59,672 - mmselfsup - INFO - Epoch [10][200/427]	lr: 4.454e+00, eta: 6:58:30, time: 0.677, data_time: 0.003, memory: 25346, loss: 0.4667
2023-02-13 21:50:33,655 - mmselfsup - INFO - Epoch [10][250/427]	lr: 4.509e+00, eta: 6:58:12, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.4689
2023-02-13 21:51:07,594 - mmselfsup - INFO - Epoch [10][300/427]	lr: 4.564e+00, eta: 6:57:52, time: 0.679, data_time: 0.003, memory: 25346, loss: 0.4739
2023-02-13 21:51:41,441 - mmselfsup - INFO - Epoch [10][350/427]	lr: 4.619e+00, eta: 6:57:32, time: 0.677, data_time: 0.003, memory: 25346, loss: 0.4758
2023-02-13 21:52:15,342 - mmselfsup - INFO - Epoch [10][400/427]	lr: 4.674e+00, eta: 6:57:12, time: 0.678, data_time: 0.003, memory: 25346, loss: 0.4800
2023-02-13 21:52:33,007 - mmselfsup - INFO - Saving checkpoint at 10 epochs
2023-02-13 21:53:13,887 - mmselfsup - INFO - Epoch [11][50/427]	lr: 4.683e+00, eta: 6:54:51, time: 0.798, data_time: 0.103, memory: 25346, loss: 0.4938
2023-02-13 21:53:47,761 - mmselfsup - INFO - Epoch [11][100/427]	lr: 4.683e+00, eta: 6:54:31, time: 0.678, data_time: 0.001, memory: 25346, loss: 0.4998
2023-02-13 21:54:21,673 - mmselfsup - INFO - Epoch [11][150/427]	lr: 4.683e+00, eta: 6:54:12, time: 0.678, data_time: 0.001, memory: 25346, loss: 0.4935
2023-02-13 21:54:55,615 - mmselfsup - INFO - Epoch [11][200/427]	lr: 4.683e+00, eta: 6:53:52, time: 0.679, data_time: 0.001, memory: 25346, loss: 0.5021
2023-02-13 21:55:29,464 - mmselfsup - INFO - Epoch [11][250/427]	lr: 4.683e+00, eta: 6:53:31, time: 0.677, data_time: 0.001, memory: 25346, loss: 0.5033
2023-02-13 21:56:03,365 - mmselfsup - INFO - Epoch [11][300/427]	lr: 4.683e+00, eta: 6:53:10, time: 0.678, data_time: 0.001, memory: 25346, loss: 0.5061
2023-02-13 21:56:37,351 - mmselfsup - INFO - Epoch [11][350/427]	lr: 4.683e+00, eta: 6:52:50, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.5152
2023-02-13 21:57:11,182 - mmselfsup - INFO - Epoch [11][400/427]	lr: 4.683e+00, eta: 6:52:28, time: 0.677, data_time: 0.001, memory: 25346, loss: 0.5143
2023-02-13 21:58:09,127 - mmselfsup - INFO - Epoch [12][50/427]	lr: 4.658e+00, eta: 6:50:19, time: 0.806, data_time: 0.114, memory: 25346, loss: 0.5266
2023-02-13 21:58:43,173 - mmselfsup - INFO - Epoch [12][100/427]	lr: 4.658e+00, eta: 6:50:00, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.5240
2023-02-13 21:59:17,170 - mmselfsup - INFO - Epoch [12][150/427]	lr: 4.658e+00, eta: 6:49:39, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.5266
2023-02-13 21:59:51,138 - mmselfsup - INFO - Epoch [12][200/427]	lr: 4.658e+00, eta: 6:49:18, time: 0.679, data_time: 0.003, memory: 25346, loss: 0.5341
2023-02-13 22:00:25,025 - mmselfsup - INFO - Epoch [12][250/427]	lr: 4.658e+00, eta: 6:48:57, time: 0.678, data_time: 0.003, memory: 25346, loss: 0.5339
2023-02-13 22:00:58,988 - mmselfsup - INFO - Epoch [12][300/427]	lr: 4.658e+00, eta: 6:48:35, time: 0.679, data_time: 0.003, memory: 25346, loss: 0.5430
2023-02-13 22:01:32,873 - mmselfsup - INFO - Epoch [12][350/427]	lr: 4.658e+00, eta: 6:48:13, time: 0.678, data_time: 0.003, memory: 25346, loss: 0.5464
2023-02-13 22:02:06,836 - mmselfsup - INFO - Epoch [12][400/427]	lr: 4.658e+00, eta: 6:47:51, time: 0.679, data_time: 0.003, memory: 25346, loss: 0.5481
2023-02-13 22:03:05,880 - mmselfsup - INFO - Epoch [13][50/427]	lr: 4.631e+00, eta: 6:45:58, time: 0.828, data_time: 0.105, memory: 25346, loss: 0.5564
2023-02-13 22:03:39,943 - mmselfsup - INFO - Epoch [13][100/427]	lr: 4.631e+00, eta: 6:45:37, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.5591
2023-02-13 22:04:13,824 - mmselfsup - INFO - Epoch [13][150/427]	lr: 4.631e+00, eta: 6:45:14, time: 0.678, data_time: 0.001, memory: 25346, loss: 0.5699
2023-02-13 22:04:47,663 - mmselfsup - INFO - Epoch [13][200/427]	lr: 4.631e+00, eta: 6:44:51, time: 0.677, data_time: 0.001, memory: 25346, loss: 0.5663
2023-02-13 22:05:21,710 - mmselfsup - INFO - Epoch [13][250/427]	lr: 4.631e+00, eta: 6:44:29, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.5667
2023-02-13 22:05:55,710 - mmselfsup - INFO - Epoch [13][300/427]	lr: 4.631e+00, eta: 6:44:07, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.5761
2023-02-13 22:06:29,718 - mmselfsup - INFO - Epoch [13][350/427]	lr: 4.631e+00, eta: 6:43:45, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.5792
2023-02-13 22:07:03,607 - mmselfsup - INFO - Epoch [13][400/427]	lr: 4.631e+00, eta: 6:43:21, time: 0.678, data_time: 0.001, memory: 25346, loss: 0.5778
2023-02-13 22:08:02,547 - mmselfsup - INFO - Epoch [14][50/427]	lr: 4.603e+00, eta: 6:41:32, time: 0.824, data_time: 0.111, memory: 25346, loss: 0.5846
2023-02-13 22:08:36,575 - mmselfsup - INFO - Epoch [14][100/427]	lr: 4.603e+00, eta: 6:41:10, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.5923
2023-02-13 22:09:10,591 - mmselfsup - INFO - Epoch [14][150/427]	lr: 4.603e+00, eta: 6:40:47, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.5917
2023-02-13 22:09:44,596 - mmselfsup - INFO - Epoch [14][200/427]	lr: 4.603e+00, eta: 6:40:24, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.5995
2023-02-13 22:10:18,482 - mmselfsup - INFO - Epoch [14][250/427]	lr: 4.603e+00, eta: 6:40:01, time: 0.678, data_time: 0.003, memory: 25346, loss: 0.6037
2023-02-13 22:10:52,490 - mmselfsup - INFO - Epoch [14][300/427]	lr: 4.603e+00, eta: 6:39:37, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.6170
2023-02-13 22:11:26,488 - mmselfsup - INFO - Epoch [14][350/427]	lr: 4.603e+00, eta: 6:39:14, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.6073
2023-02-13 22:12:00,464 - mmselfsup - INFO - Epoch [14][400/427]	lr: 4.603e+00, eta: 6:38:50, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.6093
2023-02-13 22:12:59,684 - mmselfsup - INFO - Epoch [15][50/427]	lr: 4.572e+00, eta: 6:37:08, time: 0.831, data_time: 0.095, memory: 25346, loss: 0.6143
2023-02-13 22:13:33,703 - mmselfsup - INFO - Epoch [15][100/427]	lr: 4.572e+00, eta: 6:36:45, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.6174
2023-02-13 22:14:07,849 - mmselfsup - INFO - Epoch [15][150/427]	lr: 4.572e+00, eta: 6:36:22, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.6219
2023-02-13 22:14:41,857 - mmselfsup - INFO - Epoch [15][200/427]	lr: 4.572e+00, eta: 6:35:58, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.6266
2023-02-13 22:15:15,893 - mmselfsup - INFO - Epoch [15][250/427]	lr: 4.572e+00, eta: 6:35:35, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.6376
2023-02-13 22:15:49,882 - mmselfsup - INFO - Epoch [15][300/427]	lr: 4.572e+00, eta: 6:35:10, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.6325
2023-02-13 22:16:23,877 - mmselfsup - INFO - Epoch [15][350/427]	lr: 4.572e+00, eta: 6:34:46, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.6394
2023-02-13 22:16:57,899 - mmselfsup - INFO - Epoch [15][400/427]	lr: 4.572e+00, eta: 6:34:22, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.6368
2023-02-13 22:17:56,841 - mmselfsup - INFO - Epoch [16][50/427]	lr: 4.538e+00, eta: 6:32:42, time: 0.826, data_time: 0.108, memory: 25346, loss: 0.6419
2023-02-13 22:18:30,915 - mmselfsup - INFO - Epoch [16][100/427]	lr: 4.538e+00, eta: 6:32:18, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.6613
2023-02-13 22:19:04,945 - mmselfsup - INFO - Epoch [16][150/427]	lr: 4.538e+00, eta: 6:31:54, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.6572
2023-02-13 22:19:38,966 - mmselfsup - INFO - Epoch [16][200/427]	lr: 4.538e+00, eta: 6:31:30, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.6584
2023-02-13 22:20:12,967 - mmselfsup - INFO - Epoch [16][250/427]	lr: 4.538e+00, eta: 6:31:05, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.6626
2023-02-13 22:20:46,992 - mmselfsup - INFO - Epoch [16][300/427]	lr: 4.538e+00, eta: 6:30:40, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.6621
2023-02-13 22:21:21,055 - mmselfsup - INFO - Epoch [16][350/427]	lr: 4.538e+00, eta: 6:30:16, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.6630
2023-02-13 22:21:55,155 - mmselfsup - INFO - Epoch [16][400/427]	lr: 4.538e+00, eta: 6:29:51, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.6661
2023-02-13 22:22:53,799 - mmselfsup - INFO - Epoch [17][50/427]	lr: 4.503e+00, eta: 6:28:13, time: 0.819, data_time: 0.105, memory: 25346, loss: 0.6804
2023-02-13 22:23:27,952 - mmselfsup - INFO - Epoch [17][100/427]	lr: 4.503e+00, eta: 6:27:49, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7039
2023-02-13 22:24:02,030 - mmselfsup - INFO - Epoch [17][150/427]	lr: 4.503e+00, eta: 6:27:24, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.6853
2023-02-13 22:24:36,114 - mmselfsup - INFO - Epoch [17][200/427]	lr: 4.503e+00, eta: 6:27:00, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.6998
2023-02-13 22:25:10,244 - mmselfsup - INFO - Epoch [17][250/427]	lr: 4.503e+00, eta: 6:26:35, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.6992
2023-02-13 22:25:44,425 - mmselfsup - INFO - Epoch [17][300/427]	lr: 4.503e+00, eta: 6:26:10, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.6922
2023-02-13 22:26:18,462 - mmselfsup - INFO - Epoch [17][350/427]	lr: 4.503e+00, eta: 6:25:45, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.7060
2023-02-13 22:26:52,562 - mmselfsup - INFO - Epoch [17][400/427]	lr: 4.503e+00, eta: 6:25:20, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7056
2023-02-13 22:27:51,884 - mmselfsup - INFO - Epoch [18][50/427]	lr: 4.466e+00, eta: 6:23:48, time: 0.832, data_time: 0.111, memory: 25346, loss: 0.7024
2023-02-13 22:28:26,097 - mmselfsup - INFO - Epoch [18][100/427]	lr: 4.466e+00, eta: 6:23:24, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.6994
2023-02-13 22:29:00,122 - mmselfsup - INFO - Epoch [18][150/427]	lr: 4.466e+00, eta: 6:22:58, time: 0.680, data_time: 0.003, memory: 25346, loss: 0.7054
2023-02-13 22:29:34,307 - mmselfsup - INFO - Epoch [18][200/427]	lr: 4.466e+00, eta: 6:22:33, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7083
2023-02-13 22:30:08,357 - mmselfsup - INFO - Epoch [18][250/427]	lr: 4.466e+00, eta: 6:22:08, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.7082
2023-02-13 22:30:42,542 - mmselfsup - INFO - Epoch [18][300/427]	lr: 4.466e+00, eta: 6:21:43, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7134
2023-02-13 22:31:16,605 - mmselfsup - INFO - Epoch [18][350/427]	lr: 4.466e+00, eta: 6:21:17, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.7323
2023-02-13 22:31:50,680 - mmselfsup - INFO - Epoch [18][400/427]	lr: 4.466e+00, eta: 6:20:51, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.7262
2023-02-13 22:32:49,867 - mmselfsup - INFO - Epoch [19][50/427]	lr: 4.426e+00, eta: 6:19:21, time: 0.830, data_time: 0.109, memory: 25346, loss: 0.7196
2023-02-13 22:33:23,956 - mmselfsup - INFO - Epoch [19][100/427]	lr: 4.426e+00, eta: 6:18:56, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7142
2023-02-13 22:33:58,092 - mmselfsup - INFO - Epoch [19][150/427]	lr: 4.426e+00, eta: 6:18:30, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7255
2023-02-13 22:34:32,171 - mmselfsup - INFO - Epoch [19][200/427]	lr: 4.426e+00, eta: 6:18:04, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7220
2023-02-13 22:35:06,234 - mmselfsup - INFO - Epoch [19][250/427]	lr: 4.426e+00, eta: 6:17:38, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.7290
2023-02-13 22:35:40,396 - mmselfsup - INFO - Epoch [19][300/427]	lr: 4.426e+00, eta: 6:17:12, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7347
2023-02-13 22:36:14,521 - mmselfsup - INFO - Epoch [19][350/427]	lr: 4.426e+00, eta: 6:16:46, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7390
2023-02-13 22:36:48,535 - mmselfsup - INFO - Epoch [19][400/427]	lr: 4.426e+00, eta: 6:16:20, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.7405
2023-02-13 22:37:47,475 - mmselfsup - INFO - Epoch [20][50/427]	lr: 4.385e+00, eta: 6:14:51, time: 0.825, data_time: 0.099, memory: 25346, loss: 0.7343
2023-02-13 22:38:21,613 - mmselfsup - INFO - Epoch [20][100/427]	lr: 4.385e+00, eta: 6:14:26, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.7385
2023-02-13 22:38:55,686 - mmselfsup - INFO - Epoch [20][150/427]	lr: 4.385e+00, eta: 6:13:59, time: 0.681, data_time: 0.003, memory: 25346, loss: 0.7359
2023-02-13 22:39:29,852 - mmselfsup - INFO - Epoch [20][200/427]	lr: 4.385e+00, eta: 6:13:33, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.7450
2023-02-13 22:40:03,940 - mmselfsup - INFO - Epoch [20][250/427]	lr: 4.385e+00, eta: 6:13:07, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.7521
2023-02-13 22:40:38,083 - mmselfsup - INFO - Epoch [20][300/427]	lr: 4.385e+00, eta: 6:12:40, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.7582
2023-02-13 22:41:12,159 - mmselfsup - INFO - Epoch [20][350/427]	lr: 4.385e+00, eta: 6:12:14, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.7462
2023-02-13 22:41:46,245 - mmselfsup - INFO - Epoch [20][400/427]	lr: 4.385e+00, eta: 6:11:47, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.7557
2023-02-13 22:42:03,936 - mmselfsup - INFO - Saving checkpoint at 20 epochs
2023-02-13 22:42:45,250 - mmselfsup - INFO - Epoch [21][50/427]	lr: 4.342e+00, eta: 6:10:17, time: 0.807, data_time: 0.119, memory: 25346, loss: 0.7493
2023-02-13 22:43:19,250 - mmselfsup - INFO - Epoch [21][100/427]	lr: 4.342e+00, eta: 6:09:51, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.7507
2023-02-13 22:43:53,243 - mmselfsup - INFO - Epoch [21][150/427]	lr: 4.342e+00, eta: 6:09:23, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.7508
2023-02-13 22:44:27,450 - mmselfsup - INFO - Epoch [21][200/427]	lr: 4.342e+00, eta: 6:08:57, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.7548
2023-02-13 22:45:01,684 - mmselfsup - INFO - Epoch [21][250/427]	lr: 4.342e+00, eta: 6:08:31, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.7579
2023-02-13 22:45:35,791 - mmselfsup - INFO - Epoch [21][300/427]	lr: 4.342e+00, eta: 6:08:04, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7608
2023-02-13 22:46:09,871 - mmselfsup - INFO - Epoch [21][350/427]	lr: 4.342e+00, eta: 6:07:37, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7724
2023-02-13 22:46:43,976 - mmselfsup - INFO - Epoch [21][400/427]	lr: 4.342e+00, eta: 6:07:10, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7641
2023-02-13 22:47:42,887 - mmselfsup - INFO - Epoch [22][50/427]	lr: 4.296e+00, eta: 6:05:46, time: 0.824, data_time: 0.105, memory: 25346, loss: 0.7637
2023-02-13 22:48:17,121 - mmselfsup - INFO - Epoch [22][100/427]	lr: 4.296e+00, eta: 6:05:20, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.7635
2023-02-13 22:48:51,274 - mmselfsup - INFO - Epoch [22][150/427]	lr: 4.296e+00, eta: 6:04:53, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.7705
2023-02-13 22:49:25,455 - mmselfsup - INFO - Epoch [22][200/427]	lr: 4.296e+00, eta: 6:04:26, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7718
2023-02-13 22:49:59,704 - mmselfsup - INFO - Epoch [22][250/427]	lr: 4.296e+00, eta: 6:03:59, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.8106
2023-02-13 22:50:33,849 - mmselfsup - INFO - Epoch [22][300/427]	lr: 4.296e+00, eta: 6:03:32, time: 0.683, data_time: 0.003, memory: 25346, loss: 1.0011
2023-02-13 22:51:08,064 - mmselfsup - INFO - Epoch [22][350/427]	lr: 4.296e+00, eta: 6:03:05, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.8292
2023-02-13 22:51:42,262 - mmselfsup - INFO - Epoch [22][400/427]	lr: 4.296e+00, eta: 6:02:38, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7827
2023-02-13 22:52:41,865 - mmselfsup - INFO - Epoch [23][50/427]	lr: 4.249e+00, eta: 6:01:18, time: 0.837, data_time: 0.097, memory: 25346, loss: 0.7831
2023-02-13 22:53:15,968 - mmselfsup - INFO - Epoch [23][100/427]	lr: 4.249e+00, eta: 6:00:51, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7754
2023-02-13 22:53:49,969 - mmselfsup - INFO - Epoch [23][150/427]	lr: 4.249e+00, eta: 6:00:23, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.7753
2023-02-13 22:54:24,141 - mmselfsup - INFO - Epoch [23][200/427]	lr: 4.249e+00, eta: 5:59:56, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7694
2023-02-13 22:54:58,384 - mmselfsup - INFO - Epoch [23][250/427]	lr: 4.249e+00, eta: 5:59:29, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.7805
2023-02-13 22:55:32,469 - mmselfsup - INFO - Epoch [23][300/427]	lr: 4.249e+00, eta: 5:59:01, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7928
2023-02-13 22:56:06,690 - mmselfsup - INFO - Epoch [23][350/427]	lr: 4.249e+00, eta: 5:58:34, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.8051
2023-02-13 22:56:40,909 - mmselfsup - INFO - Epoch [23][400/427]	lr: 4.249e+00, eta: 5:58:07, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.7850
2023-02-13 22:57:39,858 - mmselfsup - INFO - Epoch [24][50/427]	lr: 4.200e+00, eta: 5:56:47, time: 0.825, data_time: 0.103, memory: 25346, loss: 0.7847
2023-02-13 22:58:14,144 - mmselfsup - INFO - Epoch [24][100/427]	lr: 4.200e+00, eta: 5:56:19, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.7885
2023-02-13 22:58:48,266 - mmselfsup - INFO - Epoch [24][150/427]	lr: 4.200e+00, eta: 5:55:52, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.7793
2023-02-13 22:59:22,513 - mmselfsup - INFO - Epoch [24][200/427]	lr: 4.200e+00, eta: 5:55:24, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.7769
2023-02-13 22:59:56,837 - mmselfsup - INFO - Epoch [24][250/427]	lr: 4.200e+00, eta: 5:54:57, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.7728
2023-02-13 23:00:31,048 - mmselfsup - INFO - Epoch [24][300/427]	lr: 4.200e+00, eta: 5:54:30, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7769
2023-02-13 23:01:05,280 - mmselfsup - INFO - Epoch [24][350/427]	lr: 4.200e+00, eta: 5:54:02, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.7660
2023-02-13 23:01:39,522 - mmselfsup - INFO - Epoch [24][400/427]	lr: 4.200e+00, eta: 5:53:35, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.7732
2023-02-13 23:02:38,636 - mmselfsup - INFO - Epoch [25][50/427]	lr: 4.150e+00, eta: 5:52:16, time: 0.828, data_time: 0.111, memory: 25346, loss: 0.7722
2023-02-13 23:03:12,729 - mmselfsup - INFO - Epoch [25][100/427]	lr: 4.150e+00, eta: 5:51:48, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7626
2023-02-13 23:03:46,901 - mmselfsup - INFO - Epoch [25][150/427]	lr: 4.150e+00, eta: 5:51:21, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7697
2023-02-13 23:04:20,999 - mmselfsup - INFO - Epoch [25][200/427]	lr: 4.150e+00, eta: 5:50:52, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7689
2023-02-13 23:04:55,126 - mmselfsup - INFO - Epoch [25][250/427]	lr: 4.150e+00, eta: 5:50:24, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7635
2023-02-13 23:05:29,351 - mmselfsup - INFO - Epoch [25][300/427]	lr: 4.150e+00, eta: 5:49:57, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.7677
2023-02-13 23:06:03,636 - mmselfsup - INFO - Epoch [25][350/427]	lr: 4.150e+00, eta: 5:49:29, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.7650
2023-02-13 23:06:37,801 - mmselfsup - INFO - Epoch [25][400/427]	lr: 4.150e+00, eta: 5:49:01, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7656
2023-02-13 23:07:36,563 - mmselfsup - INFO - Epoch [26][50/427]	lr: 4.097e+00, eta: 5:47:43, time: 0.820, data_time: 0.093, memory: 25346, loss: 0.7669
2023-02-13 23:08:10,734 - mmselfsup - INFO - Epoch [26][100/427]	lr: 4.097e+00, eta: 5:47:15, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.7726
2023-02-13 23:08:44,953 - mmselfsup - INFO - Epoch [26][150/427]	lr: 4.097e+00, eta: 5:46:47, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7880
2023-02-13 23:09:19,074 - mmselfsup - INFO - Epoch [26][200/427]	lr: 4.097e+00, eta: 5:46:18, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.7940
2023-02-13 23:09:53,178 - mmselfsup - INFO - Epoch [26][250/427]	lr: 4.097e+00, eta: 5:45:50, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.7766
2023-02-13 23:10:27,442 - mmselfsup - INFO - Epoch [26][300/427]	lr: 4.097e+00, eta: 5:45:22, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.7573
2023-02-13 23:11:01,717 - mmselfsup - INFO - Epoch [26][350/427]	lr: 4.097e+00, eta: 5:44:54, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.7628
2023-02-13 23:11:35,825 - mmselfsup - INFO - Epoch [26][400/427]	lr: 4.097e+00, eta: 5:44:25, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.7562
2023-02-13 23:12:35,603 - mmselfsup - INFO - Epoch [27][50/427]	lr: 4.043e+00, eta: 5:43:11, time: 0.839, data_time: 0.113, memory: 25346, loss: 0.7503
2023-02-13 23:13:09,754 - mmselfsup - INFO - Epoch [27][100/427]	lr: 4.043e+00, eta: 5:42:43, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7511
2023-02-13 23:13:43,887 - mmselfsup - INFO - Epoch [27][150/427]	lr: 4.043e+00, eta: 5:42:15, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7606
2023-02-13 23:14:18,012 - mmselfsup - INFO - Epoch [27][200/427]	lr: 4.043e+00, eta: 5:41:46, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7408
2023-02-13 23:14:52,128 - mmselfsup - INFO - Epoch [27][250/427]	lr: 4.043e+00, eta: 5:41:17, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7434
2023-02-13 23:15:26,312 - mmselfsup - INFO - Epoch [27][300/427]	lr: 4.043e+00, eta: 5:40:49, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.7482
2023-02-13 23:16:00,489 - mmselfsup - INFO - Epoch [27][350/427]	lr: 4.043e+00, eta: 5:40:20, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.7398
2023-02-13 23:16:34,695 - mmselfsup - INFO - Epoch [27][400/427]	lr: 4.043e+00, eta: 5:39:52, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.7357
2023-02-13 23:17:33,508 - mmselfsup - INFO - Epoch [28][50/427]	lr: 3.987e+00, eta: 5:38:37, time: 0.822, data_time: 0.087, memory: 25346, loss: 0.7521
2023-02-13 23:18:07,823 - mmselfsup - INFO - Epoch [28][100/427]	lr: 3.987e+00, eta: 5:38:08, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.7400
2023-02-13 23:18:41,991 - mmselfsup - INFO - Epoch [28][150/427]	lr: 3.987e+00, eta: 5:37:40, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.7281
2023-02-13 23:19:16,229 - mmselfsup - INFO - Epoch [28][200/427]	lr: 3.987e+00, eta: 5:37:11, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.7369
2023-02-13 23:19:50,426 - mmselfsup - INFO - Epoch [28][250/427]	lr: 3.987e+00, eta: 5:36:43, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7367
2023-02-13 23:20:24,636 - mmselfsup - INFO - Epoch [28][300/427]	lr: 3.987e+00, eta: 5:36:14, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7703
2023-02-13 23:20:58,819 - mmselfsup - INFO - Epoch [28][350/427]	lr: 3.987e+00, eta: 5:35:45, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7489
2023-02-13 23:21:33,099 - mmselfsup - INFO - Epoch [28][400/427]	lr: 3.987e+00, eta: 5:35:17, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.7295
2023-02-13 23:22:32,134 - mmselfsup - INFO - Epoch [29][50/427]	lr: 3.930e+00, eta: 5:34:03, time: 0.825, data_time: 0.095, memory: 25346, loss: 0.7252
2023-02-13 23:23:06,364 - mmselfsup - INFO - Epoch [29][100/427]	lr: 3.930e+00, eta: 5:33:34, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.7412
2023-02-13 23:23:40,587 - mmselfsup - INFO - Epoch [29][150/427]	lr: 3.930e+00, eta: 5:33:06, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.7215
2023-02-13 23:24:14,688 - mmselfsup - INFO - Epoch [29][200/427]	lr: 3.930e+00, eta: 5:32:37, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7182
2023-02-13 23:24:48,950 - mmselfsup - INFO - Epoch [29][250/427]	lr: 3.930e+00, eta: 5:32:08, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.7191
2023-02-13 23:25:23,081 - mmselfsup - INFO - Epoch [29][300/427]	lr: 3.930e+00, eta: 5:31:39, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7255
2023-02-13 23:25:57,246 - mmselfsup - INFO - Epoch [29][350/427]	lr: 3.930e+00, eta: 5:31:10, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.7188
2023-02-13 23:26:31,342 - mmselfsup - INFO - Epoch [29][400/427]	lr: 3.930e+00, eta: 5:30:41, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.7170
2023-02-13 23:27:30,805 - mmselfsup - INFO - Epoch [30][50/427]	lr: 3.871e+00, eta: 5:29:29, time: 0.834, data_time: 0.108, memory: 25346, loss: 0.7155
2023-02-13 23:28:05,079 - mmselfsup - INFO - Epoch [30][100/427]	lr: 3.871e+00, eta: 5:29:01, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.7030
2023-02-13 23:28:39,309 - mmselfsup - INFO - Epoch [30][150/427]	lr: 3.871e+00, eta: 5:28:32, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.7055
2023-02-13 23:29:13,552 - mmselfsup - INFO - Epoch [30][200/427]	lr: 3.871e+00, eta: 5:28:03, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.6936
2023-02-13 23:29:47,771 - mmselfsup - INFO - Epoch [30][250/427]	lr: 3.871e+00, eta: 5:27:34, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7013
2023-02-13 23:30:22,099 - mmselfsup - INFO - Epoch [30][300/427]	lr: 3.871e+00, eta: 5:27:05, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.7027
2023-02-13 23:30:56,397 - mmselfsup - INFO - Epoch [30][350/427]	lr: 3.871e+00, eta: 5:26:36, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.6872
2023-02-13 23:31:30,673 - mmselfsup - INFO - Epoch [30][400/427]	lr: 3.871e+00, eta: 5:26:07, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.6957
2023-02-13 23:31:48,435 - mmselfsup - INFO - Saving checkpoint at 30 epochs
2023-02-13 23:32:29,962 - mmselfsup - INFO - Epoch [31][50/427]	lr: 3.811e+00, eta: 5:24:54, time: 0.811, data_time: 0.111, memory: 25346, loss: 0.6884
2023-02-13 23:33:04,056 - mmselfsup - INFO - Epoch [31][100/427]	lr: 3.811e+00, eta: 5:24:25, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.6729
2023-02-13 23:33:38,362 - mmselfsup - INFO - Epoch [31][150/427]	lr: 3.811e+00, eta: 5:23:56, time: 0.686, data_time: 0.000, memory: 25346, loss: 0.6774
2023-02-13 23:34:12,524 - mmselfsup - INFO - Epoch [31][200/427]	lr: 3.811e+00, eta: 5:23:26, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.6887
2023-02-13 23:34:46,857 - mmselfsup - INFO - Epoch [31][250/427]	lr: 3.811e+00, eta: 5:22:57, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.7111
2023-02-13 23:35:21,118 - mmselfsup - INFO - Epoch [31][300/427]	lr: 3.811e+00, eta: 5:22:28, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.6715
2023-02-13 23:35:55,344 - mmselfsup - INFO - Epoch [31][350/427]	lr: 3.811e+00, eta: 5:21:59, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.6709
2023-02-13 23:36:29,599 - mmselfsup - INFO - Epoch [31][400/427]	lr: 3.811e+00, eta: 5:21:30, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.6682
2023-02-13 23:37:29,063 - mmselfsup - INFO - Epoch [32][50/427]	lr: 3.749e+00, eta: 5:20:20, time: 0.833, data_time: 0.110, memory: 25346, loss: 0.6833
2023-02-13 23:38:03,330 - mmselfsup - INFO - Epoch [32][100/427]	lr: 3.749e+00, eta: 5:19:51, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.6932
2023-02-13 23:38:37,619 - mmselfsup - INFO - Epoch [32][150/427]	lr: 3.749e+00, eta: 5:19:22, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.6604
2023-02-13 23:39:11,986 - mmselfsup - INFO - Epoch [32][200/427]	lr: 3.749e+00, eta: 5:18:53, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.6619
2023-02-13 23:39:46,333 - mmselfsup - INFO - Epoch [32][250/427]	lr: 3.749e+00, eta: 5:18:24, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.6508
2023-02-13 23:40:20,718 - mmselfsup - INFO - Epoch [32][300/427]	lr: 3.749e+00, eta: 5:17:55, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.6632
2023-02-13 23:40:54,914 - mmselfsup - INFO - Epoch [32][350/427]	lr: 3.749e+00, eta: 5:17:25, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.6494
2023-02-13 23:41:29,257 - mmselfsup - INFO - Epoch [32][400/427]	lr: 3.749e+00, eta: 5:16:56, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.6489
2023-02-13 23:42:28,270 - mmselfsup - INFO - Epoch [33][50/427]	lr: 3.686e+00, eta: 5:15:46, time: 0.826, data_time: 0.117, memory: 25346, loss: 0.6600
2023-02-13 23:43:02,615 - mmselfsup - INFO - Epoch [33][100/427]	lr: 3.686e+00, eta: 5:15:17, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.6600
2023-02-13 23:43:37,033 - mmselfsup - INFO - Epoch [33][150/427]	lr: 3.686e+00, eta: 5:14:48, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.6514
2023-02-13 23:44:11,273 - mmselfsup - INFO - Epoch [33][200/427]	lr: 3.686e+00, eta: 5:14:18, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.6466
2023-02-13 23:44:45,530 - mmselfsup - INFO - Epoch [33][250/427]	lr: 3.686e+00, eta: 5:13:49, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.6528
2023-02-13 23:45:19,798 - mmselfsup - INFO - Epoch [33][300/427]	lr: 3.686e+00, eta: 5:13:19, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.6458
2023-02-13 23:45:53,848 - mmselfsup - INFO - Epoch [33][350/427]	lr: 3.686e+00, eta: 5:12:50, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.6472
2023-02-13 23:46:27,967 - mmselfsup - INFO - Epoch [33][400/427]	lr: 3.686e+00, eta: 5:12:20, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.6358
2023-02-13 23:47:26,977 - mmselfsup - INFO - Epoch [34][50/427]	lr: 3.622e+00, eta: 5:11:11, time: 0.825, data_time: 0.098, memory: 25346, loss: 0.6385
2023-02-13 23:48:01,332 - mmselfsup - INFO - Epoch [34][100/427]	lr: 3.622e+00, eta: 5:10:41, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.6084
2023-02-13 23:48:35,506 - mmselfsup - INFO - Epoch [34][150/427]	lr: 3.622e+00, eta: 5:10:12, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.6308
2023-02-13 23:49:09,787 - mmselfsup - INFO - Epoch [34][200/427]	lr: 3.622e+00, eta: 5:09:42, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.6295
2023-02-13 23:49:44,032 - mmselfsup - INFO - Epoch [34][250/427]	lr: 3.622e+00, eta: 5:09:12, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.6412
2023-02-13 23:50:18,328 - mmselfsup - INFO - Epoch [34][300/427]	lr: 3.622e+00, eta: 5:08:43, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.6242
2023-02-13 23:50:52,502 - mmselfsup - INFO - Epoch [34][350/427]	lr: 3.622e+00, eta: 5:08:13, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.6248
2023-02-13 23:51:26,697 - mmselfsup - INFO - Epoch [34][400/427]	lr: 3.622e+00, eta: 5:07:43, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.6263
2023-02-13 23:52:25,692 - mmselfsup - INFO - Epoch [35][50/427]	lr: 3.556e+00, eta: 5:06:35, time: 0.826, data_time: 0.115, memory: 25346, loss: 0.6193
2023-02-13 23:52:59,954 - mmselfsup - INFO - Epoch [35][100/427]	lr: 3.556e+00, eta: 5:06:06, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.6270
2023-02-13 23:53:34,308 - mmselfsup - INFO - Epoch [35][150/427]	lr: 3.556e+00, eta: 5:05:36, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.6244
2023-02-13 23:54:08,599 - mmselfsup - INFO - Epoch [35][200/427]	lr: 3.556e+00, eta: 5:05:06, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.6510
2023-02-13 23:54:42,807 - mmselfsup - INFO - Epoch [35][250/427]	lr: 3.556e+00, eta: 5:04:36, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.6939
2023-02-13 23:55:17,099 - mmselfsup - INFO - Epoch [35][300/427]	lr: 3.556e+00, eta: 5:04:07, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.6805
2023-02-13 23:55:51,364 - mmselfsup - INFO - Epoch [35][350/427]	lr: 3.556e+00, eta: 5:03:37, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.7724
2023-02-13 23:56:25,625 - mmselfsup - INFO - Epoch [35][400/427]	lr: 3.556e+00, eta: 5:03:07, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.6746
2023-02-13 23:57:24,881 - mmselfsup - INFO - Epoch [36][50/427]	lr: 3.490e+00, eta: 5:02:00, time: 0.831, data_time: 0.109, memory: 25346, loss: 0.6139
2023-02-13 23:57:59,130 - mmselfsup - INFO - Epoch [36][100/427]	lr: 3.490e+00, eta: 5:01:30, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.6118
2023-02-13 23:58:33,230 - mmselfsup - INFO - Epoch [36][150/427]	lr: 3.490e+00, eta: 5:01:00, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.6256
2023-02-13 23:59:07,418 - mmselfsup - INFO - Epoch [36][200/427]	lr: 3.490e+00, eta: 5:00:30, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.5992
2023-02-13 23:59:41,675 - mmselfsup - INFO - Epoch [36][250/427]	lr: 3.490e+00, eta: 5:00:00, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.5836
2023-02-14 00:00:15,945 - mmselfsup - INFO - Epoch [36][300/427]	lr: 3.490e+00, eta: 4:59:30, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.5946
2023-02-14 00:00:50,381 - mmselfsup - INFO - Epoch [36][350/427]	lr: 3.490e+00, eta: 4:59:01, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.5880
2023-02-14 00:01:24,694 - mmselfsup - INFO - Epoch [36][400/427]	lr: 3.490e+00, eta: 4:58:31, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5789
2023-02-14 00:02:23,522 - mmselfsup - INFO - Epoch [37][50/427]	lr: 3.422e+00, eta: 4:57:24, time: 0.822, data_time: 0.112, memory: 25346, loss: 0.5900
2023-02-14 00:02:57,677 - mmselfsup - INFO - Epoch [37][100/427]	lr: 3.422e+00, eta: 4:56:54, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5822
2023-02-14 00:03:31,726 - mmselfsup - INFO - Epoch [37][150/427]	lr: 3.422e+00, eta: 4:56:23, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.5807
2023-02-14 00:04:05,924 - mmselfsup - INFO - Epoch [37][200/427]	lr: 3.422e+00, eta: 4:55:53, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.5745
2023-02-14 00:04:40,071 - mmselfsup - INFO - Epoch [37][250/427]	lr: 3.422e+00, eta: 4:55:23, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5838
2023-02-14 00:05:14,219 - mmselfsup - INFO - Epoch [37][300/427]	lr: 3.422e+00, eta: 4:54:53, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5743
2023-02-14 00:05:48,335 - mmselfsup - INFO - Epoch [37][350/427]	lr: 3.422e+00, eta: 4:54:22, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5576
2023-02-14 00:06:22,504 - mmselfsup - INFO - Epoch [37][400/427]	lr: 3.422e+00, eta: 4:53:52, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5608
2023-02-14 00:07:21,653 - mmselfsup - INFO - Epoch [38][50/427]	lr: 3.353e+00, eta: 4:52:46, time: 0.828, data_time: 0.087, memory: 25346, loss: 0.5577
2023-02-14 00:07:55,972 - mmselfsup - INFO - Epoch [38][100/427]	lr: 3.353e+00, eta: 4:52:16, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5571
2023-02-14 00:08:30,315 - mmselfsup - INFO - Epoch [38][150/427]	lr: 3.353e+00, eta: 4:51:46, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.5685
2023-02-14 00:09:04,564 - mmselfsup - INFO - Epoch [38][200/427]	lr: 3.353e+00, eta: 4:51:16, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.5650
2023-02-14 00:09:38,711 - mmselfsup - INFO - Epoch [38][250/427]	lr: 3.353e+00, eta: 4:50:46, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.5575
2023-02-14 00:10:12,830 - mmselfsup - INFO - Epoch [38][300/427]	lr: 3.353e+00, eta: 4:50:15, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.5494
2023-02-14 00:10:47,081 - mmselfsup - INFO - Epoch [38][350/427]	lr: 3.353e+00, eta: 4:49:45, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.5432
2023-02-14 00:11:21,374 - mmselfsup - INFO - Epoch [38][400/427]	lr: 3.353e+00, eta: 4:49:15, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5509
2023-02-14 00:12:20,571 - mmselfsup - INFO - Epoch [39][50/427]	lr: 3.283e+00, eta: 4:48:10, time: 0.829, data_time: 0.110, memory: 25346, loss: 0.5319
2023-02-14 00:12:54,728 - mmselfsup - INFO - Epoch [39][100/427]	lr: 3.283e+00, eta: 4:47:39, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5437
2023-02-14 00:13:28,961 - mmselfsup - INFO - Epoch [39][150/427]	lr: 3.283e+00, eta: 4:47:09, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.5281
2023-02-14 00:14:03,137 - mmselfsup - INFO - Epoch [39][200/427]	lr: 3.283e+00, eta: 4:46:39, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5549
2023-02-14 00:14:37,224 - mmselfsup - INFO - Epoch [39][250/427]	lr: 3.283e+00, eta: 4:46:08, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.5381
2023-02-14 00:15:11,387 - mmselfsup - INFO - Epoch [39][300/427]	lr: 3.283e+00, eta: 4:45:38, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5359
2023-02-14 00:15:45,671 - mmselfsup - INFO - Epoch [39][350/427]	lr: 3.283e+00, eta: 4:45:07, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.5244
2023-02-14 00:16:19,808 - mmselfsup - INFO - Epoch [39][400/427]	lr: 3.283e+00, eta: 4:44:37, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5256
2023-02-14 00:17:19,154 - mmselfsup - INFO - Epoch [40][50/427]	lr: 3.213e+00, eta: 4:43:33, time: 0.832, data_time: 0.095, memory: 25346, loss: 0.5203
2023-02-14 00:17:53,415 - mmselfsup - INFO - Epoch [40][100/427]	lr: 3.213e+00, eta: 4:43:02, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.5112
2023-02-14 00:18:27,629 - mmselfsup - INFO - Epoch [40][150/427]	lr: 3.213e+00, eta: 4:42:32, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.5267
2023-02-14 00:19:01,866 - mmselfsup - INFO - Epoch [40][200/427]	lr: 3.213e+00, eta: 4:42:01, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.5244
2023-02-14 00:19:36,070 - mmselfsup - INFO - Epoch [40][250/427]	lr: 3.213e+00, eta: 4:41:31, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.5260
2023-02-14 00:20:10,248 - mmselfsup - INFO - Epoch [40][300/427]	lr: 3.213e+00, eta: 4:41:00, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.5236
2023-02-14 00:20:44,460 - mmselfsup - INFO - Epoch [40][350/427]	lr: 3.213e+00, eta: 4:40:30, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.5836
2023-02-14 00:21:18,741 - mmselfsup - INFO - Epoch [40][400/427]	lr: 3.213e+00, eta: 4:39:59, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5371
2023-02-14 00:21:36,538 - mmselfsup - INFO - Saving checkpoint at 40 epochs
2023-02-14 00:22:17,718 - mmselfsup - INFO - Epoch [41][50/427]	lr: 3.142e+00, eta: 4:38:54, time: 0.803, data_time: 0.107, memory: 25346, loss: 0.5112
2023-02-14 00:22:51,962 - mmselfsup - INFO - Epoch [41][100/427]	lr: 3.142e+00, eta: 4:38:23, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4983
2023-02-14 00:23:26,222 - mmselfsup - INFO - Epoch [41][150/427]	lr: 3.142e+00, eta: 4:37:53, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.5440
2023-02-14 00:24:00,390 - mmselfsup - INFO - Epoch [41][200/427]	lr: 3.142e+00, eta: 4:37:22, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5881
2023-02-14 00:24:34,517 - mmselfsup - INFO - Epoch [41][250/427]	lr: 3.142e+00, eta: 4:36:51, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.5535
2023-02-14 00:25:08,723 - mmselfsup - INFO - Epoch [41][300/427]	lr: 3.142e+00, eta: 4:36:21, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.5259
2023-02-14 00:25:42,881 - mmselfsup - INFO - Epoch [41][350/427]	lr: 3.142e+00, eta: 4:35:50, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.5134
2023-02-14 00:26:16,991 - mmselfsup - INFO - Epoch [41][400/427]	lr: 3.142e+00, eta: 4:35:19, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.5233
2023-02-14 00:27:15,816 - mmselfsup - INFO - Epoch [42][50/427]	lr: 3.070e+00, eta: 4:34:15, time: 0.822, data_time: 0.111, memory: 25346, loss: 0.5242
2023-02-14 00:27:50,113 - mmselfsup - INFO - Epoch [42][100/427]	lr: 3.070e+00, eta: 4:33:45, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5057
2023-02-14 00:28:24,426 - mmselfsup - INFO - Epoch [42][150/427]	lr: 3.070e+00, eta: 4:33:15, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5036
2023-02-14 00:28:58,708 - mmselfsup - INFO - Epoch [42][200/427]	lr: 3.070e+00, eta: 4:32:44, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4993
2023-02-14 00:29:32,937 - mmselfsup - INFO - Epoch [42][250/427]	lr: 3.070e+00, eta: 4:32:13, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.5602
2023-02-14 00:30:07,240 - mmselfsup - INFO - Epoch [42][300/427]	lr: 3.070e+00, eta: 4:31:43, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5235
2023-02-14 00:30:41,444 - mmselfsup - INFO - Epoch [42][350/427]	lr: 3.070e+00, eta: 4:31:12, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.7869
2023-02-14 00:31:15,766 - mmselfsup - INFO - Epoch [42][400/427]	lr: 3.070e+00, eta: 4:30:41, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.6818
2023-02-14 00:32:15,005 - mmselfsup - INFO - Epoch [43][50/427]	lr: 2.997e+00, eta: 4:29:39, time: 0.830, data_time: 0.111, memory: 25346, loss: 0.5706
2023-02-14 00:32:49,235 - mmselfsup - INFO - Epoch [43][100/427]	lr: 2.997e+00, eta: 4:29:08, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.5689
2023-02-14 00:33:23,446 - mmselfsup - INFO - Epoch [43][150/427]	lr: 2.997e+00, eta: 4:28:37, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.5583
2023-02-14 00:33:57,640 - mmselfsup - INFO - Epoch [43][200/427]	lr: 2.997e+00, eta: 4:28:07, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.5508
2023-02-14 00:34:31,664 - mmselfsup - INFO - Epoch [43][250/427]	lr: 2.997e+00, eta: 4:27:35, time: 0.680, data_time: 0.001, memory: 25346, loss: 0.5832
2023-02-14 00:35:05,909 - mmselfsup - INFO - Epoch [43][300/427]	lr: 2.997e+00, eta: 4:27:05, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.5389
2023-02-14 00:35:40,101 - mmselfsup - INFO - Epoch [43][350/427]	lr: 2.997e+00, eta: 4:26:34, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.5349
2023-02-14 00:36:14,326 - mmselfsup - INFO - Epoch [43][400/427]	lr: 2.997e+00, eta: 4:26:03, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.6081
2023-02-14 00:37:13,102 - mmselfsup - INFO - Epoch [44][50/427]	lr: 2.924e+00, eta: 4:25:00, time: 0.819, data_time: 0.116, memory: 25346, loss: 0.5764
2023-02-14 00:37:47,478 - mmselfsup - INFO - Epoch [44][100/427]	lr: 2.924e+00, eta: 4:24:30, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.7717
2023-02-14 00:38:21,727 - mmselfsup - INFO - Epoch [44][150/427]	lr: 2.924e+00, eta: 4:23:59, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.5754
2023-02-14 00:38:56,019 - mmselfsup - INFO - Epoch [44][200/427]	lr: 2.924e+00, eta: 4:23:28, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5425
2023-02-14 00:39:30,219 - mmselfsup - INFO - Epoch [44][250/427]	lr: 2.924e+00, eta: 4:22:57, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.5248
2023-02-14 00:40:04,583 - mmselfsup - INFO - Epoch [44][300/427]	lr: 2.924e+00, eta: 4:22:27, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.5199
2023-02-14 00:40:38,900 - mmselfsup - INFO - Epoch [44][350/427]	lr: 2.924e+00, eta: 4:21:56, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.5491
2023-02-14 00:41:13,156 - mmselfsup - INFO - Epoch [44][400/427]	lr: 2.924e+00, eta: 4:21:25, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.5307
2023-02-14 00:42:12,303 - mmselfsup - INFO - Epoch [45][50/427]	lr: 2.850e+00, eta: 4:20:23, time: 0.827, data_time: 0.111, memory: 25346, loss: 0.5272
2023-02-14 00:42:46,539 - mmselfsup - INFO - Epoch [45][100/427]	lr: 2.850e+00, eta: 4:19:52, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.5509
2023-02-14 00:43:20,741 - mmselfsup - INFO - Epoch [45][150/427]	lr: 2.850e+00, eta: 4:19:21, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.5470
2023-02-14 00:43:55,104 - mmselfsup - INFO - Epoch [45][200/427]	lr: 2.850e+00, eta: 4:18:50, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.5138
2023-02-14 00:44:29,260 - mmselfsup - INFO - Epoch [45][250/427]	lr: 2.850e+00, eta: 4:18:19, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.4960
2023-02-14 00:45:03,517 - mmselfsup - INFO - Epoch [45][300/427]	lr: 2.850e+00, eta: 4:17:49, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4823
2023-02-14 00:45:37,628 - mmselfsup - INFO - Epoch [45][350/427]	lr: 2.850e+00, eta: 4:17:18, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.5052
2023-02-14 00:46:11,874 - mmselfsup - INFO - Epoch [45][400/427]	lr: 2.850e+00, eta: 4:16:47, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4861
2023-02-14 00:47:11,306 - mmselfsup - INFO - Epoch [46][50/427]	lr: 2.775e+00, eta: 4:15:45, time: 0.834, data_time: 0.105, memory: 25346, loss: 0.8442
2023-02-14 00:47:45,502 - mmselfsup - INFO - Epoch [46][100/427]	lr: 2.775e+00, eta: 4:15:15, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.8563
2023-02-14 00:48:19,749 - mmselfsup - INFO - Epoch [46][150/427]	lr: 2.775e+00, eta: 4:14:44, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.6260
2023-02-14 00:48:53,925 - mmselfsup - INFO - Epoch [46][200/427]	lr: 2.775e+00, eta: 4:14:13, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.5441
2023-02-14 00:49:28,428 - mmselfsup - INFO - Epoch [46][250/427]	lr: 2.775e+00, eta: 4:13:42, time: 0.690, data_time: 0.003, memory: 25346, loss: 0.5192
2023-02-14 00:50:02,799 - mmselfsup - INFO - Epoch [46][300/427]	lr: 2.775e+00, eta: 4:13:11, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.4828
2023-02-14 00:50:37,132 - mmselfsup - INFO - Epoch [46][350/427]	lr: 2.775e+00, eta: 4:12:40, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.4858
2023-02-14 00:51:11,455 - mmselfsup - INFO - Epoch [46][400/427]	lr: 2.775e+00, eta: 4:12:09, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4814
2023-02-14 00:52:10,103 - mmselfsup - INFO - Epoch [47][50/427]	lr: 2.701e+00, eta: 4:11:08, time: 0.818, data_time: 0.117, memory: 25346, loss: 0.4687
2023-02-14 00:52:44,369 - mmselfsup - INFO - Epoch [47][100/427]	lr: 2.701e+00, eta: 4:10:37, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4705
2023-02-14 00:53:18,620 - mmselfsup - INFO - Epoch [47][150/427]	lr: 2.701e+00, eta: 4:10:06, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4666
2023-02-14 00:53:52,939 - mmselfsup - INFO - Epoch [47][200/427]	lr: 2.701e+00, eta: 4:09:35, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.4627
2023-02-14 00:54:27,125 - mmselfsup - INFO - Epoch [47][250/427]	lr: 2.701e+00, eta: 4:09:03, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.4649
2023-02-14 00:55:01,519 - mmselfsup - INFO - Epoch [47][300/427]	lr: 2.701e+00, eta: 4:08:33, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.4689
2023-02-14 00:55:35,689 - mmselfsup - INFO - Epoch [47][350/427]	lr: 2.701e+00, eta: 4:08:01, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.4582
2023-02-14 00:56:09,950 - mmselfsup - INFO - Epoch [47][400/427]	lr: 2.701e+00, eta: 4:07:30, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4632
2023-02-14 00:57:09,385 - mmselfsup - INFO - Epoch [48][50/427]	lr: 2.626e+00, eta: 4:06:30, time: 0.834, data_time: 0.114, memory: 25346, loss: 0.4675
2023-02-14 00:57:43,760 - mmselfsup - INFO - Epoch [48][100/427]	lr: 2.626e+00, eta: 4:05:59, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.4597
2023-02-14 00:58:18,114 - mmselfsup - INFO - Epoch [48][150/427]	lr: 2.626e+00, eta: 4:05:28, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.4550
2023-02-14 00:58:52,456 - mmselfsup - INFO - Epoch [48][200/427]	lr: 2.626e+00, eta: 4:04:57, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.4452
2023-02-14 00:59:26,835 - mmselfsup - INFO - Epoch [48][250/427]	lr: 2.626e+00, eta: 4:04:26, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.4441
2023-02-14 01:00:01,055 - mmselfsup - INFO - Epoch [48][300/427]	lr: 2.626e+00, eta: 4:03:55, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.4477
2023-02-14 01:00:35,357 - mmselfsup - INFO - Epoch [48][350/427]	lr: 2.626e+00, eta: 4:03:24, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4413
2023-02-14 01:01:09,483 - mmselfsup - INFO - Epoch [48][400/427]	lr: 2.626e+00, eta: 4:02:53, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.4328
2023-02-14 01:02:08,871 - mmselfsup - INFO - Epoch [49][50/427]	lr: 2.551e+00, eta: 4:01:53, time: 0.832, data_time: 0.099, memory: 25346, loss: 0.4316
2023-02-14 01:02:43,088 - mmselfsup - INFO - Epoch [49][100/427]	lr: 2.551e+00, eta: 4:01:21, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.4315
2023-02-14 01:03:17,478 - mmselfsup - INFO - Epoch [49][150/427]	lr: 2.551e+00, eta: 4:00:50, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.4281
2023-02-14 01:03:51,699 - mmselfsup - INFO - Epoch [49][200/427]	lr: 2.551e+00, eta: 4:00:19, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.4352
2023-02-14 01:04:25,937 - mmselfsup - INFO - Epoch [49][250/427]	lr: 2.551e+00, eta: 3:59:48, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4356
2023-02-14 01:05:00,243 - mmselfsup - INFO - Epoch [49][300/427]	lr: 2.551e+00, eta: 3:59:17, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.4177
2023-02-14 01:05:34,525 - mmselfsup - INFO - Epoch [49][350/427]	lr: 2.551e+00, eta: 3:58:46, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.4274
2023-02-14 01:06:08,847 - mmselfsup - INFO - Epoch [49][400/427]	lr: 2.551e+00, eta: 3:58:14, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.4280
2023-02-14 01:07:08,251 - mmselfsup - INFO - Epoch [50][50/427]	lr: 2.475e+00, eta: 3:57:15, time: 0.832, data_time: 0.092, memory: 25346, loss: 0.4125
2023-02-14 01:07:42,580 - mmselfsup - INFO - Epoch [50][100/427]	lr: 2.475e+00, eta: 3:56:44, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.4560
2023-02-14 01:08:16,976 - mmselfsup - INFO - Epoch [50][150/427]	lr: 2.475e+00, eta: 3:56:12, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.4580
2023-02-14 01:08:51,443 - mmselfsup - INFO - Epoch [50][200/427]	lr: 2.475e+00, eta: 3:55:41, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.4171
2023-02-14 01:09:25,744 - mmselfsup - INFO - Epoch [50][250/427]	lr: 2.475e+00, eta: 3:55:10, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4160
2023-02-14 01:10:00,036 - mmselfsup - INFO - Epoch [50][300/427]	lr: 2.475e+00, eta: 3:54:39, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4229
2023-02-14 01:10:34,318 - mmselfsup - INFO - Epoch [50][350/427]	lr: 2.475e+00, eta: 3:54:08, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4095
2023-02-14 01:11:08,552 - mmselfsup - INFO - Epoch [50][400/427]	lr: 2.475e+00, eta: 3:53:36, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.4108
2023-02-14 01:11:26,318 - mmselfsup - INFO - Saving checkpoint at 50 epochs
2023-02-14 01:12:07,595 - mmselfsup - INFO - Epoch [51][50/427]	lr: 2.400e+00, eta: 3:52:36, time: 0.805, data_time: 0.091, memory: 25346, loss: 0.3969
2023-02-14 01:12:41,976 - mmselfsup - INFO - Epoch [51][100/427]	lr: 2.400e+00, eta: 3:52:05, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.4053
2023-02-14 01:13:16,304 - mmselfsup - INFO - Epoch [51][150/427]	lr: 2.400e+00, eta: 3:51:33, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.4184
2023-02-14 01:13:50,494 - mmselfsup - INFO - Epoch [51][200/427]	lr: 2.400e+00, eta: 3:51:02, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.4173
2023-02-14 01:14:24,824 - mmselfsup - INFO - Epoch [51][250/427]	lr: 2.400e+00, eta: 3:50:31, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.4238
2023-02-14 01:14:59,059 - mmselfsup - INFO - Epoch [51][300/427]	lr: 2.400e+00, eta: 3:49:59, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.4141
2023-02-14 01:15:33,365 - mmselfsup - INFO - Epoch [51][350/427]	lr: 2.400e+00, eta: 3:49:28, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.4061
2023-02-14 01:16:07,577 - mmselfsup - INFO - Epoch [51][400/427]	lr: 2.400e+00, eta: 3:48:57, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.4057
2023-02-14 01:17:06,437 - mmselfsup - INFO - Epoch [52][50/427]	lr: 2.325e+00, eta: 3:47:57, time: 0.823, data_time: 0.110, memory: 25346, loss: 0.4035
2023-02-14 01:17:40,896 - mmselfsup - INFO - Epoch [52][100/427]	lr: 2.325e+00, eta: 3:47:26, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.4022
2023-02-14 01:18:15,316 - mmselfsup - INFO - Epoch [52][150/427]	lr: 2.325e+00, eta: 3:46:55, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.3983
2023-02-14 01:18:49,660 - mmselfsup - INFO - Epoch [52][200/427]	lr: 2.325e+00, eta: 3:46:24, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.3961
2023-02-14 01:19:24,189 - mmselfsup - INFO - Epoch [52][250/427]	lr: 2.325e+00, eta: 3:45:52, time: 0.691, data_time: 0.003, memory: 25346, loss: 0.3908
2023-02-14 01:19:58,587 - mmselfsup - INFO - Epoch [52][300/427]	lr: 2.325e+00, eta: 3:45:21, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.3934
2023-02-14 01:20:32,921 - mmselfsup - INFO - Epoch [52][350/427]	lr: 2.325e+00, eta: 3:44:50, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.4061
2023-02-14 01:21:07,211 - mmselfsup - INFO - Epoch [52][400/427]	lr: 2.325e+00, eta: 3:44:18, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3920
2023-02-14 01:22:06,270 - mmselfsup - INFO - Epoch [53][50/427]	lr: 2.249e+00, eta: 3:43:19, time: 0.825, data_time: 0.111, memory: 25346, loss: 0.3845
2023-02-14 01:22:40,489 - mmselfsup - INFO - Epoch [53][100/427]	lr: 2.249e+00, eta: 3:42:48, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3992
2023-02-14 01:23:14,783 - mmselfsup - INFO - Epoch [53][150/427]	lr: 2.249e+00, eta: 3:42:17, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3953
2023-02-14 01:23:48,979 - mmselfsup - INFO - Epoch [53][200/427]	lr: 2.249e+00, eta: 3:41:45, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3949
2023-02-14 01:24:23,111 - mmselfsup - INFO - Epoch [53][250/427]	lr: 2.249e+00, eta: 3:41:14, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.4150
2023-02-14 01:24:57,387 - mmselfsup - INFO - Epoch [53][300/427]	lr: 2.249e+00, eta: 3:40:42, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3924
2023-02-14 01:25:31,786 - mmselfsup - INFO - Epoch [53][350/427]	lr: 2.249e+00, eta: 3:40:11, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.3987
2023-02-14 01:26:06,111 - mmselfsup - INFO - Epoch [53][400/427]	lr: 2.249e+00, eta: 3:39:39, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3860
2023-02-14 01:27:05,726 - mmselfsup - INFO - Epoch [54][50/427]	lr: 2.174e+00, eta: 3:38:41, time: 0.836, data_time: 0.107, memory: 25346, loss: 0.3744
2023-02-14 01:27:39,958 - mmselfsup - INFO - Epoch [54][100/427]	lr: 2.174e+00, eta: 3:38:10, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3971
2023-02-14 01:28:14,197 - mmselfsup - INFO - Epoch [54][150/427]	lr: 2.174e+00, eta: 3:37:38, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3819
2023-02-14 01:28:48,485 - mmselfsup - INFO - Epoch [54][200/427]	lr: 2.174e+00, eta: 3:37:07, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3731
2023-02-14 01:29:22,744 - mmselfsup - INFO - Epoch [54][250/427]	lr: 2.174e+00, eta: 3:36:35, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3831
2023-02-14 01:29:57,072 - mmselfsup - INFO - Epoch [54][300/427]	lr: 2.174e+00, eta: 3:36:04, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.3775
2023-02-14 01:30:31,356 - mmselfsup - INFO - Epoch [54][350/427]	lr: 2.174e+00, eta: 3:35:32, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3892
2023-02-14 01:31:05,664 - mmselfsup - INFO - Epoch [54][400/427]	lr: 2.174e+00, eta: 3:35:01, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.4176
2023-02-14 01:32:05,247 - mmselfsup - INFO - Epoch [55][50/427]	lr: 2.099e+00, eta: 3:34:03, time: 0.837, data_time: 0.099, memory: 25346, loss: 0.3768
2023-02-14 01:32:39,567 - mmselfsup - INFO - Epoch [55][100/427]	lr: 2.099e+00, eta: 3:33:31, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3717
2023-02-14 01:33:13,926 - mmselfsup - INFO - Epoch [55][150/427]	lr: 2.099e+00, eta: 3:33:00, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.3642
2023-02-14 01:33:48,187 - mmselfsup - INFO - Epoch [55][200/427]	lr: 2.099e+00, eta: 3:32:28, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3672
2023-02-14 01:34:22,417 - mmselfsup - INFO - Epoch [55][250/427]	lr: 2.099e+00, eta: 3:31:57, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3680
2023-02-14 01:34:56,544 - mmselfsup - INFO - Epoch [55][300/427]	lr: 2.099e+00, eta: 3:31:25, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.3711
2023-02-14 01:35:30,763 - mmselfsup - INFO - Epoch [55][350/427]	lr: 2.099e+00, eta: 3:30:54, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3707
2023-02-14 01:36:05,098 - mmselfsup - INFO - Epoch [55][400/427]	lr: 2.099e+00, eta: 3:30:22, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.3655
2023-02-14 01:37:04,310 - mmselfsup - INFO - Epoch [56][50/427]	lr: 2.025e+00, eta: 3:29:24, time: 0.829, data_time: 0.098, memory: 25346, loss: 0.3608
2023-02-14 01:37:38,600 - mmselfsup - INFO - Epoch [56][100/427]	lr: 2.025e+00, eta: 3:28:53, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3615
2023-02-14 01:38:12,825 - mmselfsup - INFO - Epoch [56][150/427]	lr: 2.025e+00, eta: 3:28:21, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3595
2023-02-14 01:38:47,065 - mmselfsup - INFO - Epoch [56][200/427]	lr: 2.025e+00, eta: 3:27:49, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3566
2023-02-14 01:39:21,386 - mmselfsup - INFO - Epoch [56][250/427]	lr: 2.025e+00, eta: 3:27:18, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3554
2023-02-14 01:39:55,648 - mmselfsup - INFO - Epoch [56][300/427]	lr: 2.025e+00, eta: 3:26:46, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3669
2023-02-14 01:40:29,849 - mmselfsup - INFO - Epoch [56][350/427]	lr: 2.025e+00, eta: 3:26:14, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3451
2023-02-14 01:41:04,067 - mmselfsup - INFO - Epoch [56][400/427]	lr: 2.025e+00, eta: 3:25:43, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3598
2023-02-14 01:42:03,177 - mmselfsup - INFO - Epoch [57][50/427]	lr: 1.950e+00, eta: 3:24:45, time: 0.827, data_time: 0.088, memory: 25346, loss: 0.3520
2023-02-14 01:42:37,406 - mmselfsup - INFO - Epoch [57][100/427]	lr: 1.950e+00, eta: 3:24:13, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3575
2023-02-14 01:43:11,692 - mmselfsup - INFO - Epoch [57][150/427]	lr: 1.950e+00, eta: 3:23:42, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3424
2023-02-14 01:43:45,836 - mmselfsup - INFO - Epoch [57][200/427]	lr: 1.950e+00, eta: 3:23:10, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.3526
2023-02-14 01:44:20,117 - mmselfsup - INFO - Epoch [57][250/427]	lr: 1.950e+00, eta: 3:22:38, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3538
2023-02-14 01:44:54,422 - mmselfsup - INFO - Epoch [57][300/427]	lr: 1.950e+00, eta: 3:22:07, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3485
2023-02-14 01:45:28,761 - mmselfsup - INFO - Epoch [57][350/427]	lr: 1.950e+00, eta: 3:21:35, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.3592
2023-02-14 01:46:02,913 - mmselfsup - INFO - Epoch [57][400/427]	lr: 1.950e+00, eta: 3:21:03, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.3522
2023-02-14 01:47:01,751 - mmselfsup - INFO - Epoch [58][50/427]	lr: 1.876e+00, eta: 3:20:06, time: 0.821, data_time: 0.108, memory: 25346, loss: 0.3436
2023-02-14 01:47:36,032 - mmselfsup - INFO - Epoch [58][100/427]	lr: 1.876e+00, eta: 3:19:34, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3492
2023-02-14 01:48:10,279 - mmselfsup - INFO - Epoch [58][150/427]	lr: 1.876e+00, eta: 3:19:02, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3407
2023-02-14 01:48:44,590 - mmselfsup - INFO - Epoch [58][200/427]	lr: 1.876e+00, eta: 3:18:31, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3521
2023-02-14 01:49:18,810 - mmselfsup - INFO - Epoch [58][250/427]	lr: 1.876e+00, eta: 3:17:59, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3453
2023-02-14 01:49:52,983 - mmselfsup - INFO - Epoch [58][300/427]	lr: 1.876e+00, eta: 3:17:27, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.3642
2023-02-14 01:50:27,254 - mmselfsup - INFO - Epoch [58][350/427]	lr: 1.876e+00, eta: 3:16:55, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3529
2023-02-14 01:51:01,591 - mmselfsup - INFO - Epoch [58][400/427]	lr: 1.876e+00, eta: 3:16:24, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.3436
2023-02-14 01:52:00,678 - mmselfsup - INFO - Epoch [59][50/427]	lr: 1.803e+00, eta: 3:15:27, time: 0.826, data_time: 0.101, memory: 25346, loss: 0.3439
2023-02-14 01:52:34,921 - mmselfsup - INFO - Epoch [59][100/427]	lr: 1.803e+00, eta: 3:14:55, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3235
2023-02-14 01:53:09,175 - mmselfsup - INFO - Epoch [59][150/427]	lr: 1.803e+00, eta: 3:14:23, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3444
2023-02-14 01:53:43,453 - mmselfsup - INFO - Epoch [59][200/427]	lr: 1.803e+00, eta: 3:13:51, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3360
2023-02-14 01:54:17,633 - mmselfsup - INFO - Epoch [59][250/427]	lr: 1.803e+00, eta: 3:13:20, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3350
2023-02-14 01:54:52,006 - mmselfsup - INFO - Epoch [59][300/427]	lr: 1.803e+00, eta: 3:12:48, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.3316
2023-02-14 01:55:26,303 - mmselfsup - INFO - Epoch [59][350/427]	lr: 1.803e+00, eta: 3:12:16, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3426
2023-02-14 01:56:00,522 - mmselfsup - INFO - Epoch [59][400/427]	lr: 1.803e+00, eta: 3:11:44, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3351
2023-02-14 01:56:59,566 - mmselfsup - INFO - Epoch [60][50/427]	lr: 1.730e+00, eta: 3:10:47, time: 0.825, data_time: 0.098, memory: 25346, loss: 0.3324
2023-02-14 01:57:33,977 - mmselfsup - INFO - Epoch [60][100/427]	lr: 1.730e+00, eta: 3:10:16, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.3237
2023-02-14 01:58:08,223 - mmselfsup - INFO - Epoch [60][150/427]	lr: 1.730e+00, eta: 3:09:44, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3440
2023-02-14 01:58:42,486 - mmselfsup - INFO - Epoch [60][200/427]	lr: 1.730e+00, eta: 3:09:12, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3481
2023-02-14 01:59:16,780 - mmselfsup - INFO - Epoch [60][250/427]	lr: 1.730e+00, eta: 3:08:40, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3310
2023-02-14 01:59:51,080 - mmselfsup - INFO - Epoch [60][300/427]	lr: 1.730e+00, eta: 3:08:09, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3396
2023-02-14 02:00:25,273 - mmselfsup - INFO - Epoch [60][350/427]	lr: 1.730e+00, eta: 3:07:37, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3290
2023-02-14 02:00:59,550 - mmselfsup - INFO - Epoch [60][400/427]	lr: 1.730e+00, eta: 3:07:05, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3331
2023-02-14 02:01:17,315 - mmselfsup - INFO - Saving checkpoint at 60 epochs
2023-02-14 02:01:58,796 - mmselfsup - INFO - Epoch [61][50/427]	lr: 1.658e+00, eta: 3:06:08, time: 0.808, data_time: 0.117, memory: 25346, loss: 0.3290
2023-02-14 02:02:33,092 - mmselfsup - INFO - Epoch [61][100/427]	lr: 1.658e+00, eta: 3:05:36, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3410
2023-02-14 02:03:07,354 - mmselfsup - INFO - Epoch [61][150/427]	lr: 1.658e+00, eta: 3:05:04, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3196
2023-02-14 02:03:41,653 - mmselfsup - INFO - Epoch [61][200/427]	lr: 1.658e+00, eta: 3:04:32, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3113
2023-02-14 02:04:15,886 - mmselfsup - INFO - Epoch [61][250/427]	lr: 1.658e+00, eta: 3:04:00, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3289
2023-02-14 02:04:50,088 - mmselfsup - INFO - Epoch [61][300/427]	lr: 1.658e+00, eta: 3:03:28, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3325
2023-02-14 02:05:24,132 - mmselfsup - INFO - Epoch [61][350/427]	lr: 1.658e+00, eta: 3:02:56, time: 0.681, data_time: 0.001, memory: 25346, loss: 0.3210
2023-02-14 02:05:58,331 - mmselfsup - INFO - Epoch [61][400/427]	lr: 1.658e+00, eta: 3:02:24, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3132
2023-02-14 02:06:57,504 - mmselfsup - INFO - Epoch [62][50/427]	lr: 1.587e+00, eta: 3:01:28, time: 0.828, data_time: 0.104, memory: 25346, loss: 0.3224
2023-02-14 02:07:31,703 - mmselfsup - INFO - Epoch [62][100/427]	lr: 1.587e+00, eta: 3:00:56, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3301
2023-02-14 02:08:06,076 - mmselfsup - INFO - Epoch [62][150/427]	lr: 1.587e+00, eta: 3:00:24, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.3249
2023-02-14 02:08:40,353 - mmselfsup - INFO - Epoch [62][200/427]	lr: 1.587e+00, eta: 2:59:53, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3119
2023-02-14 02:09:14,651 - mmselfsup - INFO - Epoch [62][250/427]	lr: 1.587e+00, eta: 2:59:21, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3141
2023-02-14 02:09:48,845 - mmselfsup - INFO - Epoch [62][300/427]	lr: 1.587e+00, eta: 2:58:49, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3190
2023-02-14 02:10:23,207 - mmselfsup - INFO - Epoch [62][350/427]	lr: 1.587e+00, eta: 2:58:17, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.3023
2023-02-14 02:10:57,460 - mmselfsup - INFO - Epoch [62][400/427]	lr: 1.587e+00, eta: 2:57:45, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3111
2023-02-14 02:11:56,645 - mmselfsup - INFO - Epoch [63][50/427]	lr: 1.517e+00, eta: 2:56:49, time: 0.829, data_time: 0.087, memory: 25346, loss: 0.3139
2023-02-14 02:12:30,919 - mmselfsup - INFO - Epoch [63][100/427]	lr: 1.517e+00, eta: 2:56:17, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3058
2023-02-14 02:13:05,161 - mmselfsup - INFO - Epoch [63][150/427]	lr: 1.517e+00, eta: 2:55:45, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3167
2023-02-14 02:13:39,435 - mmselfsup - INFO - Epoch [63][200/427]	lr: 1.517e+00, eta: 2:55:13, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3112
2023-02-14 02:14:13,658 - mmselfsup - INFO - Epoch [63][250/427]	lr: 1.517e+00, eta: 2:54:41, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.3068
2023-02-14 02:14:48,039 - mmselfsup - INFO - Epoch [63][300/427]	lr: 1.517e+00, eta: 2:54:09, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.3081
2023-02-14 02:15:22,434 - mmselfsup - INFO - Epoch [63][350/427]	lr: 1.517e+00, eta: 2:53:38, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.3058
2023-02-14 02:15:56,660 - mmselfsup - INFO - Epoch [63][400/427]	lr: 1.517e+00, eta: 2:53:06, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3015
2023-02-14 02:16:55,599 - mmselfsup - INFO - Epoch [64][50/427]	lr: 1.447e+00, eta: 2:52:10, time: 0.824, data_time: 0.098, memory: 25346, loss: 0.3155
2023-02-14 02:17:29,951 - mmselfsup - INFO - Epoch [64][100/427]	lr: 1.447e+00, eta: 2:51:38, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.3080
2023-02-14 02:18:04,149 - mmselfsup - INFO - Epoch [64][150/427]	lr: 1.447e+00, eta: 2:51:06, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3017
2023-02-14 02:18:38,437 - mmselfsup - INFO - Epoch [64][200/427]	lr: 1.447e+00, eta: 2:50:34, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3003
2023-02-14 02:19:12,775 - mmselfsup - INFO - Epoch [64][250/427]	lr: 1.447e+00, eta: 2:50:02, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2956
2023-02-14 02:19:47,065 - mmselfsup - INFO - Epoch [64][300/427]	lr: 1.447e+00, eta: 2:49:30, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.3102
2023-02-14 02:20:21,320 - mmselfsup - INFO - Epoch [64][350/427]	lr: 1.447e+00, eta: 2:48:58, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3040
2023-02-14 02:20:55,550 - mmselfsup - INFO - Epoch [64][400/427]	lr: 1.447e+00, eta: 2:48:26, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.3058
2023-02-14 02:21:54,390 - mmselfsup - INFO - Epoch [65][50/427]	lr: 1.378e+00, eta: 2:47:30, time: 0.823, data_time: 0.111, memory: 25346, loss: 0.2956
2023-02-14 02:22:28,613 - mmselfsup - INFO - Epoch [65][100/427]	lr: 1.378e+00, eta: 2:46:58, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2933
2023-02-14 02:23:02,872 - mmselfsup - INFO - Epoch [65][150/427]	lr: 1.378e+00, eta: 2:46:26, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.3062
2023-02-14 02:23:37,152 - mmselfsup - INFO - Epoch [65][200/427]	lr: 1.378e+00, eta: 2:45:54, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.3067
2023-02-14 02:24:11,409 - mmselfsup - INFO - Epoch [65][250/427]	lr: 1.378e+00, eta: 2:45:22, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2974
2023-02-14 02:24:45,647 - mmselfsup - INFO - Epoch [65][300/427]	lr: 1.378e+00, eta: 2:44:50, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2907
2023-02-14 02:25:19,978 - mmselfsup - INFO - Epoch [65][350/427]	lr: 1.378e+00, eta: 2:44:18, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2954
2023-02-14 02:25:54,145 - mmselfsup - INFO - Epoch [65][400/427]	lr: 1.378e+00, eta: 2:43:46, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.3007
2023-02-14 02:26:53,224 - mmselfsup - INFO - Epoch [66][50/427]	lr: 1.310e+00, eta: 2:42:50, time: 0.826, data_time: 0.094, memory: 25346, loss: 0.2877
2023-02-14 02:27:27,440 - mmselfsup - INFO - Epoch [66][100/427]	lr: 1.310e+00, eta: 2:42:18, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.3049
2023-02-14 02:28:01,675 - mmselfsup - INFO - Epoch [66][150/427]	lr: 1.310e+00, eta: 2:41:46, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2969
2023-02-14 02:28:35,896 - mmselfsup - INFO - Epoch [66][200/427]	lr: 1.310e+00, eta: 2:41:14, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2882
2023-02-14 02:29:10,129 - mmselfsup - INFO - Epoch [66][250/427]	lr: 1.310e+00, eta: 2:40:42, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2982
2023-02-14 02:29:44,477 - mmselfsup - INFO - Epoch [66][300/427]	lr: 1.310e+00, eta: 2:40:10, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.3111
2023-02-14 02:30:18,755 - mmselfsup - INFO - Epoch [66][350/427]	lr: 1.310e+00, eta: 2:39:38, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2993
2023-02-14 02:30:53,118 - mmselfsup - INFO - Epoch [66][400/427]	lr: 1.310e+00, eta: 2:39:06, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2929
2023-02-14 02:31:52,181 - mmselfsup - INFO - Epoch [67][50/427]	lr: 1.244e+00, eta: 2:38:11, time: 0.826, data_time: 0.110, memory: 25346, loss: 0.2807
2023-02-14 02:32:26,415 - mmselfsup - INFO - Epoch [67][100/427]	lr: 1.244e+00, eta: 2:37:39, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2959
2023-02-14 02:33:00,649 - mmselfsup - INFO - Epoch [67][150/427]	lr: 1.244e+00, eta: 2:37:07, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2958
2023-02-14 02:33:34,853 - mmselfsup - INFO - Epoch [67][200/427]	lr: 1.244e+00, eta: 2:36:35, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2988
2023-02-14 02:34:09,083 - mmselfsup - INFO - Epoch [67][250/427]	lr: 1.244e+00, eta: 2:36:03, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2896
2023-02-14 02:34:43,413 - mmselfsup - INFO - Epoch [67][300/427]	lr: 1.244e+00, eta: 2:35:30, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2865
2023-02-14 02:35:17,737 - mmselfsup - INFO - Epoch [67][350/427]	lr: 1.244e+00, eta: 2:34:58, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2919
2023-02-14 02:35:51,991 - mmselfsup - INFO - Epoch [67][400/427]	lr: 1.244e+00, eta: 2:34:26, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2786
2023-02-14 02:36:50,827 - mmselfsup - INFO - Epoch [68][50/427]	lr: 1.178e+00, eta: 2:33:31, time: 0.822, data_time: 0.088, memory: 25346, loss: 0.2786
2023-02-14 02:37:25,111 - mmselfsup - INFO - Epoch [68][100/427]	lr: 1.178e+00, eta: 2:32:59, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2895
2023-02-14 02:37:59,360 - mmselfsup - INFO - Epoch [68][150/427]	lr: 1.178e+00, eta: 2:32:27, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2821
2023-02-14 02:38:33,562 - mmselfsup - INFO - Epoch [68][200/427]	lr: 1.178e+00, eta: 2:31:55, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2799
2023-02-14 02:39:07,822 - mmselfsup - INFO - Epoch [68][250/427]	lr: 1.178e+00, eta: 2:31:23, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2809
2023-02-14 02:39:42,082 - mmselfsup - INFO - Epoch [68][300/427]	lr: 1.178e+00, eta: 2:30:51, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2814
2023-02-14 02:40:16,367 - mmselfsup - INFO - Epoch [68][350/427]	lr: 1.178e+00, eta: 2:30:18, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2893
2023-02-14 02:40:50,666 - mmselfsup - INFO - Epoch [68][400/427]	lr: 1.178e+00, eta: 2:29:46, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2855
2023-02-14 02:41:49,659 - mmselfsup - INFO - Epoch [69][50/427]	lr: 1.114e+00, eta: 2:28:52, time: 0.827, data_time: 0.113, memory: 25346, loss: 0.2792
2023-02-14 02:42:23,833 - mmselfsup - INFO - Epoch [69][100/427]	lr: 1.114e+00, eta: 2:28:19, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2839
2023-02-14 02:42:58,131 - mmselfsup - INFO - Epoch [69][150/427]	lr: 1.114e+00, eta: 2:27:47, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2792
2023-02-14 02:43:32,381 - mmselfsup - INFO - Epoch [69][200/427]	lr: 1.114e+00, eta: 2:27:15, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2743
2023-02-14 02:44:06,616 - mmselfsup - INFO - Epoch [69][250/427]	lr: 1.114e+00, eta: 2:26:43, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2854
2023-02-14 02:44:40,840 - mmselfsup - INFO - Epoch [69][300/427]	lr: 1.114e+00, eta: 2:26:11, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2813
2023-02-14 02:45:15,049 - mmselfsup - INFO - Epoch [69][350/427]	lr: 1.114e+00, eta: 2:25:39, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2759
2023-02-14 02:45:49,382 - mmselfsup - INFO - Epoch [69][400/427]	lr: 1.114e+00, eta: 2:25:06, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2775
2023-02-14 02:46:48,007 - mmselfsup - INFO - Epoch [70][50/427]	lr: 1.051e+00, eta: 2:24:12, time: 0.818, data_time: 0.087, memory: 25346, loss: 0.2814
2023-02-14 02:47:22,214 - mmselfsup - INFO - Epoch [70][100/427]	lr: 1.051e+00, eta: 2:23:39, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2706
2023-02-14 02:47:56,435 - mmselfsup - INFO - Epoch [70][150/427]	lr: 1.051e+00, eta: 2:23:07, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2725
2023-02-14 02:48:30,725 - mmselfsup - INFO - Epoch [70][200/427]	lr: 1.051e+00, eta: 2:22:35, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2758
2023-02-14 02:49:05,049 - mmselfsup - INFO - Epoch [70][250/427]	lr: 1.051e+00, eta: 2:22:03, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2726
2023-02-14 02:49:39,308 - mmselfsup - INFO - Epoch [70][300/427]	lr: 1.051e+00, eta: 2:21:31, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2755
2023-02-14 02:50:13,541 - mmselfsup - INFO - Epoch [70][350/427]	lr: 1.051e+00, eta: 2:20:58, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2663
2023-02-14 02:50:47,721 - mmselfsup - INFO - Epoch [70][400/427]	lr: 1.051e+00, eta: 2:20:26, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2685
2023-02-14 02:51:05,458 - mmselfsup - INFO - Saving checkpoint at 70 epochs
2023-02-14 02:51:46,889 - mmselfsup - INFO - Epoch [71][50/427]	lr: 9.893e-01, eta: 2:19:31, time: 0.808, data_time: 0.117, memory: 25346, loss: 0.2649
2023-02-14 02:52:21,074 - mmselfsup - INFO - Epoch [71][100/427]	lr: 9.893e-01, eta: 2:18:59, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2626
2023-02-14 02:52:55,290 - mmselfsup - INFO - Epoch [71][150/427]	lr: 9.893e-01, eta: 2:18:27, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2704
2023-02-14 02:53:29,560 - mmselfsup - INFO - Epoch [71][200/427]	lr: 9.893e-01, eta: 2:17:55, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2751
2023-02-14 02:54:03,680 - mmselfsup - INFO - Epoch [71][250/427]	lr: 9.893e-01, eta: 2:17:22, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.2604
2023-02-14 02:54:37,896 - mmselfsup - INFO - Epoch [71][300/427]	lr: 9.893e-01, eta: 2:16:50, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2693
2023-02-14 02:55:12,024 - mmselfsup - INFO - Epoch [71][350/427]	lr: 9.893e-01, eta: 2:16:18, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2642
2023-02-14 02:55:46,107 - mmselfsup - INFO - Epoch [71][400/427]	lr: 9.893e-01, eta: 2:15:46, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.2720
2023-02-14 02:56:45,197 - mmselfsup - INFO - Epoch [72][50/427]	lr: 9.290e-01, eta: 2:14:51, time: 0.827, data_time: 0.108, memory: 25346, loss: 0.2637
2023-02-14 02:57:19,477 - mmselfsup - INFO - Epoch [72][100/427]	lr: 9.290e-01, eta: 2:14:19, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2651
2023-02-14 02:57:53,690 - mmselfsup - INFO - Epoch [72][150/427]	lr: 9.290e-01, eta: 2:13:47, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2610
2023-02-14 02:58:27,997 - mmselfsup - INFO - Epoch [72][200/427]	lr: 9.290e-01, eta: 2:13:15, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2676
2023-02-14 02:59:02,458 - mmselfsup - INFO - Epoch [72][250/427]	lr: 9.290e-01, eta: 2:12:43, time: 0.689, data_time: 0.003, memory: 25346, loss: 0.2673
2023-02-14 02:59:36,757 - mmselfsup - INFO - Epoch [72][300/427]	lr: 9.290e-01, eta: 2:12:10, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2675
2023-02-14 03:00:11,087 - mmselfsup - INFO - Epoch [72][350/427]	lr: 9.290e-01, eta: 2:11:38, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2599
2023-02-14 03:00:45,397 - mmselfsup - INFO - Epoch [72][400/427]	lr: 9.290e-01, eta: 2:11:06, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2598
2023-02-14 03:01:44,586 - mmselfsup - INFO - Epoch [73][50/427]	lr: 8.702e-01, eta: 2:10:12, time: 0.829, data_time: 0.115, memory: 25346, loss: 0.2684
2023-02-14 03:02:18,777 - mmselfsup - INFO - Epoch [73][100/427]	lr: 8.702e-01, eta: 2:09:40, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2575
2023-02-14 03:02:53,079 - mmselfsup - INFO - Epoch [73][150/427]	lr: 8.702e-01, eta: 2:09:07, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2629
2023-02-14 03:03:27,282 - mmselfsup - INFO - Epoch [73][200/427]	lr: 8.702e-01, eta: 2:08:35, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2564
2023-02-14 03:04:01,582 - mmselfsup - INFO - Epoch [73][250/427]	lr: 8.702e-01, eta: 2:08:03, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2536
2023-02-14 03:04:35,946 - mmselfsup - INFO - Epoch [73][300/427]	lr: 8.702e-01, eta: 2:07:30, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2614
2023-02-14 03:05:10,222 - mmselfsup - INFO - Epoch [73][350/427]	lr: 8.702e-01, eta: 2:06:58, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2615
2023-02-14 03:05:44,504 - mmselfsup - INFO - Epoch [73][400/427]	lr: 8.702e-01, eta: 2:06:26, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2654
2023-02-14 03:06:43,417 - mmselfsup - INFO - Epoch [74][50/427]	lr: 8.129e-01, eta: 2:05:32, time: 0.823, data_time: 0.108, memory: 25346, loss: 0.2543
2023-02-14 03:07:17,745 - mmselfsup - INFO - Epoch [74][100/427]	lr: 8.129e-01, eta: 2:05:00, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2552
2023-02-14 03:07:52,072 - mmselfsup - INFO - Epoch [74][150/427]	lr: 8.129e-01, eta: 2:04:27, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2549
2023-02-14 03:08:26,327 - mmselfsup - INFO - Epoch [74][200/427]	lr: 8.129e-01, eta: 2:03:55, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2621
2023-02-14 03:09:00,627 - mmselfsup - INFO - Epoch [74][250/427]	lr: 8.129e-01, eta: 2:03:23, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2479
2023-02-14 03:09:34,871 - mmselfsup - INFO - Epoch [74][300/427]	lr: 8.129e-01, eta: 2:02:51, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2578
2023-02-14 03:10:09,088 - mmselfsup - INFO - Epoch [74][350/427]	lr: 8.129e-01, eta: 2:02:18, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2576
2023-02-14 03:10:43,441 - mmselfsup - INFO - Epoch [74][400/427]	lr: 8.129e-01, eta: 2:01:46, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2511
2023-02-14 03:11:42,394 - mmselfsup - INFO - Epoch [75][50/427]	lr: 7.571e-01, eta: 2:00:52, time: 0.823, data_time: 0.114, memory: 25346, loss: 0.2548
2023-02-14 03:12:16,624 - mmselfsup - INFO - Epoch [75][100/427]	lr: 7.571e-01, eta: 2:00:20, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2510
2023-02-14 03:12:50,971 - mmselfsup - INFO - Epoch [75][150/427]	lr: 7.571e-01, eta: 1:59:47, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2474
2023-02-14 03:13:25,114 - mmselfsup - INFO - Epoch [75][200/427]	lr: 7.571e-01, eta: 1:59:15, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2562
2023-02-14 03:13:59,204 - mmselfsup - INFO - Epoch [75][250/427]	lr: 7.571e-01, eta: 1:58:43, time: 0.682, data_time: 0.001, memory: 25346, loss: 0.2492
2023-02-14 03:14:33,410 - mmselfsup - INFO - Epoch [75][300/427]	lr: 7.571e-01, eta: 1:58:10, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2486
2023-02-14 03:15:07,605 - mmselfsup - INFO - Epoch [75][350/427]	lr: 7.571e-01, eta: 1:57:38, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2575
2023-02-14 03:15:41,900 - mmselfsup - INFO - Epoch [75][400/427]	lr: 7.571e-01, eta: 1:57:06, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2450
2023-02-14 03:16:40,576 - mmselfsup - INFO - Epoch [76][50/427]	lr: 7.029e-01, eta: 1:56:12, time: 0.818, data_time: 0.090, memory: 25346, loss: 0.2487
2023-02-14 03:17:14,825 - mmselfsup - INFO - Epoch [76][100/427]	lr: 7.029e-01, eta: 1:55:40, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2465
2023-02-14 03:17:49,183 - mmselfsup - INFO - Epoch [76][150/427]	lr: 7.029e-01, eta: 1:55:07, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2489
2023-02-14 03:18:23,352 - mmselfsup - INFO - Epoch [76][200/427]	lr: 7.029e-01, eta: 1:54:35, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.2562
2023-02-14 03:18:57,638 - mmselfsup - INFO - Epoch [76][250/427]	lr: 7.029e-01, eta: 1:54:03, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2469
2023-02-14 03:19:31,910 - mmselfsup - INFO - Epoch [76][300/427]	lr: 7.029e-01, eta: 1:53:30, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2467
2023-02-14 03:20:06,135 - mmselfsup - INFO - Epoch [76][350/427]	lr: 7.029e-01, eta: 1:52:58, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2427
2023-02-14 03:20:40,376 - mmselfsup - INFO - Epoch [76][400/427]	lr: 7.029e-01, eta: 1:52:25, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2472
2023-02-14 03:21:39,774 - mmselfsup - INFO - Epoch [77][50/427]	lr: 6.505e-01, eta: 1:51:32, time: 0.833, data_time: 0.115, memory: 25346, loss: 0.2474
2023-02-14 03:22:13,947 - mmselfsup - INFO - Epoch [77][100/427]	lr: 6.505e-01, eta: 1:51:00, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2421
2023-02-14 03:22:48,229 - mmselfsup - INFO - Epoch [77][150/427]	lr: 6.505e-01, eta: 1:50:27, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2509
2023-02-14 03:23:22,453 - mmselfsup - INFO - Epoch [77][200/427]	lr: 6.505e-01, eta: 1:49:55, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2322
2023-02-14 03:23:56,720 - mmselfsup - INFO - Epoch [77][250/427]	lr: 6.505e-01, eta: 1:49:23, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2428
2023-02-14 03:24:30,935 - mmselfsup - INFO - Epoch [77][300/427]	lr: 6.505e-01, eta: 1:48:50, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2499
2023-02-14 03:25:05,192 - mmselfsup - INFO - Epoch [77][350/427]	lr: 6.505e-01, eta: 1:48:18, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2491
2023-02-14 03:25:39,448 - mmselfsup - INFO - Epoch [77][400/427]	lr: 6.505e-01, eta: 1:47:45, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2357
2023-02-14 03:26:38,543 - mmselfsup - INFO - Epoch [78][50/427]	lr: 5.997e-01, eta: 1:46:52, time: 0.826, data_time: 0.101, memory: 25346, loss: 0.2396
2023-02-14 03:27:12,797 - mmselfsup - INFO - Epoch [78][100/427]	lr: 5.997e-01, eta: 1:46:20, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2389
2023-02-14 03:27:47,117 - mmselfsup - INFO - Epoch [78][150/427]	lr: 5.997e-01, eta: 1:45:47, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2397
2023-02-14 03:28:21,526 - mmselfsup - INFO - Epoch [78][200/427]	lr: 5.997e-01, eta: 1:45:15, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.2379
2023-02-14 03:28:55,840 - mmselfsup - INFO - Epoch [78][250/427]	lr: 5.997e-01, eta: 1:44:43, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2414
2023-02-14 03:29:30,079 - mmselfsup - INFO - Epoch [78][300/427]	lr: 5.997e-01, eta: 1:44:10, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2413
2023-02-14 03:30:04,301 - mmselfsup - INFO - Epoch [78][350/427]	lr: 5.997e-01, eta: 1:43:38, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2371
2023-02-14 03:30:38,663 - mmselfsup - INFO - Epoch [78][400/427]	lr: 5.997e-01, eta: 1:43:05, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2408
2023-02-14 03:31:37,940 - mmselfsup - INFO - Epoch [79][50/427]	lr: 5.508e-01, eta: 1:42:12, time: 0.831, data_time: 0.098, memory: 25346, loss: 0.2356
2023-02-14 03:32:12,201 - mmselfsup - INFO - Epoch [79][100/427]	lr: 5.508e-01, eta: 1:41:40, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2396
2023-02-14 03:32:46,462 - mmselfsup - INFO - Epoch [79][150/427]	lr: 5.508e-01, eta: 1:41:07, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2353
2023-02-14 03:33:20,686 - mmselfsup - INFO - Epoch [79][200/427]	lr: 5.508e-01, eta: 1:40:35, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2417
2023-02-14 03:33:54,900 - mmselfsup - INFO - Epoch [79][250/427]	lr: 5.508e-01, eta: 1:40:03, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2348
2023-02-14 03:34:29,145 - mmselfsup - INFO - Epoch [79][300/427]	lr: 5.508e-01, eta: 1:39:30, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2383
2023-02-14 03:35:03,322 - mmselfsup - INFO - Epoch [79][350/427]	lr: 5.508e-01, eta: 1:38:58, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2351
2023-02-14 03:35:37,653 - mmselfsup - INFO - Epoch [79][400/427]	lr: 5.508e-01, eta: 1:38:25, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2412
2023-02-14 03:36:37,172 - mmselfsup - INFO - Epoch [80][50/427]	lr: 5.036e-01, eta: 1:37:32, time: 0.834, data_time: 0.115, memory: 25346, loss: 0.2354
2023-02-14 03:37:11,534 - mmselfsup - INFO - Epoch [80][100/427]	lr: 5.036e-01, eta: 1:37:00, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2342
2023-02-14 03:37:45,752 - mmselfsup - INFO - Epoch [80][150/427]	lr: 5.036e-01, eta: 1:36:27, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2389
2023-02-14 03:38:19,910 - mmselfsup - INFO - Epoch [80][200/427]	lr: 5.036e-01, eta: 1:35:55, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.2361
2023-02-14 03:38:54,149 - mmselfsup - INFO - Epoch [80][250/427]	lr: 5.036e-01, eta: 1:35:23, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2405
2023-02-14 03:39:28,448 - mmselfsup - INFO - Epoch [80][300/427]	lr: 5.036e-01, eta: 1:34:50, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2344
2023-02-14 03:40:02,687 - mmselfsup - INFO - Epoch [80][350/427]	lr: 5.036e-01, eta: 1:34:18, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2373
2023-02-14 03:40:36,999 - mmselfsup - INFO - Epoch [80][400/427]	lr: 5.036e-01, eta: 1:33:45, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2331
2023-02-14 03:40:54,698 - mmselfsup - INFO - Saving checkpoint at 80 epochs
2023-02-14 03:41:36,362 - mmselfsup - INFO - Epoch [81][50/427]	lr: 4.584e-01, eta: 1:32:52, time: 0.809, data_time: 0.115, memory: 25346, loss: 0.2368
2023-02-14 03:42:10,693 - mmselfsup - INFO - Epoch [81][100/427]	lr: 4.584e-01, eta: 1:32:20, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2333
2023-02-14 03:42:44,999 - mmselfsup - INFO - Epoch [81][150/427]	lr: 4.584e-01, eta: 1:31:47, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2408
2023-02-14 03:43:19,224 - mmselfsup - INFO - Epoch [81][200/427]	lr: 4.584e-01, eta: 1:31:15, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2253
2023-02-14 03:43:53,561 - mmselfsup - INFO - Epoch [81][250/427]	lr: 4.584e-01, eta: 1:30:42, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2324
2023-02-14 03:44:27,758 - mmselfsup - INFO - Epoch [81][300/427]	lr: 4.584e-01, eta: 1:30:10, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2314
2023-02-14 03:45:01,999 - mmselfsup - INFO - Epoch [81][350/427]	lr: 4.584e-01, eta: 1:29:37, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2356
2023-02-14 03:45:36,293 - mmselfsup - INFO - Epoch [81][400/427]	lr: 4.584e-01, eta: 1:29:05, time: 0.686, data_time: 0.000, memory: 25346, loss: 0.2297
2023-02-14 03:46:35,830 - mmselfsup - INFO - Epoch [82][50/427]	lr: 4.150e-01, eta: 1:28:12, time: 0.836, data_time: 0.110, memory: 25346, loss: 0.2339
2023-02-14 03:47:10,232 - mmselfsup - INFO - Epoch [82][100/427]	lr: 4.150e-01, eta: 1:27:40, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.2331
2023-02-14 03:47:44,627 - mmselfsup - INFO - Epoch [82][150/427]	lr: 4.150e-01, eta: 1:27:07, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.2335
2023-02-14 03:48:18,896 - mmselfsup - INFO - Epoch [82][200/427]	lr: 4.150e-01, eta: 1:26:35, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2293
2023-02-14 03:48:53,160 - mmselfsup - INFO - Epoch [82][250/427]	lr: 4.150e-01, eta: 1:26:02, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2221
2023-02-14 03:49:27,386 - mmselfsup - INFO - Epoch [82][300/427]	lr: 4.150e-01, eta: 1:25:30, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2255
2023-02-14 03:50:01,611 - mmselfsup - INFO - Epoch [82][350/427]	lr: 4.150e-01, eta: 1:24:57, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2306
2023-02-14 03:50:35,940 - mmselfsup - INFO - Epoch [82][400/427]	lr: 4.150e-01, eta: 1:24:25, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2246
2023-02-14 03:51:34,940 - mmselfsup - INFO - Epoch [83][50/427]	lr: 3.736e-01, eta: 1:23:32, time: 0.826, data_time: 0.105, memory: 25346, loss: 0.2319
2023-02-14 03:52:09,150 - mmselfsup - INFO - Epoch [83][100/427]	lr: 3.736e-01, eta: 1:23:00, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2281
2023-02-14 03:52:43,560 - mmselfsup - INFO - Epoch [83][150/427]	lr: 3.736e-01, eta: 1:22:27, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.2291
2023-02-14 03:53:17,859 - mmselfsup - INFO - Epoch [83][200/427]	lr: 3.736e-01, eta: 1:21:55, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2275
2023-02-14 03:53:52,145 - mmselfsup - INFO - Epoch [83][250/427]	lr: 3.736e-01, eta: 1:21:22, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2378
2023-02-14 03:54:26,412 - mmselfsup - INFO - Epoch [83][300/427]	lr: 3.736e-01, eta: 1:20:50, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2310
2023-02-14 03:55:00,659 - mmselfsup - INFO - Epoch [83][350/427]	lr: 3.736e-01, eta: 1:20:17, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2346
2023-02-14 03:55:34,900 - mmselfsup - INFO - Epoch [83][400/427]	lr: 3.736e-01, eta: 1:19:44, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2287
2023-02-14 03:56:34,310 - mmselfsup - INFO - Epoch [84][50/427]	lr: 3.342e-01, eta: 1:18:52, time: 0.833, data_time: 0.093, memory: 25346, loss: 0.2241
2023-02-14 03:57:08,659 - mmselfsup - INFO - Epoch [84][100/427]	lr: 3.342e-01, eta: 1:18:20, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2227
2023-02-14 03:57:42,944 - mmselfsup - INFO - Epoch [84][150/427]	lr: 3.342e-01, eta: 1:17:47, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2280
2023-02-14 03:58:17,043 - mmselfsup - INFO - Epoch [84][200/427]	lr: 3.342e-01, eta: 1:17:14, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.2217
2023-02-14 03:58:51,266 - mmselfsup - INFO - Epoch [84][250/427]	lr: 3.342e-01, eta: 1:16:42, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2271
2023-02-14 03:59:25,451 - mmselfsup - INFO - Epoch [84][300/427]	lr: 3.342e-01, eta: 1:16:09, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2250
2023-02-14 03:59:59,560 - mmselfsup - INFO - Epoch [84][350/427]	lr: 3.342e-01, eta: 1:15:37, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.2314
2023-02-14 04:00:33,770 - mmselfsup - INFO - Epoch [84][400/427]	lr: 3.342e-01, eta: 1:15:04, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2256
2023-02-14 04:01:32,504 - mmselfsup - INFO - Epoch [85][50/427]	lr: 2.969e-01, eta: 1:14:12, time: 0.820, data_time: 0.088, memory: 25346, loss: 0.2197
2023-02-14 04:02:06,724 - mmselfsup - INFO - Epoch [85][100/427]	lr: 2.969e-01, eta: 1:13:39, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2212
2023-02-14 04:02:40,956 - mmselfsup - INFO - Epoch [85][150/427]	lr: 2.969e-01, eta: 1:13:07, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2265
2023-02-14 04:03:15,146 - mmselfsup - INFO - Epoch [85][200/427]	lr: 2.969e-01, eta: 1:12:34, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2231
2023-02-14 04:03:49,439 - mmselfsup - INFO - Epoch [85][250/427]	lr: 2.969e-01, eta: 1:12:01, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2273
2023-02-14 04:04:23,684 - mmselfsup - INFO - Epoch [85][300/427]	lr: 2.969e-01, eta: 1:11:29, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2220
2023-02-14 04:04:57,946 - mmselfsup - INFO - Epoch [85][350/427]	lr: 2.969e-01, eta: 1:10:56, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2212
2023-02-14 04:05:32,159 - mmselfsup - INFO - Epoch [85][400/427]	lr: 2.969e-01, eta: 1:10:24, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2267
2023-02-14 04:06:31,617 - mmselfsup - INFO - Epoch [86][50/427]	lr: 2.616e-01, eta: 1:09:32, time: 0.833, data_time: 0.109, memory: 25346, loss: 0.2271
2023-02-14 04:07:05,973 - mmselfsup - INFO - Epoch [86][100/427]	lr: 2.616e-01, eta: 1:08:59, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2231
2023-02-14 04:07:40,198 - mmselfsup - INFO - Epoch [86][150/427]	lr: 2.616e-01, eta: 1:08:26, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2271
2023-02-14 04:08:14,388 - mmselfsup - INFO - Epoch [86][200/427]	lr: 2.616e-01, eta: 1:07:54, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2210
2023-02-14 04:08:48,653 - mmselfsup - INFO - Epoch [86][250/427]	lr: 2.616e-01, eta: 1:07:21, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2258
2023-02-14 04:09:22,901 - mmselfsup - INFO - Epoch [86][300/427]	lr: 2.616e-01, eta: 1:06:49, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2192
2023-02-14 04:09:57,110 - mmselfsup - INFO - Epoch [86][350/427]	lr: 2.616e-01, eta: 1:06:16, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2220
2023-02-14 04:10:31,294 - mmselfsup - INFO - Epoch [86][400/427]	lr: 2.616e-01, eta: 1:05:43, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2289
2023-02-14 04:11:30,366 - mmselfsup - INFO - Epoch [87][50/427]	lr: 2.284e-01, eta: 1:04:51, time: 0.827, data_time: 0.095, memory: 25346, loss: 0.2261
2023-02-14 04:12:04,673 - mmselfsup - INFO - Epoch [87][100/427]	lr: 2.284e-01, eta: 1:04:19, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2229
2023-02-14 04:12:38,805 - mmselfsup - INFO - Epoch [87][150/427]	lr: 2.284e-01, eta: 1:03:46, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2131
2023-02-14 04:13:12,977 - mmselfsup - INFO - Epoch [87][200/427]	lr: 2.284e-01, eta: 1:03:14, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2217
2023-02-14 04:13:47,143 - mmselfsup - INFO - Epoch [87][250/427]	lr: 2.284e-01, eta: 1:02:41, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2181
2023-02-14 04:14:21,389 - mmselfsup - INFO - Epoch [87][300/427]	lr: 2.284e-01, eta: 1:02:08, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2207
2023-02-14 04:14:55,614 - mmselfsup - INFO - Epoch [87][350/427]	lr: 2.284e-01, eta: 1:01:36, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2292
2023-02-14 04:15:29,866 - mmselfsup - INFO - Epoch [87][400/427]	lr: 2.284e-01, eta: 1:01:03, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2240
2023-02-14 04:16:29,140 - mmselfsup - INFO - Epoch [88][50/427]	lr: 1.974e-01, eta: 1:00:11, time: 0.831, data_time: 0.108, memory: 25346, loss: 0.2248
2023-02-14 04:17:03,356 - mmselfsup - INFO - Epoch [88][100/427]	lr: 1.974e-01, eta: 0:59:39, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2211
2023-02-14 04:17:37,482 - mmselfsup - INFO - Epoch [88][150/427]	lr: 1.974e-01, eta: 0:59:06, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.2266
2023-02-14 04:18:11,789 - mmselfsup - INFO - Epoch [88][200/427]	lr: 1.974e-01, eta: 0:58:33, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2203
2023-02-14 04:18:46,024 - mmselfsup - INFO - Epoch [88][250/427]	lr: 1.974e-01, eta: 0:58:01, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2161
2023-02-14 04:19:20,198 - mmselfsup - INFO - Epoch [88][300/427]	lr: 1.974e-01, eta: 0:57:28, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.2202
2023-02-14 04:19:54,396 - mmselfsup - INFO - Epoch [88][350/427]	lr: 1.974e-01, eta: 0:56:55, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2192
2023-02-14 04:20:28,644 - mmselfsup - INFO - Epoch [88][400/427]	lr: 1.974e-01, eta: 0:56:23, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2193
2023-02-14 04:21:28,114 - mmselfsup - INFO - Epoch [89][50/427]	lr: 1.685e-01, eta: 0:55:31, time: 0.834, data_time: 0.109, memory: 25346, loss: 0.2178
2023-02-14 04:22:02,318 - mmselfsup - INFO - Epoch [89][100/427]	lr: 1.685e-01, eta: 0:54:58, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2225
2023-02-14 04:22:36,593 - mmselfsup - INFO - Epoch [89][150/427]	lr: 1.685e-01, eta: 0:54:26, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2213
2023-02-14 04:23:10,854 - mmselfsup - INFO - Epoch [89][200/427]	lr: 1.685e-01, eta: 0:53:53, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2217
2023-02-14 04:23:45,116 - mmselfsup - INFO - Epoch [89][250/427]	lr: 1.685e-01, eta: 0:53:20, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2182
2023-02-14 04:24:19,389 - mmselfsup - INFO - Epoch [89][300/427]	lr: 1.685e-01, eta: 0:52:48, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2127
2023-02-14 04:24:53,568 - mmselfsup - INFO - Epoch [89][350/427]	lr: 1.685e-01, eta: 0:52:15, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2257
2023-02-14 04:25:27,725 - mmselfsup - INFO - Epoch [89][400/427]	lr: 1.685e-01, eta: 0:51:42, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2151
2023-02-14 04:26:26,789 - mmselfsup - INFO - Epoch [90][50/427]	lr: 1.419e-01, eta: 0:50:51, time: 0.825, data_time: 0.114, memory: 25346, loss: 0.2191
2023-02-14 04:27:00,935 - mmselfsup - INFO - Epoch [90][100/427]	lr: 1.419e-01, eta: 0:50:18, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.2173
2023-02-14 04:27:35,171 - mmselfsup - INFO - Epoch [90][150/427]	lr: 1.419e-01, eta: 0:49:45, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2231
2023-02-14 04:28:09,373 - mmselfsup - INFO - Epoch [90][200/427]	lr: 1.419e-01, eta: 0:49:13, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2220
2023-02-14 04:28:43,622 - mmselfsup - INFO - Epoch [90][250/427]	lr: 1.419e-01, eta: 0:48:40, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2185
2023-02-14 04:29:17,880 - mmselfsup - INFO - Epoch [90][300/427]	lr: 1.419e-01, eta: 0:48:07, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2240
2023-02-14 04:29:52,041 - mmselfsup - INFO - Epoch [90][350/427]	lr: 1.419e-01, eta: 0:47:35, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.2170
2023-02-14 04:30:26,158 - mmselfsup - INFO - Epoch [90][400/427]	lr: 1.419e-01, eta: 0:47:02, time: 0.682, data_time: 0.003, memory: 25346, loss: 0.2174
2023-02-14 04:30:43,931 - mmselfsup - INFO - Saving checkpoint at 90 epochs
2023-02-14 04:31:25,411 - mmselfsup - INFO - Epoch [91][50/427]	lr: 1.175e-01, eta: 0:46:10, time: 0.805, data_time: 0.109, memory: 25346, loss: 0.2237
2023-02-14 04:31:59,718 - mmselfsup - INFO - Epoch [91][100/427]	lr: 1.175e-01, eta: 0:45:38, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2157
2023-02-14 04:32:34,025 - mmselfsup - INFO - Epoch [91][150/427]	lr: 1.175e-01, eta: 0:45:05, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2118
2023-02-14 04:33:08,239 - mmselfsup - INFO - Epoch [91][200/427]	lr: 1.175e-01, eta: 0:44:32, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2237
2023-02-14 04:33:42,512 - mmselfsup - INFO - Epoch [91][250/427]	lr: 1.175e-01, eta: 0:43:59, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2189
2023-02-14 04:34:16,732 - mmselfsup - INFO - Epoch [91][300/427]	lr: 1.175e-01, eta: 0:43:27, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2178
2023-02-14 04:34:51,054 - mmselfsup - INFO - Epoch [91][350/427]	lr: 1.175e-01, eta: 0:42:54, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2207
2023-02-14 04:35:25,317 - mmselfsup - INFO - Epoch [91][400/427]	lr: 1.175e-01, eta: 0:42:21, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2154
2023-02-14 04:36:24,018 - mmselfsup - INFO - Epoch [92][50/427]	lr: 9.530e-02, eta: 0:41:30, time: 0.818, data_time: 0.110, memory: 25346, loss: 0.2103
2023-02-14 04:36:58,378 - mmselfsup - INFO - Epoch [92][100/427]	lr: 9.530e-02, eta: 0:40:57, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2140
2023-02-14 04:37:32,720 - mmselfsup - INFO - Epoch [92][150/427]	lr: 9.530e-02, eta: 0:40:25, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2127
2023-02-14 04:38:06,998 - mmselfsup - INFO - Epoch [92][200/427]	lr: 9.530e-02, eta: 0:39:52, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2148
2023-02-14 04:38:41,402 - mmselfsup - INFO - Epoch [92][250/427]	lr: 9.530e-02, eta: 0:39:19, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.2247
2023-02-14 04:39:15,673 - mmselfsup - INFO - Epoch [92][300/427]	lr: 9.530e-02, eta: 0:38:46, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2156
2023-02-14 04:39:50,002 - mmselfsup - INFO - Epoch [92][350/427]	lr: 9.530e-02, eta: 0:38:14, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2211
2023-02-14 04:40:24,415 - mmselfsup - INFO - Epoch [92][400/427]	lr: 9.530e-02, eta: 0:37:41, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.2179
2023-02-14 04:41:23,860 - mmselfsup - INFO - Epoch [93][50/427]	lr: 7.540e-02, eta: 0:36:50, time: 0.834, data_time: 0.087, memory: 25346, loss: 0.2198
2023-02-14 04:41:58,124 - mmselfsup - INFO - Epoch [93][100/427]	lr: 7.540e-02, eta: 0:36:17, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2168
2023-02-14 04:42:32,494 - mmselfsup - INFO - Epoch [93][150/427]	lr: 7.540e-02, eta: 0:35:44, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2216
2023-02-14 04:43:06,677 - mmselfsup - INFO - Epoch [93][200/427]	lr: 7.540e-02, eta: 0:35:12, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2144
2023-02-14 04:43:40,887 - mmselfsup - INFO - Epoch [93][250/427]	lr: 7.540e-02, eta: 0:34:39, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2180
2023-02-14 04:44:15,164 - mmselfsup - INFO - Epoch [93][300/427]	lr: 7.540e-02, eta: 0:34:06, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2150
2023-02-14 04:44:49,495 - mmselfsup - INFO - Epoch [93][350/427]	lr: 7.540e-02, eta: 0:33:33, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2170
2023-02-14 04:45:23,744 - mmselfsup - INFO - Epoch [93][400/427]	lr: 7.540e-02, eta: 0:33:01, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2117
2023-02-14 04:46:22,792 - mmselfsup - INFO - Epoch [94][50/427]	lr: 5.780e-02, eta: 0:32:09, time: 0.826, data_time: 0.116, memory: 25346, loss: 0.2137
2023-02-14 04:46:57,024 - mmselfsup - INFO - Epoch [94][100/427]	lr: 5.780e-02, eta: 0:31:37, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2182
2023-02-14 04:47:31,221 - mmselfsup - INFO - Epoch [94][150/427]	lr: 5.780e-02, eta: 0:31:04, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2168
2023-02-14 04:48:05,442 - mmselfsup - INFO - Epoch [94][200/427]	lr: 5.780e-02, eta: 0:30:31, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2207
2023-02-14 04:48:39,752 - mmselfsup - INFO - Epoch [94][250/427]	lr: 5.780e-02, eta: 0:29:58, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2131
2023-02-14 04:49:14,112 - mmselfsup - INFO - Epoch [94][300/427]	lr: 5.780e-02, eta: 0:29:26, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2083
2023-02-14 04:49:48,342 - mmselfsup - INFO - Epoch [94][350/427]	lr: 5.780e-02, eta: 0:28:53, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2178
2023-02-14 04:50:22,685 - mmselfsup - INFO - Epoch [94][400/427]	lr: 5.780e-02, eta: 0:28:20, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2144
2023-02-14 04:51:21,738 - mmselfsup - INFO - Epoch [95][50/427]	lr: 4.251e-02, eta: 0:27:29, time: 0.826, data_time: 0.102, memory: 25346, loss: 0.2110
2023-02-14 04:51:56,056 - mmselfsup - INFO - Epoch [95][100/427]	lr: 4.251e-02, eta: 0:26:56, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2104
2023-02-14 04:52:30,385 - mmselfsup - INFO - Epoch [95][150/427]	lr: 4.251e-02, eta: 0:26:24, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2135
2023-02-14 04:53:04,760 - mmselfsup - INFO - Epoch [95][200/427]	lr: 4.251e-02, eta: 0:25:51, time: 0.687, data_time: 0.001, memory: 25346, loss: 0.2178
2023-02-14 04:53:39,138 - mmselfsup - INFO - Epoch [95][250/427]	lr: 4.251e-02, eta: 0:25:18, time: 0.688, data_time: 0.001, memory: 25346, loss: 0.2089
2023-02-14 04:54:13,443 - mmselfsup - INFO - Epoch [95][300/427]	lr: 4.251e-02, eta: 0:24:45, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2158
2023-02-14 04:54:47,749 - mmselfsup - INFO - Epoch [95][350/427]	lr: 4.251e-02, eta: 0:24:13, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2164
2023-02-14 04:55:21,959 - mmselfsup - INFO - Epoch [95][400/427]	lr: 4.251e-02, eta: 0:23:40, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2159
2023-02-14 04:56:21,002 - mmselfsup - INFO - Epoch [96][50/427]	lr: 2.955e-02, eta: 0:22:49, time: 0.826, data_time: 0.109, memory: 25346, loss: 0.2206
2023-02-14 04:56:55,371 - mmselfsup - INFO - Epoch [96][100/427]	lr: 2.955e-02, eta: 0:22:16, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2149
2023-02-14 04:57:29,748 - mmselfsup - INFO - Epoch [96][150/427]	lr: 2.955e-02, eta: 0:21:43, time: 0.688, data_time: 0.003, memory: 25346, loss: 0.2089
2023-02-14 04:58:04,011 - mmselfsup - INFO - Epoch [96][200/427]	lr: 2.955e-02, eta: 0:21:10, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2207
2023-02-14 04:58:38,347 - mmselfsup - INFO - Epoch [96][250/427]	lr: 2.955e-02, eta: 0:20:38, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2112
2023-02-14 04:59:12,631 - mmselfsup - INFO - Epoch [96][300/427]	lr: 2.955e-02, eta: 0:20:05, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2080
2023-02-14 04:59:46,937 - mmselfsup - INFO - Epoch [96][350/427]	lr: 2.955e-02, eta: 0:19:32, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2165
2023-02-14 05:00:21,240 - mmselfsup - INFO - Epoch [96][400/427]	lr: 2.955e-02, eta: 0:18:59, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2106
2023-02-14 05:01:20,347 - mmselfsup - INFO - Epoch [97][50/427]	lr: 1.892e-02, eta: 0:18:08, time: 0.828, data_time: 0.116, memory: 25346, loss: 0.2184
2023-02-14 05:01:54,557 - mmselfsup - INFO - Epoch [97][100/427]	lr: 1.892e-02, eta: 0:17:36, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2170
2023-02-14 05:02:28,698 - mmselfsup - INFO - Epoch [97][150/427]	lr: 1.892e-02, eta: 0:17:03, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2215
2023-02-14 05:03:02,987 - mmselfsup - INFO - Epoch [97][200/427]	lr: 1.892e-02, eta: 0:16:30, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2138
2023-02-14 05:03:37,152 - mmselfsup - INFO - Epoch [97][250/427]	lr: 1.892e-02, eta: 0:15:57, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2163
2023-02-14 05:04:11,307 - mmselfsup - INFO - Epoch [97][300/427]	lr: 1.892e-02, eta: 0:15:24, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2167
2023-02-14 05:04:45,563 - mmselfsup - INFO - Epoch [97][350/427]	lr: 1.892e-02, eta: 0:14:52, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2104
2023-02-14 05:05:19,843 - mmselfsup - INFO - Epoch [97][400/427]	lr: 1.892e-02, eta: 0:14:19, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2151
2023-02-14 05:06:18,962 - mmselfsup - INFO - Epoch [98][50/427]	lr: 1.065e-02, eta: 0:13:28, time: 0.827, data_time: 0.099, memory: 25346, loss: 0.2066
2023-02-14 05:06:53,191 - mmselfsup - INFO - Epoch [98][100/427]	lr: 1.065e-02, eta: 0:12:55, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2167
2023-02-14 05:07:27,340 - mmselfsup - INFO - Epoch [98][150/427]	lr: 1.065e-02, eta: 0:12:22, time: 0.683, data_time: 0.003, memory: 25346, loss: 0.2111
2023-02-14 05:08:01,606 - mmselfsup - INFO - Epoch [98][200/427]	lr: 1.065e-02, eta: 0:11:50, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2093
2023-02-14 05:08:35,914 - mmselfsup - INFO - Epoch [98][250/427]	lr: 1.065e-02, eta: 0:11:17, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2139
2023-02-14 05:09:10,238 - mmselfsup - INFO - Epoch [98][300/427]	lr: 1.065e-02, eta: 0:10:44, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2150
2023-02-14 05:09:44,458 - mmselfsup - INFO - Epoch [98][350/427]	lr: 1.065e-02, eta: 0:10:11, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2112
2023-02-14 05:10:18,754 - mmselfsup - INFO - Epoch [98][400/427]	lr: 1.065e-02, eta: 0:09:38, time: 0.686, data_time: 0.003, memory: 25346, loss: 0.2138
2023-02-14 05:11:17,839 - mmselfsup - INFO - Epoch [99][50/427]	lr: 4.736e-03, eta: 0:08:48, time: 0.826, data_time: 0.115, memory: 25346, loss: 0.2094
2023-02-14 05:11:52,108 - mmselfsup - INFO - Epoch [99][100/427]	lr: 4.736e-03, eta: 0:08:15, time: 0.685, data_time: 0.001, memory: 25346, loss: 0.2140
2023-02-14 05:12:26,314 - mmselfsup - INFO - Epoch [99][150/427]	lr: 4.736e-03, eta: 0:07:42, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2155
2023-02-14 05:13:00,486 - mmselfsup - INFO - Epoch [99][200/427]	lr: 4.736e-03, eta: 0:07:09, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2129
2023-02-14 05:13:34,649 - mmselfsup - INFO - Epoch [99][250/427]	lr: 4.736e-03, eta: 0:06:36, time: 0.683, data_time: 0.001, memory: 25346, loss: 0.2115
2023-02-14 05:14:08,831 - mmselfsup - INFO - Epoch [99][300/427]	lr: 4.736e-03, eta: 0:06:03, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2124
2023-02-14 05:14:43,145 - mmselfsup - INFO - Epoch [99][350/427]	lr: 4.736e-03, eta: 0:05:31, time: 0.686, data_time: 0.001, memory: 25346, loss: 0.2113
2023-02-14 05:15:17,366 - mmselfsup - INFO - Epoch [99][400/427]	lr: 4.736e-03, eta: 0:04:58, time: 0.684, data_time: 0.001, memory: 25346, loss: 0.2125
2023-02-14 05:16:16,306 - mmselfsup - INFO - Epoch [100][50/427]	lr: 1.184e-03, eta: 0:04:07, time: 0.824, data_time: 0.089, memory: 25346, loss: 0.2100
2023-02-14 05:16:50,487 - mmselfsup - INFO - Epoch [100][100/427]	lr: 1.184e-03, eta: 0:03:34, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2117
2023-02-14 05:17:24,827 - mmselfsup - INFO - Epoch [100][150/427]	lr: 1.184e-03, eta: 0:03:01, time: 0.687, data_time: 0.003, memory: 25346, loss: 0.2121
2023-02-14 05:17:59,046 - mmselfsup - INFO - Epoch [100][200/427]	lr: 1.184e-03, eta: 0:02:29, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2114
2023-02-14 05:18:33,262 - mmselfsup - INFO - Epoch [100][250/427]	lr: 1.184e-03, eta: 0:01:56, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2136
2023-02-14 05:19:07,509 - mmselfsup - INFO - Epoch [100][300/427]	lr: 1.184e-03, eta: 0:01:23, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2096
2023-02-14 05:19:41,779 - mmselfsup - INFO - Epoch [100][350/427]	lr: 1.184e-03, eta: 0:00:50, time: 0.685, data_time: 0.003, memory: 25346, loss: 0.2181
2023-02-14 05:20:15,977 - mmselfsup - INFO - Epoch [100][400/427]	lr: 1.184e-03, eta: 0:00:17, time: 0.684, data_time: 0.003, memory: 25346, loss: 0.2189
2023-02-14 05:20:33,768 - mmselfsup - INFO - Saving checkpoint at 100 epochs
